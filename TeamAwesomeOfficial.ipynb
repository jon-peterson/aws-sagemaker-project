{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('s3://{}/{}'.format('forecastingteamawesomedataset/kaggle-dataset', 'train.csv'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stid</th>\n",
       "      <th>nlat</th>\n",
       "      <th>elon</th>\n",
       "      <th>elev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACME</td>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADAX</td>\n",
       "      <td>34.79851</td>\n",
       "      <td>-96.66909</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALTU</td>\n",
       "      <td>34.58722</td>\n",
       "      <td>-99.33808</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APAC</td>\n",
       "      <td>34.91418</td>\n",
       "      <td>-98.29216</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARNE</td>\n",
       "      <td>36.07204</td>\n",
       "      <td>-99.90308</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stid      nlat      elon  elev\n",
       "0  ACME  34.80833 -98.02325   397\n",
       "1  ADAX  34.79851 -96.66909   295\n",
       "2  ALTU  34.58722 -99.33808   416\n",
       "3  APAC  34.91418 -98.29216   440\n",
       "4  ARNE  36.07204 -99.90308   719"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_info = pd.read_csv('s3://{}/{}'.format('forecastingteamawesomedataset/kaggle-dataset', 'station_info.csv'))\n",
    "station_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/bb20f9b9e24f9a6250f95a432f8d9a7d745f8d24039d7a5a6eaadb7783ba/kaggle-1.5.6.tar.gz (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 20.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (1.23)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (1.11.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2019.6.16)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2.7.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kaggle) (2.20.0)\n",
      "Collecting tqdm (from kaggle)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 27.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting python-slugify (from kaggle)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/5f/7b84a0bba8a0fdd50c046f8b57dcf179dc16237ad33446079b7c484de04c/python-slugify-4.0.0.tar.gz\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->kaggle) (2.6)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/a5/c0b6468d3824fe3fde30dbb5e1f687b291608f9473681bbf7dabbf5a87d7/text_unidecode-1.3-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 30.4MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
      "  Running setup.py bdist_wheel for kaggle ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/57/4e/e8/bb28d035162fb8f17f8ca5d42c3230e284c6aa565b42b72674\n",
      "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/11/94/81/312969455540cb0e6a773e5d68a73c14128bfdfd4a7969bb4f\n",
      "Successfully built kaggle python-slugify\n",
      "Installing collected packages: tqdm, text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.5.6 python-slugify-4.0.0 text-unidecode-1.3 tqdm-4.36.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat ‘../../kaggle.json’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mv ../../kaggle.json /home/ec2-user/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netCDF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a92aca04109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'netCDF4'"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netcdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0d40396ada04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetcdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'netcdf'"
     ]
    }
   ],
   "source": [
    "from netcdf import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevations = Dataset(\"/home/ec2-user/dataset/gefs_elevations.nc\", \"r\", format=\"NETCDF4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "NetCDF: Attribute not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf83c4c46b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0melevations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__getattr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.getncattr\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._get_att\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: NetCDF: Attribute not found"
     ]
    }
   ],
   "source": [
    "elevations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    dimensions(sizes): lon(16), lat(9)\n",
      "    variables(dimensions): float32 elevation_control(lat,lon), float32 elevation_perturbation(lat,lon), float32 latitude(lat,lon), float32 longitude(lat,lon)\n",
      "    groups: \n"
     ]
    }
   ],
   "source": [
    "print(elevations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('elevation_control', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 elevation_control(lat, lon)\n",
      "    units: m\n",
      "unlimited dimensions: \n",
      "current shape = (9, 16)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used), ('elevation_perturbation', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 elevation_perturbation(lat, lon)\n",
      "    units: m\n",
      "unlimited dimensions: \n",
      "current shape = (9, 16)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used), ('latitude', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 latitude(lat, lon)\n",
      "unlimited dimensions: \n",
      "current shape = (9, 16)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used), ('longitude', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 longitude(lat, lon)\n",
      "unlimited dimensions: \n",
      "current shape = (9, 16)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used)])\n"
     ]
    }
   ],
   "source": [
    "print(elevations.variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): lon(16), lat(9)\n",
       "    variables(dimensions): float32 elevation_control(lat,lon), float32 elevation_perturbation(lat,lon), float32 latitude(lat,lon), float32 longitude(lat,lon)\n",
       "    groups: "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cd75a77447fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'train.csv' does not exist: b'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d5b9721296fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'train.csv' does not exist: b'train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "df = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/ec2-user/dataset/train.csv' does not exist: b'/home/ec2-user/dataset/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f9bb82d6813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ec2-user/dataset/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/ec2-user/dataset/train.csv' does not exist: b'/home/ec2-user/dataset/train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/ec2-user/dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19940106</td>\n",
       "      <td>6639000</td>\n",
       "      <td>6817200</td>\n",
       "      <td>8157900</td>\n",
       "      <td>7673100</td>\n",
       "      <td>3500400</td>\n",
       "      <td>2245200</td>\n",
       "      <td>9719400</td>\n",
       "      <td>6137100</td>\n",
       "      <td>4328700</td>\n",
       "      <td>...</td>\n",
       "      <td>5257200</td>\n",
       "      <td>6687000</td>\n",
       "      <td>5631600</td>\n",
       "      <td>7990500</td>\n",
       "      <td>9402600</td>\n",
       "      <td>8463600</td>\n",
       "      <td>7323900</td>\n",
       "      <td>7113600</td>\n",
       "      <td>2124600</td>\n",
       "      <td>3324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19940107</td>\n",
       "      <td>13244700</td>\n",
       "      <td>12418800</td>\n",
       "      <td>12369900</td>\n",
       "      <td>12873000</td>\n",
       "      <td>12181800</td>\n",
       "      <td>9877800</td>\n",
       "      <td>12114300</td>\n",
       "      <td>12175200</td>\n",
       "      <td>11836500</td>\n",
       "      <td>...</td>\n",
       "      <td>11495100</td>\n",
       "      <td>12486300</td>\n",
       "      <td>12098700</td>\n",
       "      <td>13191300</td>\n",
       "      <td>11855100</td>\n",
       "      <td>11494800</td>\n",
       "      <td>12620400</td>\n",
       "      <td>12380700</td>\n",
       "      <td>10944900</td>\n",
       "      <td>11403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19940108</td>\n",
       "      <td>12927900</td>\n",
       "      <td>12375600</td>\n",
       "      <td>12634500</td>\n",
       "      <td>13066500</td>\n",
       "      <td>11608800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>12029400</td>\n",
       "      <td>12217500</td>\n",
       "      <td>11505300</td>\n",
       "      <td>...</td>\n",
       "      <td>11284800</td>\n",
       "      <td>12471300</td>\n",
       "      <td>12072300</td>\n",
       "      <td>10429500</td>\n",
       "      <td>11939100</td>\n",
       "      <td>11280300</td>\n",
       "      <td>12419700</td>\n",
       "      <td>12225900</td>\n",
       "      <td>11029200</td>\n",
       "      <td>11268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19940109</td>\n",
       "      <td>12600300</td>\n",
       "      <td>11601000</td>\n",
       "      <td>12156000</td>\n",
       "      <td>12464700</td>\n",
       "      <td>10866000</td>\n",
       "      <td>11295300</td>\n",
       "      <td>11937900</td>\n",
       "      <td>10443300</td>\n",
       "      <td>9218400</td>\n",
       "      <td>...</td>\n",
       "      <td>8755800</td>\n",
       "      <td>12391800</td>\n",
       "      <td>11369400</td>\n",
       "      <td>11324400</td>\n",
       "      <td>11498700</td>\n",
       "      <td>9737100</td>\n",
       "      <td>11985000</td>\n",
       "      <td>11454900</td>\n",
       "      <td>10518300</td>\n",
       "      <td>8577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19940110</td>\n",
       "      <td>6406500</td>\n",
       "      <td>3935700</td>\n",
       "      <td>12321900</td>\n",
       "      <td>8164800</td>\n",
       "      <td>11328600</td>\n",
       "      <td>10785000</td>\n",
       "      <td>12081600</td>\n",
       "      <td>1873800</td>\n",
       "      <td>9658800</td>\n",
       "      <td>...</td>\n",
       "      <td>3155100</td>\n",
       "      <td>3879900</td>\n",
       "      <td>11709300</td>\n",
       "      <td>3808500</td>\n",
       "      <td>11526900</td>\n",
       "      <td>2064300</td>\n",
       "      <td>7641000</td>\n",
       "      <td>2282400</td>\n",
       "      <td>10859700</td>\n",
       "      <td>2520600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19940111</td>\n",
       "      <td>12743400</td>\n",
       "      <td>7137000</td>\n",
       "      <td>12966300</td>\n",
       "      <td>12774600</td>\n",
       "      <td>12005100</td>\n",
       "      <td>11424900</td>\n",
       "      <td>12149400</td>\n",
       "      <td>2835600</td>\n",
       "      <td>2574000</td>\n",
       "      <td>...</td>\n",
       "      <td>3411600</td>\n",
       "      <td>12371400</td>\n",
       "      <td>11973000</td>\n",
       "      <td>12872400</td>\n",
       "      <td>11970900</td>\n",
       "      <td>1908000</td>\n",
       "      <td>1998300</td>\n",
       "      <td>1577100</td>\n",
       "      <td>11256600</td>\n",
       "      <td>2923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19940112</td>\n",
       "      <td>10453500</td>\n",
       "      <td>7371000</td>\n",
       "      <td>12855300</td>\n",
       "      <td>11448000</td>\n",
       "      <td>11493000</td>\n",
       "      <td>11794200</td>\n",
       "      <td>11780400</td>\n",
       "      <td>6759900</td>\n",
       "      <td>3571800</td>\n",
       "      <td>...</td>\n",
       "      <td>2370600</td>\n",
       "      <td>8449200</td>\n",
       "      <td>11049000</td>\n",
       "      <td>8650500</td>\n",
       "      <td>11623200</td>\n",
       "      <td>5994300</td>\n",
       "      <td>4441500</td>\n",
       "      <td>6324900</td>\n",
       "      <td>10424100</td>\n",
       "      <td>7131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19940113</td>\n",
       "      <td>12985200</td>\n",
       "      <td>12510600</td>\n",
       "      <td>13198500</td>\n",
       "      <td>12726900</td>\n",
       "      <td>12289200</td>\n",
       "      <td>12149100</td>\n",
       "      <td>12467100</td>\n",
       "      <td>9930900</td>\n",
       "      <td>9628500</td>\n",
       "      <td>...</td>\n",
       "      <td>8013000</td>\n",
       "      <td>12463200</td>\n",
       "      <td>12099900</td>\n",
       "      <td>13268400</td>\n",
       "      <td>12205500</td>\n",
       "      <td>10277100</td>\n",
       "      <td>9863700</td>\n",
       "      <td>8588400</td>\n",
       "      <td>11655300</td>\n",
       "      <td>10062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19940114</td>\n",
       "      <td>13080000</td>\n",
       "      <td>12552000</td>\n",
       "      <td>13446600</td>\n",
       "      <td>13026600</td>\n",
       "      <td>12393000</td>\n",
       "      <td>12227700</td>\n",
       "      <td>12488700</td>\n",
       "      <td>10799100</td>\n",
       "      <td>10770900</td>\n",
       "      <td>...</td>\n",
       "      <td>6584100</td>\n",
       "      <td>12793800</td>\n",
       "      <td>12221100</td>\n",
       "      <td>12990000</td>\n",
       "      <td>12166800</td>\n",
       "      <td>5569500</td>\n",
       "      <td>11459400</td>\n",
       "      <td>11072100</td>\n",
       "      <td>11614200</td>\n",
       "      <td>9288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19940115</td>\n",
       "      <td>11826300</td>\n",
       "      <td>11997300</td>\n",
       "      <td>11313300</td>\n",
       "      <td>11793300</td>\n",
       "      <td>10750200</td>\n",
       "      <td>10290600</td>\n",
       "      <td>11184600</td>\n",
       "      <td>12337500</td>\n",
       "      <td>11489700</td>\n",
       "      <td>...</td>\n",
       "      <td>8565600</td>\n",
       "      <td>11604000</td>\n",
       "      <td>12094800</td>\n",
       "      <td>11343900</td>\n",
       "      <td>10906500</td>\n",
       "      <td>11029500</td>\n",
       "      <td>12093300</td>\n",
       "      <td>12119100</td>\n",
       "      <td>10898700</td>\n",
       "      <td>10905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19940116</td>\n",
       "      <td>1974000</td>\n",
       "      <td>1339800</td>\n",
       "      <td>3120600</td>\n",
       "      <td>1058700</td>\n",
       "      <td>7187100</td>\n",
       "      <td>9792900</td>\n",
       "      <td>1405500</td>\n",
       "      <td>711900</td>\n",
       "      <td>1133700</td>\n",
       "      <td>...</td>\n",
       "      <td>640200</td>\n",
       "      <td>1401300</td>\n",
       "      <td>1288500</td>\n",
       "      <td>1257600</td>\n",
       "      <td>1125300</td>\n",
       "      <td>739800</td>\n",
       "      <td>756900</td>\n",
       "      <td>469500</td>\n",
       "      <td>3777300</td>\n",
       "      <td>748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19940117</td>\n",
       "      <td>13541700</td>\n",
       "      <td>13021200</td>\n",
       "      <td>13757100</td>\n",
       "      <td>13432800</td>\n",
       "      <td>12486600</td>\n",
       "      <td>11738100</td>\n",
       "      <td>12719700</td>\n",
       "      <td>7774200</td>\n",
       "      <td>9512400</td>\n",
       "      <td>...</td>\n",
       "      <td>10155300</td>\n",
       "      <td>13079400</td>\n",
       "      <td>12507900</td>\n",
       "      <td>13869000</td>\n",
       "      <td>12336300</td>\n",
       "      <td>11681100</td>\n",
       "      <td>9247500</td>\n",
       "      <td>10576200</td>\n",
       "      <td>11586900</td>\n",
       "      <td>9893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19940118</td>\n",
       "      <td>13673700</td>\n",
       "      <td>13042200</td>\n",
       "      <td>13881000</td>\n",
       "      <td>13586100</td>\n",
       "      <td>13158300</td>\n",
       "      <td>12724200</td>\n",
       "      <td>12984000</td>\n",
       "      <td>9400800</td>\n",
       "      <td>11339400</td>\n",
       "      <td>...</td>\n",
       "      <td>8709300</td>\n",
       "      <td>13125900</td>\n",
       "      <td>12694500</td>\n",
       "      <td>13959900</td>\n",
       "      <td>12517800</td>\n",
       "      <td>12465000</td>\n",
       "      <td>12720600</td>\n",
       "      <td>10228200</td>\n",
       "      <td>12076800</td>\n",
       "      <td>10843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19940119</td>\n",
       "      <td>6796800</td>\n",
       "      <td>8217000</td>\n",
       "      <td>13563300</td>\n",
       "      <td>7861800</td>\n",
       "      <td>13200900</td>\n",
       "      <td>13184700</td>\n",
       "      <td>12884700</td>\n",
       "      <td>10399500</td>\n",
       "      <td>12214500</td>\n",
       "      <td>...</td>\n",
       "      <td>8323200</td>\n",
       "      <td>7311600</td>\n",
       "      <td>12622800</td>\n",
       "      <td>5567400</td>\n",
       "      <td>12464700</td>\n",
       "      <td>14057400</td>\n",
       "      <td>11255400</td>\n",
       "      <td>12093900</td>\n",
       "      <td>12372300</td>\n",
       "      <td>10474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19940120</td>\n",
       "      <td>5658900</td>\n",
       "      <td>4757700</td>\n",
       "      <td>1976100</td>\n",
       "      <td>4926000</td>\n",
       "      <td>3088800</td>\n",
       "      <td>10210500</td>\n",
       "      <td>3547200</td>\n",
       "      <td>10607100</td>\n",
       "      <td>5247900</td>\n",
       "      <td>...</td>\n",
       "      <td>4558800</td>\n",
       "      <td>5358300</td>\n",
       "      <td>5883600</td>\n",
       "      <td>2109900</td>\n",
       "      <td>4254900</td>\n",
       "      <td>6372900</td>\n",
       "      <td>8222700</td>\n",
       "      <td>9356400</td>\n",
       "      <td>6758100</td>\n",
       "      <td>7672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19940121</td>\n",
       "      <td>7073400</td>\n",
       "      <td>10822800</td>\n",
       "      <td>4021800</td>\n",
       "      <td>6464100</td>\n",
       "      <td>4468500</td>\n",
       "      <td>11169300</td>\n",
       "      <td>5096100</td>\n",
       "      <td>12288000</td>\n",
       "      <td>11910300</td>\n",
       "      <td>...</td>\n",
       "      <td>11721600</td>\n",
       "      <td>9132300</td>\n",
       "      <td>7908900</td>\n",
       "      <td>3649500</td>\n",
       "      <td>6578100</td>\n",
       "      <td>11701200</td>\n",
       "      <td>12758400</td>\n",
       "      <td>12867300</td>\n",
       "      <td>5543700</td>\n",
       "      <td>11705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19940122</td>\n",
       "      <td>3354000</td>\n",
       "      <td>2764800</td>\n",
       "      <td>2997900</td>\n",
       "      <td>2726100</td>\n",
       "      <td>8875800</td>\n",
       "      <td>12647400</td>\n",
       "      <td>2881500</td>\n",
       "      <td>3448500</td>\n",
       "      <td>8096400</td>\n",
       "      <td>...</td>\n",
       "      <td>7866600</td>\n",
       "      <td>2923200</td>\n",
       "      <td>4995000</td>\n",
       "      <td>1614900</td>\n",
       "      <td>3245400</td>\n",
       "      <td>3254100</td>\n",
       "      <td>3469800</td>\n",
       "      <td>2306100</td>\n",
       "      <td>9792600</td>\n",
       "      <td>5962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19940123</td>\n",
       "      <td>2579700</td>\n",
       "      <td>2339100</td>\n",
       "      <td>6508500</td>\n",
       "      <td>5418600</td>\n",
       "      <td>12936900</td>\n",
       "      <td>12553500</td>\n",
       "      <td>9266400</td>\n",
       "      <td>1948800</td>\n",
       "      <td>6969000</td>\n",
       "      <td>...</td>\n",
       "      <td>2103000</td>\n",
       "      <td>2631900</td>\n",
       "      <td>10932900</td>\n",
       "      <td>2564700</td>\n",
       "      <td>8967900</td>\n",
       "      <td>1685400</td>\n",
       "      <td>2399700</td>\n",
       "      <td>2453400</td>\n",
       "      <td>12991800</td>\n",
       "      <td>3377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19940124</td>\n",
       "      <td>2387700</td>\n",
       "      <td>2096700</td>\n",
       "      <td>6390000</td>\n",
       "      <td>3164700</td>\n",
       "      <td>9348300</td>\n",
       "      <td>7642200</td>\n",
       "      <td>5437500</td>\n",
       "      <td>1742100</td>\n",
       "      <td>3155100</td>\n",
       "      <td>...</td>\n",
       "      <td>1695300</td>\n",
       "      <td>2771400</td>\n",
       "      <td>5281800</td>\n",
       "      <td>3616500</td>\n",
       "      <td>4453500</td>\n",
       "      <td>1114200</td>\n",
       "      <td>2427300</td>\n",
       "      <td>1808400</td>\n",
       "      <td>9188100</td>\n",
       "      <td>2060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19940125</td>\n",
       "      <td>8390700</td>\n",
       "      <td>8621700</td>\n",
       "      <td>11639100</td>\n",
       "      <td>11404200</td>\n",
       "      <td>11722200</td>\n",
       "      <td>13093500</td>\n",
       "      <td>11776500</td>\n",
       "      <td>7530900</td>\n",
       "      <td>4177200</td>\n",
       "      <td>...</td>\n",
       "      <td>6538500</td>\n",
       "      <td>9204900</td>\n",
       "      <td>8374800</td>\n",
       "      <td>12530400</td>\n",
       "      <td>10918200</td>\n",
       "      <td>6499500</td>\n",
       "      <td>7495200</td>\n",
       "      <td>6218700</td>\n",
       "      <td>12260100</td>\n",
       "      <td>7261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19940126</td>\n",
       "      <td>7326600</td>\n",
       "      <td>8104500</td>\n",
       "      <td>10221900</td>\n",
       "      <td>8622300</td>\n",
       "      <td>12958500</td>\n",
       "      <td>7973100</td>\n",
       "      <td>11801400</td>\n",
       "      <td>5955000</td>\n",
       "      <td>7979100</td>\n",
       "      <td>...</td>\n",
       "      <td>3879300</td>\n",
       "      <td>8131500</td>\n",
       "      <td>11583600</td>\n",
       "      <td>8124000</td>\n",
       "      <td>11062800</td>\n",
       "      <td>3655800</td>\n",
       "      <td>6165600</td>\n",
       "      <td>3794100</td>\n",
       "      <td>11837400</td>\n",
       "      <td>6168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19940127</td>\n",
       "      <td>10743900</td>\n",
       "      <td>8913000</td>\n",
       "      <td>13336500</td>\n",
       "      <td>11616000</td>\n",
       "      <td>12311700</td>\n",
       "      <td>9855300</td>\n",
       "      <td>13071000</td>\n",
       "      <td>10467300</td>\n",
       "      <td>11326800</td>\n",
       "      <td>...</td>\n",
       "      <td>6998400</td>\n",
       "      <td>10048800</td>\n",
       "      <td>12608400</td>\n",
       "      <td>11251800</td>\n",
       "      <td>12623100</td>\n",
       "      <td>5641200</td>\n",
       "      <td>5928900</td>\n",
       "      <td>4412100</td>\n",
       "      <td>12267900</td>\n",
       "      <td>11280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19940128</td>\n",
       "      <td>12812100</td>\n",
       "      <td>13680900</td>\n",
       "      <td>12890700</td>\n",
       "      <td>13834800</td>\n",
       "      <td>13994700</td>\n",
       "      <td>13548000</td>\n",
       "      <td>13434300</td>\n",
       "      <td>11479200</td>\n",
       "      <td>8445900</td>\n",
       "      <td>...</td>\n",
       "      <td>9601500</td>\n",
       "      <td>13080300</td>\n",
       "      <td>11967900</td>\n",
       "      <td>12759300</td>\n",
       "      <td>12837300</td>\n",
       "      <td>9696300</td>\n",
       "      <td>12404400</td>\n",
       "      <td>10782600</td>\n",
       "      <td>11917500</td>\n",
       "      <td>8725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19940129</td>\n",
       "      <td>9065100</td>\n",
       "      <td>5018100</td>\n",
       "      <td>10373700</td>\n",
       "      <td>10304100</td>\n",
       "      <td>13456800</td>\n",
       "      <td>13098600</td>\n",
       "      <td>13869900</td>\n",
       "      <td>8671200</td>\n",
       "      <td>12211500</td>\n",
       "      <td>...</td>\n",
       "      <td>12647400</td>\n",
       "      <td>7510500</td>\n",
       "      <td>13708200</td>\n",
       "      <td>5198700</td>\n",
       "      <td>13585800</td>\n",
       "      <td>7622400</td>\n",
       "      <td>5036400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>12272700</td>\n",
       "      <td>11245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19940130</td>\n",
       "      <td>3954900</td>\n",
       "      <td>7685700</td>\n",
       "      <td>3863100</td>\n",
       "      <td>4220700</td>\n",
       "      <td>5629200</td>\n",
       "      <td>2520000</td>\n",
       "      <td>5959500</td>\n",
       "      <td>11066400</td>\n",
       "      <td>6694800</td>\n",
       "      <td>...</td>\n",
       "      <td>9793800</td>\n",
       "      <td>6205500</td>\n",
       "      <td>5371200</td>\n",
       "      <td>6331800</td>\n",
       "      <td>6069600</td>\n",
       "      <td>12349500</td>\n",
       "      <td>9744900</td>\n",
       "      <td>10736400</td>\n",
       "      <td>5056800</td>\n",
       "      <td>7212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>20071202</td>\n",
       "      <td>12177000</td>\n",
       "      <td>11132400</td>\n",
       "      <td>12620100</td>\n",
       "      <td>11796600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12604500</td>\n",
       "      <td>11340600</td>\n",
       "      <td>7562100</td>\n",
       "      <td>8957100</td>\n",
       "      <td>...</td>\n",
       "      <td>5517600</td>\n",
       "      <td>10997700</td>\n",
       "      <td>10304400</td>\n",
       "      <td>12716700</td>\n",
       "      <td>11124600</td>\n",
       "      <td>3845700</td>\n",
       "      <td>8625000</td>\n",
       "      <td>7135500</td>\n",
       "      <td>10745100</td>\n",
       "      <td>7718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>20071203</td>\n",
       "      <td>13453800</td>\n",
       "      <td>12737700</td>\n",
       "      <td>12828300</td>\n",
       "      <td>12809700</td>\n",
       "      <td>12264300</td>\n",
       "      <td>12439200</td>\n",
       "      <td>12496800</td>\n",
       "      <td>12446100</td>\n",
       "      <td>11181000</td>\n",
       "      <td>...</td>\n",
       "      <td>11936700</td>\n",
       "      <td>13204500</td>\n",
       "      <td>12599400</td>\n",
       "      <td>13403400</td>\n",
       "      <td>12964500</td>\n",
       "      <td>12735900</td>\n",
       "      <td>13133100</td>\n",
       "      <td>12933900</td>\n",
       "      <td>11300400</td>\n",
       "      <td>12414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>20071204</td>\n",
       "      <td>12525900</td>\n",
       "      <td>10054800</td>\n",
       "      <td>12328200</td>\n",
       "      <td>12057900</td>\n",
       "      <td>12027900</td>\n",
       "      <td>11833500</td>\n",
       "      <td>11980200</td>\n",
       "      <td>10532100</td>\n",
       "      <td>9660300</td>\n",
       "      <td>...</td>\n",
       "      <td>10314000</td>\n",
       "      <td>12219900</td>\n",
       "      <td>12349200</td>\n",
       "      <td>12531600</td>\n",
       "      <td>12132600</td>\n",
       "      <td>10984800</td>\n",
       "      <td>12141600</td>\n",
       "      <td>11658900</td>\n",
       "      <td>10941300</td>\n",
       "      <td>10495500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>20071205</td>\n",
       "      <td>10543500</td>\n",
       "      <td>8691900</td>\n",
       "      <td>12200100</td>\n",
       "      <td>11790300</td>\n",
       "      <td>9699300</td>\n",
       "      <td>10742100</td>\n",
       "      <td>12017700</td>\n",
       "      <td>10608900</td>\n",
       "      <td>10084500</td>\n",
       "      <td>...</td>\n",
       "      <td>7831500</td>\n",
       "      <td>10752600</td>\n",
       "      <td>11216700</td>\n",
       "      <td>10968900</td>\n",
       "      <td>12183600</td>\n",
       "      <td>9241500</td>\n",
       "      <td>8371200</td>\n",
       "      <td>8967900</td>\n",
       "      <td>9399900</td>\n",
       "      <td>11064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>20071206</td>\n",
       "      <td>6214500</td>\n",
       "      <td>5707500</td>\n",
       "      <td>6339900</td>\n",
       "      <td>6242400</td>\n",
       "      <td>7813800</td>\n",
       "      <td>8302800</td>\n",
       "      <td>6486600</td>\n",
       "      <td>5260200</td>\n",
       "      <td>2649300</td>\n",
       "      <td>...</td>\n",
       "      <td>2001300</td>\n",
       "      <td>7011000</td>\n",
       "      <td>6879900</td>\n",
       "      <td>7605000</td>\n",
       "      <td>7298700</td>\n",
       "      <td>3600900</td>\n",
       "      <td>5550900</td>\n",
       "      <td>5042700</td>\n",
       "      <td>6977700</td>\n",
       "      <td>3435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>20071207</td>\n",
       "      <td>8417400</td>\n",
       "      <td>6139800</td>\n",
       "      <td>6981000</td>\n",
       "      <td>6928200</td>\n",
       "      <td>2229600</td>\n",
       "      <td>1899600</td>\n",
       "      <td>2492100</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1651800</td>\n",
       "      <td>...</td>\n",
       "      <td>1268700</td>\n",
       "      <td>6294300</td>\n",
       "      <td>1935000</td>\n",
       "      <td>10633800</td>\n",
       "      <td>3195900</td>\n",
       "      <td>1305000</td>\n",
       "      <td>3027000</td>\n",
       "      <td>2423100</td>\n",
       "      <td>1835700</td>\n",
       "      <td>1757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>20071208</td>\n",
       "      <td>1683900</td>\n",
       "      <td>1832700</td>\n",
       "      <td>2153700</td>\n",
       "      <td>1853400</td>\n",
       "      <td>2145600</td>\n",
       "      <td>1705200</td>\n",
       "      <td>1886100</td>\n",
       "      <td>1878000</td>\n",
       "      <td>1591200</td>\n",
       "      <td>...</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1853400</td>\n",
       "      <td>1761000</td>\n",
       "      <td>1690200</td>\n",
       "      <td>2251500</td>\n",
       "      <td>1556100</td>\n",
       "      <td>4414800</td>\n",
       "      <td>5186100</td>\n",
       "      <td>1826400</td>\n",
       "      <td>1762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>20071209</td>\n",
       "      <td>1512300</td>\n",
       "      <td>1394400</td>\n",
       "      <td>1685700</td>\n",
       "      <td>1443900</td>\n",
       "      <td>3815100</td>\n",
       "      <td>5288700</td>\n",
       "      <td>2505600</td>\n",
       "      <td>1129800</td>\n",
       "      <td>3070200</td>\n",
       "      <td>...</td>\n",
       "      <td>1245900</td>\n",
       "      <td>1558800</td>\n",
       "      <td>2174100</td>\n",
       "      <td>1551300</td>\n",
       "      <td>2354400</td>\n",
       "      <td>1321500</td>\n",
       "      <td>1269300</td>\n",
       "      <td>1362300</td>\n",
       "      <td>3344400</td>\n",
       "      <td>1480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>20071210</td>\n",
       "      <td>1729500</td>\n",
       "      <td>1530600</td>\n",
       "      <td>1451400</td>\n",
       "      <td>1747500</td>\n",
       "      <td>1628400</td>\n",
       "      <td>3313200</td>\n",
       "      <td>1207800</td>\n",
       "      <td>1018500</td>\n",
       "      <td>2247000</td>\n",
       "      <td>...</td>\n",
       "      <td>1927800</td>\n",
       "      <td>1509300</td>\n",
       "      <td>1086000</td>\n",
       "      <td>1509000</td>\n",
       "      <td>1559400</td>\n",
       "      <td>1415400</td>\n",
       "      <td>1537200</td>\n",
       "      <td>1957500</td>\n",
       "      <td>2050800</td>\n",
       "      <td>2024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>20071211</td>\n",
       "      <td>1512600</td>\n",
       "      <td>819600</td>\n",
       "      <td>1236900</td>\n",
       "      <td>1648800</td>\n",
       "      <td>2398800</td>\n",
       "      <td>2607600</td>\n",
       "      <td>1206600</td>\n",
       "      <td>1074300</td>\n",
       "      <td>1185600</td>\n",
       "      <td>...</td>\n",
       "      <td>1446900</td>\n",
       "      <td>1281600</td>\n",
       "      <td>1359000</td>\n",
       "      <td>1127400</td>\n",
       "      <td>1397700</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1028100</td>\n",
       "      <td>2350500</td>\n",
       "      <td>1860900</td>\n",
       "      <td>1549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>20071212</td>\n",
       "      <td>2126100</td>\n",
       "      <td>1477500</td>\n",
       "      <td>5262600</td>\n",
       "      <td>2496900</td>\n",
       "      <td>6317400</td>\n",
       "      <td>8295600</td>\n",
       "      <td>5603100</td>\n",
       "      <td>1671900</td>\n",
       "      <td>1977300</td>\n",
       "      <td>...</td>\n",
       "      <td>1392000</td>\n",
       "      <td>1669200</td>\n",
       "      <td>2820000</td>\n",
       "      <td>2087100</td>\n",
       "      <td>4227900</td>\n",
       "      <td>1423800</td>\n",
       "      <td>1272300</td>\n",
       "      <td>916500</td>\n",
       "      <td>4206300</td>\n",
       "      <td>1726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>20071213</td>\n",
       "      <td>3476400</td>\n",
       "      <td>3016200</td>\n",
       "      <td>8252400</td>\n",
       "      <td>3503700</td>\n",
       "      <td>11037600</td>\n",
       "      <td>11493600</td>\n",
       "      <td>5314800</td>\n",
       "      <td>2861700</td>\n",
       "      <td>3190500</td>\n",
       "      <td>...</td>\n",
       "      <td>2737500</td>\n",
       "      <td>3606900</td>\n",
       "      <td>4408800</td>\n",
       "      <td>3242400</td>\n",
       "      <td>4901700</td>\n",
       "      <td>4254600</td>\n",
       "      <td>3825000</td>\n",
       "      <td>3884700</td>\n",
       "      <td>10216500</td>\n",
       "      <td>3342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>20071214</td>\n",
       "      <td>1262700</td>\n",
       "      <td>1544700</td>\n",
       "      <td>921000</td>\n",
       "      <td>1227000</td>\n",
       "      <td>1224300</td>\n",
       "      <td>1073100</td>\n",
       "      <td>1342200</td>\n",
       "      <td>2343000</td>\n",
       "      <td>1903800</td>\n",
       "      <td>...</td>\n",
       "      <td>6358800</td>\n",
       "      <td>1354200</td>\n",
       "      <td>1438800</td>\n",
       "      <td>1148100</td>\n",
       "      <td>1734000</td>\n",
       "      <td>5733600</td>\n",
       "      <td>2664600</td>\n",
       "      <td>4408500</td>\n",
       "      <td>1123800</td>\n",
       "      <td>1885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>20071215</td>\n",
       "      <td>3074400</td>\n",
       "      <td>1500600</td>\n",
       "      <td>6650400</td>\n",
       "      <td>3486300</td>\n",
       "      <td>7247400</td>\n",
       "      <td>9481800</td>\n",
       "      <td>6552000</td>\n",
       "      <td>1593000</td>\n",
       "      <td>6715200</td>\n",
       "      <td>...</td>\n",
       "      <td>2708700</td>\n",
       "      <td>3045600</td>\n",
       "      <td>7341600</td>\n",
       "      <td>3783000</td>\n",
       "      <td>6811500</td>\n",
       "      <td>1209300</td>\n",
       "      <td>1220700</td>\n",
       "      <td>1414500</td>\n",
       "      <td>5080800</td>\n",
       "      <td>4438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>20071216</td>\n",
       "      <td>12523800</td>\n",
       "      <td>12006300</td>\n",
       "      <td>12210300</td>\n",
       "      <td>12167100</td>\n",
       "      <td>11643000</td>\n",
       "      <td>12363300</td>\n",
       "      <td>11934600</td>\n",
       "      <td>11676900</td>\n",
       "      <td>10769100</td>\n",
       "      <td>...</td>\n",
       "      <td>11114700</td>\n",
       "      <td>12364200</td>\n",
       "      <td>13882200</td>\n",
       "      <td>12572400</td>\n",
       "      <td>12260400</td>\n",
       "      <td>11368200</td>\n",
       "      <td>12465300</td>\n",
       "      <td>11695800</td>\n",
       "      <td>11119800</td>\n",
       "      <td>11652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>20071217</td>\n",
       "      <td>12089700</td>\n",
       "      <td>11742900</td>\n",
       "      <td>12130200</td>\n",
       "      <td>11095800</td>\n",
       "      <td>10793100</td>\n",
       "      <td>11952600</td>\n",
       "      <td>10750200</td>\n",
       "      <td>11702700</td>\n",
       "      <td>10053900</td>\n",
       "      <td>...</td>\n",
       "      <td>10693800</td>\n",
       "      <td>12084300</td>\n",
       "      <td>11665800</td>\n",
       "      <td>11436600</td>\n",
       "      <td>11346600</td>\n",
       "      <td>11470500</td>\n",
       "      <td>12253200</td>\n",
       "      <td>11610000</td>\n",
       "      <td>10333500</td>\n",
       "      <td>10910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>20071218</td>\n",
       "      <td>12300600</td>\n",
       "      <td>11602500</td>\n",
       "      <td>12087900</td>\n",
       "      <td>11866200</td>\n",
       "      <td>11452500</td>\n",
       "      <td>11811900</td>\n",
       "      <td>11696700</td>\n",
       "      <td>11232600</td>\n",
       "      <td>10142100</td>\n",
       "      <td>...</td>\n",
       "      <td>10272300</td>\n",
       "      <td>12104700</td>\n",
       "      <td>11622300</td>\n",
       "      <td>12438300</td>\n",
       "      <td>10248000</td>\n",
       "      <td>9291600</td>\n",
       "      <td>11736900</td>\n",
       "      <td>10968300</td>\n",
       "      <td>10684800</td>\n",
       "      <td>10991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>20071219</td>\n",
       "      <td>12206100</td>\n",
       "      <td>11478000</td>\n",
       "      <td>12020400</td>\n",
       "      <td>11741100</td>\n",
       "      <td>10871400</td>\n",
       "      <td>11946600</td>\n",
       "      <td>11505600</td>\n",
       "      <td>11184600</td>\n",
       "      <td>9769800</td>\n",
       "      <td>...</td>\n",
       "      <td>9334800</td>\n",
       "      <td>11976900</td>\n",
       "      <td>11744400</td>\n",
       "      <td>12171600</td>\n",
       "      <td>11703000</td>\n",
       "      <td>11015700</td>\n",
       "      <td>11782800</td>\n",
       "      <td>11303100</td>\n",
       "      <td>10367700</td>\n",
       "      <td>9920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>20071220</td>\n",
       "      <td>12275700</td>\n",
       "      <td>11530800</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11865600</td>\n",
       "      <td>11614800</td>\n",
       "      <td>11707500</td>\n",
       "      <td>11698800</td>\n",
       "      <td>11053200</td>\n",
       "      <td>10116900</td>\n",
       "      <td>...</td>\n",
       "      <td>10092600</td>\n",
       "      <td>10224300</td>\n",
       "      <td>11861700</td>\n",
       "      <td>12374400</td>\n",
       "      <td>11761500</td>\n",
       "      <td>8301000</td>\n",
       "      <td>8917500</td>\n",
       "      <td>7947300</td>\n",
       "      <td>10679100</td>\n",
       "      <td>11154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>20071221</td>\n",
       "      <td>9737700</td>\n",
       "      <td>9611100</td>\n",
       "      <td>8948400</td>\n",
       "      <td>8915100</td>\n",
       "      <td>10750800</td>\n",
       "      <td>8196000</td>\n",
       "      <td>10429500</td>\n",
       "      <td>9107700</td>\n",
       "      <td>8867700</td>\n",
       "      <td>...</td>\n",
       "      <td>8824800</td>\n",
       "      <td>9253500</td>\n",
       "      <td>11017500</td>\n",
       "      <td>10610400</td>\n",
       "      <td>10148700</td>\n",
       "      <td>8847600</td>\n",
       "      <td>9985200</td>\n",
       "      <td>9971700</td>\n",
       "      <td>9173100</td>\n",
       "      <td>9162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>20071222</td>\n",
       "      <td>1958400</td>\n",
       "      <td>2235300</td>\n",
       "      <td>2364900</td>\n",
       "      <td>2108700</td>\n",
       "      <td>5511000</td>\n",
       "      <td>8464800</td>\n",
       "      <td>2461800</td>\n",
       "      <td>677100</td>\n",
       "      <td>1535400</td>\n",
       "      <td>...</td>\n",
       "      <td>597600</td>\n",
       "      <td>1733100</td>\n",
       "      <td>2578200</td>\n",
       "      <td>1802400</td>\n",
       "      <td>2522700</td>\n",
       "      <td>2739600</td>\n",
       "      <td>6989100</td>\n",
       "      <td>7298400</td>\n",
       "      <td>4391700</td>\n",
       "      <td>902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>20071223</td>\n",
       "      <td>12325200</td>\n",
       "      <td>11827200</td>\n",
       "      <td>11969100</td>\n",
       "      <td>11844900</td>\n",
       "      <td>11698500</td>\n",
       "      <td>10371300</td>\n",
       "      <td>11761200</td>\n",
       "      <td>11413200</td>\n",
       "      <td>10904400</td>\n",
       "      <td>...</td>\n",
       "      <td>11088900</td>\n",
       "      <td>12319800</td>\n",
       "      <td>12013800</td>\n",
       "      <td>12653400</td>\n",
       "      <td>12035100</td>\n",
       "      <td>11593500</td>\n",
       "      <td>12151200</td>\n",
       "      <td>11898000</td>\n",
       "      <td>9895800</td>\n",
       "      <td>11434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>20071224</td>\n",
       "      <td>12225900</td>\n",
       "      <td>11599500</td>\n",
       "      <td>12061500</td>\n",
       "      <td>11865300</td>\n",
       "      <td>11774700</td>\n",
       "      <td>11851800</td>\n",
       "      <td>11720400</td>\n",
       "      <td>11379600</td>\n",
       "      <td>10454100</td>\n",
       "      <td>...</td>\n",
       "      <td>10995600</td>\n",
       "      <td>12055800</td>\n",
       "      <td>12075900</td>\n",
       "      <td>12308100</td>\n",
       "      <td>11938200</td>\n",
       "      <td>11466300</td>\n",
       "      <td>11734800</td>\n",
       "      <td>11522100</td>\n",
       "      <td>10793700</td>\n",
       "      <td>11464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>20071225</td>\n",
       "      <td>12256500</td>\n",
       "      <td>11818200</td>\n",
       "      <td>12093000</td>\n",
       "      <td>11823000</td>\n",
       "      <td>11540400</td>\n",
       "      <td>9601500</td>\n",
       "      <td>11670000</td>\n",
       "      <td>11565300</td>\n",
       "      <td>10189500</td>\n",
       "      <td>...</td>\n",
       "      <td>11088600</td>\n",
       "      <td>12075300</td>\n",
       "      <td>11924400</td>\n",
       "      <td>12457800</td>\n",
       "      <td>11772600</td>\n",
       "      <td>11641200</td>\n",
       "      <td>12170400</td>\n",
       "      <td>11836800</td>\n",
       "      <td>10649700</td>\n",
       "      <td>11517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5107</th>\n",
       "      <td>20071226</td>\n",
       "      <td>1851300</td>\n",
       "      <td>1551600</td>\n",
       "      <td>3227700</td>\n",
       "      <td>1722900</td>\n",
       "      <td>7181100</td>\n",
       "      <td>11574900</td>\n",
       "      <td>3165000</td>\n",
       "      <td>3704100</td>\n",
       "      <td>2848500</td>\n",
       "      <td>...</td>\n",
       "      <td>1938300</td>\n",
       "      <td>2681400</td>\n",
       "      <td>2894400</td>\n",
       "      <td>3147300</td>\n",
       "      <td>3001200</td>\n",
       "      <td>1406400</td>\n",
       "      <td>1595400</td>\n",
       "      <td>1460700</td>\n",
       "      <td>6298200</td>\n",
       "      <td>2382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>20071227</td>\n",
       "      <td>1408500</td>\n",
       "      <td>1371000</td>\n",
       "      <td>1742400</td>\n",
       "      <td>1389300</td>\n",
       "      <td>1349400</td>\n",
       "      <td>1466400</td>\n",
       "      <td>1245900</td>\n",
       "      <td>1674000</td>\n",
       "      <td>922500</td>\n",
       "      <td>...</td>\n",
       "      <td>1511100</td>\n",
       "      <td>1220700</td>\n",
       "      <td>1083300</td>\n",
       "      <td>1887900</td>\n",
       "      <td>1467900</td>\n",
       "      <td>1404000</td>\n",
       "      <td>1510500</td>\n",
       "      <td>1747500</td>\n",
       "      <td>1079100</td>\n",
       "      <td>1100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5109</th>\n",
       "      <td>20071228</td>\n",
       "      <td>10060800</td>\n",
       "      <td>6581700</td>\n",
       "      <td>12027300</td>\n",
       "      <td>11312100</td>\n",
       "      <td>9665100</td>\n",
       "      <td>7993800</td>\n",
       "      <td>11962200</td>\n",
       "      <td>9010500</td>\n",
       "      <td>5747700</td>\n",
       "      <td>...</td>\n",
       "      <td>3735600</td>\n",
       "      <td>7612500</td>\n",
       "      <td>6875700</td>\n",
       "      <td>12105300</td>\n",
       "      <td>11741700</td>\n",
       "      <td>3844800</td>\n",
       "      <td>7136100</td>\n",
       "      <td>5670000</td>\n",
       "      <td>7592400</td>\n",
       "      <td>6705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>20071229</td>\n",
       "      <td>11388000</td>\n",
       "      <td>11353800</td>\n",
       "      <td>11946900</td>\n",
       "      <td>9662400</td>\n",
       "      <td>10938300</td>\n",
       "      <td>11315100</td>\n",
       "      <td>11402400</td>\n",
       "      <td>10683300</td>\n",
       "      <td>8954400</td>\n",
       "      <td>...</td>\n",
       "      <td>10457700</td>\n",
       "      <td>10316700</td>\n",
       "      <td>10559700</td>\n",
       "      <td>11873100</td>\n",
       "      <td>11369400</td>\n",
       "      <td>10711500</td>\n",
       "      <td>11822100</td>\n",
       "      <td>11594100</td>\n",
       "      <td>9687900</td>\n",
       "      <td>10586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>20071230</td>\n",
       "      <td>12441000</td>\n",
       "      <td>11883300</td>\n",
       "      <td>12409200</td>\n",
       "      <td>12155400</td>\n",
       "      <td>11937600</td>\n",
       "      <td>12314100</td>\n",
       "      <td>12006000</td>\n",
       "      <td>11695800</td>\n",
       "      <td>10249500</td>\n",
       "      <td>...</td>\n",
       "      <td>11152800</td>\n",
       "      <td>12258900</td>\n",
       "      <td>12251100</td>\n",
       "      <td>12754200</td>\n",
       "      <td>12230100</td>\n",
       "      <td>11771100</td>\n",
       "      <td>12162900</td>\n",
       "      <td>11933400</td>\n",
       "      <td>10940700</td>\n",
       "      <td>11281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>20071231</td>\n",
       "      <td>12450300</td>\n",
       "      <td>12104100</td>\n",
       "      <td>12015600</td>\n",
       "      <td>12516600</td>\n",
       "      <td>8480100</td>\n",
       "      <td>9302400</td>\n",
       "      <td>11198100</td>\n",
       "      <td>9687300</td>\n",
       "      <td>9957900</td>\n",
       "      <td>...</td>\n",
       "      <td>10182000</td>\n",
       "      <td>11819700</td>\n",
       "      <td>10313700</td>\n",
       "      <td>12808200</td>\n",
       "      <td>11411700</td>\n",
       "      <td>9657300</td>\n",
       "      <td>11844600</td>\n",
       "      <td>11749800</td>\n",
       "      <td>8191800</td>\n",
       "      <td>11211300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5113 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0     19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1     19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2     19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3     19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4     19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "5     19940106   6639000   6817200   8157900   7673100   3500400   2245200   \n",
       "6     19940107  13244700  12418800  12369900  12873000  12181800   9877800   \n",
       "7     19940108  12927900  12375600  12634500  13066500  11608800  11545200   \n",
       "8     19940109  12600300  11601000  12156000  12464700  10866000  11295300   \n",
       "9     19940110   6406500   3935700  12321900   8164800  11328600  10785000   \n",
       "10    19940111  12743400   7137000  12966300  12774600  12005100  11424900   \n",
       "11    19940112  10453500   7371000  12855300  11448000  11493000  11794200   \n",
       "12    19940113  12985200  12510600  13198500  12726900  12289200  12149100   \n",
       "13    19940114  13080000  12552000  13446600  13026600  12393000  12227700   \n",
       "14    19940115  11826300  11997300  11313300  11793300  10750200  10290600   \n",
       "15    19940116   1974000   1339800   3120600   1058700   7187100   9792900   \n",
       "16    19940117  13541700  13021200  13757100  13432800  12486600  11738100   \n",
       "17    19940118  13673700  13042200  13881000  13586100  13158300  12724200   \n",
       "18    19940119   6796800   8217000  13563300   7861800  13200900  13184700   \n",
       "19    19940120   5658900   4757700   1976100   4926000   3088800  10210500   \n",
       "20    19940121   7073400  10822800   4021800   6464100   4468500  11169300   \n",
       "21    19940122   3354000   2764800   2997900   2726100   8875800  12647400   \n",
       "22    19940123   2579700   2339100   6508500   5418600  12936900  12553500   \n",
       "23    19940124   2387700   2096700   6390000   3164700   9348300   7642200   \n",
       "24    19940125   8390700   8621700  11639100  11404200  11722200  13093500   \n",
       "25    19940126   7326600   8104500  10221900   8622300  12958500   7973100   \n",
       "26    19940127  10743900   8913000  13336500  11616000  12311700   9855300   \n",
       "27    19940128  12812100  13680900  12890700  13834800  13994700  13548000   \n",
       "28    19940129   9065100   5018100  10373700  10304100  13456800  13098600   \n",
       "29    19940130   3954900   7685700   3863100   4220700   5629200   2520000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5083  20071202  12177000  11132400  12620100  11796600  11907000  12604500   \n",
       "5084  20071203  13453800  12737700  12828300  12809700  12264300  12439200   \n",
       "5085  20071204  12525900  10054800  12328200  12057900  12027900  11833500   \n",
       "5086  20071205  10543500   8691900  12200100  11790300   9699300  10742100   \n",
       "5087  20071206   6214500   5707500   6339900   6242400   7813800   8302800   \n",
       "5088  20071207   8417400   6139800   6981000   6928200   2229600   1899600   \n",
       "5089  20071208   1683900   1832700   2153700   1853400   2145600   1705200   \n",
       "5090  20071209   1512300   1394400   1685700   1443900   3815100   5288700   \n",
       "5091  20071210   1729500   1530600   1451400   1747500   1628400   3313200   \n",
       "5092  20071211   1512600    819600   1236900   1648800   2398800   2607600   \n",
       "5093  20071212   2126100   1477500   5262600   2496900   6317400   8295600   \n",
       "5094  20071213   3476400   3016200   8252400   3503700  11037600  11493600   \n",
       "5095  20071214   1262700   1544700    921000   1227000   1224300   1073100   \n",
       "5096  20071215   3074400   1500600   6650400   3486300   7247400   9481800   \n",
       "5097  20071216  12523800  12006300  12210300  12167100  11643000  12363300   \n",
       "5098  20071217  12089700  11742900  12130200  11095800  10793100  11952600   \n",
       "5099  20071218  12300600  11602500  12087900  11866200  11452500  11811900   \n",
       "5100  20071219  12206100  11478000  12020400  11741100  10871400  11946600   \n",
       "5101  20071220  12275700  11530800  12134400  11865600  11614800  11707500   \n",
       "5102  20071221   9737700   9611100   8948400   8915100  10750800   8196000   \n",
       "5103  20071222   1958400   2235300   2364900   2108700   5511000   8464800   \n",
       "5104  20071223  12325200  11827200  11969100  11844900  11698500  10371300   \n",
       "5105  20071224  12225900  11599500  12061500  11865300  11774700  11851800   \n",
       "5106  20071225  12256500  11818200  12093000  11823000  11540400   9601500   \n",
       "5107  20071226   1851300   1551600   3227700   1722900   7181100  11574900   \n",
       "5108  20071227   1408500   1371000   1742400   1389300   1349400   1466400   \n",
       "5109  20071228  10060800   6581700  12027300  11312100   9665100   7993800   \n",
       "5110  20071229  11388000  11353800  11946900   9662400  10938300  11315100   \n",
       "5111  20071230  12441000  11883300  12409200  12155400  11937600  12314100   \n",
       "5112  20071231  12450300  12104100  12015600  12516600   8480100   9302400   \n",
       "\n",
       "          BESS      BIXB      BLAC  ...      VINI      WASH      WATO  \\\n",
       "0     11487900  11182800  10848300  ...  10771800  12116400  11308800   \n",
       "1      9235200   3963300   3318300  ...   4314300  10733400   9154800   \n",
       "2     11895900   4512600   5266500  ...   2976900  11775000  10700400   \n",
       "3     12186600   3212700   8270100  ...   3476400  12159600  11907000   \n",
       "4      6411300   9566100   8009400  ...   6393300  11419500   7334400   \n",
       "5      9719400   6137100   4328700  ...   5257200   6687000   5631600   \n",
       "6     12114300  12175200  11836500  ...  11495100  12486300  12098700   \n",
       "7     12029400  12217500  11505300  ...  11284800  12471300  12072300   \n",
       "8     11937900  10443300   9218400  ...   8755800  12391800  11369400   \n",
       "9     12081600   1873800   9658800  ...   3155100   3879900  11709300   \n",
       "10    12149400   2835600   2574000  ...   3411600  12371400  11973000   \n",
       "11    11780400   6759900   3571800  ...   2370600   8449200  11049000   \n",
       "12    12467100   9930900   9628500  ...   8013000  12463200  12099900   \n",
       "13    12488700  10799100  10770900  ...   6584100  12793800  12221100   \n",
       "14    11184600  12337500  11489700  ...   8565600  11604000  12094800   \n",
       "15     1405500    711900   1133700  ...    640200   1401300   1288500   \n",
       "16    12719700   7774200   9512400  ...  10155300  13079400  12507900   \n",
       "17    12984000   9400800  11339400  ...   8709300  13125900  12694500   \n",
       "18    12884700  10399500  12214500  ...   8323200   7311600  12622800   \n",
       "19     3547200  10607100   5247900  ...   4558800   5358300   5883600   \n",
       "20     5096100  12288000  11910300  ...  11721600   9132300   7908900   \n",
       "21     2881500   3448500   8096400  ...   7866600   2923200   4995000   \n",
       "22     9266400   1948800   6969000  ...   2103000   2631900  10932900   \n",
       "23     5437500   1742100   3155100  ...   1695300   2771400   5281800   \n",
       "24    11776500   7530900   4177200  ...   6538500   9204900   8374800   \n",
       "25    11801400   5955000   7979100  ...   3879300   8131500  11583600   \n",
       "26    13071000  10467300  11326800  ...   6998400  10048800  12608400   \n",
       "27    13434300  11479200   8445900  ...   9601500  13080300  11967900   \n",
       "28    13869900   8671200  12211500  ...  12647400   7510500  13708200   \n",
       "29     5959500  11066400   6694800  ...   9793800   6205500   5371200   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5083  11340600   7562100   8957100  ...   5517600  10997700  10304400   \n",
       "5084  12496800  12446100  11181000  ...  11936700  13204500  12599400   \n",
       "5085  11980200  10532100   9660300  ...  10314000  12219900  12349200   \n",
       "5086  12017700  10608900  10084500  ...   7831500  10752600  11216700   \n",
       "5087   6486600   5260200   2649300  ...   2001300   7011000   6879900   \n",
       "5088   2492100   1154700   1651800  ...   1268700   6294300   1935000   \n",
       "5089   1886100   1878000   1591200  ...   1500000   1853400   1761000   \n",
       "5090   2505600   1129800   3070200  ...   1245900   1558800   2174100   \n",
       "5091   1207800   1018500   2247000  ...   1927800   1509300   1086000   \n",
       "5092   1206600   1074300   1185600  ...   1446900   1281600   1359000   \n",
       "5093   5603100   1671900   1977300  ...   1392000   1669200   2820000   \n",
       "5094   5314800   2861700   3190500  ...   2737500   3606900   4408800   \n",
       "5095   1342200   2343000   1903800  ...   6358800   1354200   1438800   \n",
       "5096   6552000   1593000   6715200  ...   2708700   3045600   7341600   \n",
       "5097  11934600  11676900  10769100  ...  11114700  12364200  13882200   \n",
       "5098  10750200  11702700  10053900  ...  10693800  12084300  11665800   \n",
       "5099  11696700  11232600  10142100  ...  10272300  12104700  11622300   \n",
       "5100  11505600  11184600   9769800  ...   9334800  11976900  11744400   \n",
       "5101  11698800  11053200  10116900  ...  10092600  10224300  11861700   \n",
       "5102  10429500   9107700   8867700  ...   8824800   9253500  11017500   \n",
       "5103   2461800    677100   1535400  ...    597600   1733100   2578200   \n",
       "5104  11761200  11413200  10904400  ...  11088900  12319800  12013800   \n",
       "5105  11720400  11379600  10454100  ...  10995600  12055800  12075900   \n",
       "5106  11670000  11565300  10189500  ...  11088600  12075300  11924400   \n",
       "5107   3165000   3704100   2848500  ...   1938300   2681400   2894400   \n",
       "5108   1245900   1674000    922500  ...   1511100   1220700   1083300   \n",
       "5109  11962200   9010500   5747700  ...   3735600   7612500   6875700   \n",
       "5110  11402400  10683300   8954400  ...  10457700  10316700  10559700   \n",
       "5111  12006000  11695800  10249500  ...  11152800  12258900  12251100   \n",
       "5112  11198100   9687300   9957900  ...  10182000  11819700  10313700   \n",
       "\n",
       "          WAUR      WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0     12361800  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1     12041400   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2     12687300  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3     12953100  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4     10178700   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "5      7990500   9402600   8463600   7323900   7113600   2124600   3324300  \n",
       "6     13191300  11855100  11494800  12620400  12380700  10944900  11403600  \n",
       "7     10429500  11939100  11280300  12419700  12225900  11029200  11268900  \n",
       "8     11324400  11498700   9737100  11985000  11454900  10518300   8577300  \n",
       "9      3808500  11526900   2064300   7641000   2282400  10859700   2520600  \n",
       "10    12872400  11970900   1908000   1998300   1577100  11256600   2923500  \n",
       "11     8650500  11623200   5994300   4441500   6324900  10424100   7131000  \n",
       "12    13268400  12205500  10277100   9863700   8588400  11655300  10062300  \n",
       "13    12990000  12166800   5569500  11459400  11072100  11614200   9288600  \n",
       "14    11343900  10906500  11029500  12093300  12119100  10898700  10905000  \n",
       "15     1257600   1125300    739800    756900    469500   3777300    748500  \n",
       "16    13869000  12336300  11681100   9247500  10576200  11586900   9893100  \n",
       "17    13959900  12517800  12465000  12720600  10228200  12076800  10843800  \n",
       "18     5567400  12464700  14057400  11255400  12093900  12372300  10474200  \n",
       "19     2109900   4254900   6372900   8222700   9356400   6758100   7672200  \n",
       "20     3649500   6578100  11701200  12758400  12867300   5543700  11705100  \n",
       "21     1614900   3245400   3254100   3469800   2306100   9792600   5962200  \n",
       "22     2564700   8967900   1685400   2399700   2453400  12991800   3377100  \n",
       "23     3616500   4453500   1114200   2427300   1808400   9188100   2060700  \n",
       "24    12530400  10918200   6499500   7495200   6218700  12260100   7261800  \n",
       "25     8124000  11062800   3655800   6165600   3794100  11837400   6168900  \n",
       "26    11251800  12623100   5641200   5928900   4412100  12267900  11280000  \n",
       "27    12759300  12837300   9696300  12404400  10782600  11917500   8725500  \n",
       "28     5198700  13585800   7622400   5036400   4905000  12272700  11245500  \n",
       "29     6331800   6069600  12349500   9744900  10736400   5056800   7212000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5083  12716700  11124600   3845700   8625000   7135500  10745100   7718700  \n",
       "5084  13403400  12964500  12735900  13133100  12933900  11300400  12414000  \n",
       "5085  12531600  12132600  10984800  12141600  11658900  10941300  10495500  \n",
       "5086  10968900  12183600   9241500   8371200   8967900   9399900  11064900  \n",
       "5087   7605000   7298700   3600900   5550900   5042700   6977700   3435000  \n",
       "5088  10633800   3195900   1305000   3027000   2423100   1835700   1757400  \n",
       "5089   1690200   2251500   1556100   4414800   5186100   1826400   1762500  \n",
       "5090   1551300   2354400   1321500   1269300   1362300   3344400   1480200  \n",
       "5091   1509000   1559400   1415400   1537200   1957500   2050800   2024400  \n",
       "5092   1127400   1397700   1154700   1028100   2350500   1860900   1549800  \n",
       "5093   2087100   4227900   1423800   1272300    916500   4206300   1726800  \n",
       "5094   3242400   4901700   4254600   3825000   3884700  10216500   3342000  \n",
       "5095   1148100   1734000   5733600   2664600   4408500   1123800   1885800  \n",
       "5096   3783000   6811500   1209300   1220700   1414500   5080800   4438800  \n",
       "5097  12572400  12260400  11368200  12465300  11695800  11119800  11652000  \n",
       "5098  11436600  11346600  11470500  12253200  11610000  10333500  10910400  \n",
       "5099  12438300  10248000   9291600  11736900  10968300  10684800  10991700  \n",
       "5100  12171600  11703000  11015700  11782800  11303100  10367700   9920100  \n",
       "5101  12374400  11761500   8301000   8917500   7947300  10679100  11154000  \n",
       "5102  10610400  10148700   8847600   9985200   9971700   9173100   9162900  \n",
       "5103   1802400   2522700   2739600   6989100   7298400   4391700    902400  \n",
       "5104  12653400  12035100  11593500  12151200  11898000   9895800  11434800  \n",
       "5105  12308100  11938200  11466300  11734800  11522100  10793700  11464800  \n",
       "5106  12457800  11772600  11641200  12170400  11836800  10649700  11517300  \n",
       "5107   3147300   3001200   1406400   1595400   1460700   6298200   2382300  \n",
       "5108   1887900   1467900   1404000   1510500   1747500   1079100   1100400  \n",
       "5109  12105300  11741700   3844800   7136100   5670000   7592400   6705900  \n",
       "5110  11873100  11369400  10711500  11822100  11594100   9687900  10586400  \n",
       "5111  12754200  12230100  11771100  12162900  11933400  10940700  11281800  \n",
       "5112  12808200  11411700   9657300  11844600  11749800   8191800  11211300  \n",
       "\n",
       "[5113 rows x 99 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['ACME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(df, freq='D', split_type = 'train', cols_to_use = ['ACME']):\n",
    "    rt_set = []\n",
    "    \n",
    "    # use 70% for training\n",
    "    if split_type == 'train':\n",
    "        lower_bound = 0\n",
    "        upper_bound = round(df.shape[0] * .7)\n",
    "        \n",
    "    # use 15% for validation\n",
    "    elif split_type == 'validation':\n",
    "        lower_bound = round(df.shape[0] * .7)\n",
    "        upper_bound = round(df.shape[0] * .85)\n",
    "        \n",
    "    # use 15% for test\n",
    "    elif split_type == 'test':\n",
    "        lower_bound = round(df.shape[0] * .85)\n",
    "        upper_bound = df.shape[0]\n",
    "            \n",
    "    # loop through columns you want to use\n",
    "    for h in list(df):\n",
    "        if h in cols_to_use:\n",
    "            \n",
    "            target_column = df[h].values.tolist()[lower_bound:upper_bound]\n",
    "            \n",
    "            date_str = str(df.iloc[0]['Date'])\n",
    "            \n",
    "            year = date_str[0:4]\n",
    "            month = date_str[4:6]\n",
    "            date = date_str[7:]\n",
    "                                                \n",
    "            start_dataset = pd.Timestamp(\"{}-{}-{} 00:00:00\".format(year, month, date, freq=freq))\n",
    "                        \n",
    "            # create a new json object for each column\n",
    "            json_obj = {'start': str(start_dataset),\n",
    "                       'target':target_column}\n",
    "    \n",
    "            rt_set.append(json_obj)\n",
    "    \n",
    "    return rt_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8f89d1b267cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'freq' is not defined"
     ]
    }
   ],
   "source": [
    "train_set = get_split(df, freq)\n",
    "test_set = get_split(df, freq, split_type = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_split(df, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1ea2761d96d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '1994-01-01 00:00:00',\n",
       "  'target': [12384900,\n",
       "   11908500,\n",
       "   12470700,\n",
       "   12725400,\n",
       "   10894800,\n",
       "   6639000,\n",
       "   13244700,\n",
       "   12927900,\n",
       "   12600300,\n",
       "   6406500,\n",
       "   12743400,\n",
       "   10453500,\n",
       "   12985200,\n",
       "   13080000,\n",
       "   11826300,\n",
       "   1974000,\n",
       "   13541700,\n",
       "   13673700,\n",
       "   6796800,\n",
       "   5658900,\n",
       "   7073400,\n",
       "   3354000,\n",
       "   2579700,\n",
       "   2387700,\n",
       "   8390700,\n",
       "   7326600,\n",
       "   10743900,\n",
       "   12812100,\n",
       "   9065100,\n",
       "   3954900,\n",
       "   15324900,\n",
       "   15259800,\n",
       "   15264900,\n",
       "   14489400,\n",
       "   10506600,\n",
       "   15325500,\n",
       "   15568500,\n",
       "   9486600,\n",
       "   3012000,\n",
       "   7137000,\n",
       "   15829800,\n",
       "   16185000,\n",
       "   16245600,\n",
       "   17941800,\n",
       "   11963700,\n",
       "   16709700,\n",
       "   16625100,\n",
       "   16555200,\n",
       "   7427700,\n",
       "   2082000,\n",
       "   17161200,\n",
       "   2443500,\n",
       "   3140100,\n",
       "   18781200,\n",
       "   18937200,\n",
       "   19162500,\n",
       "   19760400,\n",
       "   9084000,\n",
       "   1798200,\n",
       "   3118800,\n",
       "   19929900,\n",
       "   17554200,\n",
       "   20229300,\n",
       "   20141400,\n",
       "   14808000,\n",
       "   8989500,\n",
       "   1375800,\n",
       "   16026600,\n",
       "   21753900,\n",
       "   20451900,\n",
       "   6376500,\n",
       "   21710400,\n",
       "   22170600,\n",
       "   21982200,\n",
       "   22007400,\n",
       "   20682600,\n",
       "   22692000,\n",
       "   20535600,\n",
       "   13570500,\n",
       "   23943000,\n",
       "   21197700,\n",
       "   21195900,\n",
       "   12319800,\n",
       "   18210000,\n",
       "   2527500,\n",
       "   18444600,\n",
       "   24110100,\n",
       "   24049800,\n",
       "   24738000,\n",
       "   24766200,\n",
       "   24579000,\n",
       "   21041400,\n",
       "   25072200,\n",
       "   15807000,\n",
       "   5219400,\n",
       "   26026500,\n",
       "   25310400,\n",
       "   22787100,\n",
       "   19146600,\n",
       "   6097800,\n",
       "   11318100,\n",
       "   26455500,\n",
       "   26019900,\n",
       "   22852200,\n",
       "   25971600,\n",
       "   27102900,\n",
       "   27312000,\n",
       "   27440100,\n",
       "   25749000,\n",
       "   19738200,\n",
       "   16588800,\n",
       "   25292400,\n",
       "   24645000,\n",
       "   16526100,\n",
       "   11119200,\n",
       "   25897200,\n",
       "   6033300,\n",
       "   22697100,\n",
       "   2339700,\n",
       "   19995000,\n",
       "   24334200,\n",
       "   3290100,\n",
       "   12036900,\n",
       "   22262100,\n",
       "   26693100,\n",
       "   22156500,\n",
       "   14926800,\n",
       "   23795400,\n",
       "   7746600,\n",
       "   14021400,\n",
       "   9796800,\n",
       "   20584500,\n",
       "   20631000,\n",
       "   17381100,\n",
       "   28557300,\n",
       "   24748500,\n",
       "   19879500,\n",
       "   25688100,\n",
       "   27304500,\n",
       "   28724400,\n",
       "   28100700,\n",
       "   27672000,\n",
       "   27557700,\n",
       "   23702100,\n",
       "   24495300,\n",
       "   22462200,\n",
       "   10118400,\n",
       "   8797200,\n",
       "   18129900,\n",
       "   25717800,\n",
       "   29058300,\n",
       "   28208400,\n",
       "   26998800,\n",
       "   25136100,\n",
       "   26884200,\n",
       "   27795000,\n",
       "   25331400,\n",
       "   26971500,\n",
       "   27443100,\n",
       "   19671900,\n",
       "   16786200,\n",
       "   9270300,\n",
       "   28360500,\n",
       "   28767300,\n",
       "   25071000,\n",
       "   11430300,\n",
       "   19471500,\n",
       "   24864900,\n",
       "   27896700,\n",
       "   24625200,\n",
       "   25742100,\n",
       "   26133300,\n",
       "   27492000,\n",
       "   26890200,\n",
       "   27198300,\n",
       "   28450200,\n",
       "   28235100,\n",
       "   28850100,\n",
       "   19938300,\n",
       "   24283500,\n",
       "   27440100,\n",
       "   26653800,\n",
       "   27022500,\n",
       "   27841800,\n",
       "   28008300,\n",
       "   27493800,\n",
       "   22945500,\n",
       "   25452600,\n",
       "   20948700,\n",
       "   13686000,\n",
       "   22726800,\n",
       "   20129100,\n",
       "   18383400,\n",
       "   26090100,\n",
       "   24473100,\n",
       "   25812600,\n",
       "   26548500,\n",
       "   25943100,\n",
       "   27840300,\n",
       "   28393500,\n",
       "   27226800,\n",
       "   16731600,\n",
       "   28234800,\n",
       "   20788200,\n",
       "   23977500,\n",
       "   18775800,\n",
       "   23494800,\n",
       "   28453200,\n",
       "   27543900,\n",
       "   24582600,\n",
       "   23734800,\n",
       "   26240400,\n",
       "   26601900,\n",
       "   22685100,\n",
       "   25404600,\n",
       "   16614000,\n",
       "   26692500,\n",
       "   25923600,\n",
       "   26162700,\n",
       "   25593600,\n",
       "   26544000,\n",
       "   24890400,\n",
       "   25350300,\n",
       "   24976200,\n",
       "   24972600,\n",
       "   20469000,\n",
       "   19839900,\n",
       "   27149700,\n",
       "   21291000,\n",
       "   23178000,\n",
       "   16622100,\n",
       "   12325200,\n",
       "   25245600,\n",
       "   25217400,\n",
       "   23240400,\n",
       "   20825400,\n",
       "   19134900,\n",
       "   22869000,\n",
       "   24476400,\n",
       "   24480900,\n",
       "   22998900,\n",
       "   22580100,\n",
       "   7375500,\n",
       "   8647200,\n",
       "   13092300,\n",
       "   17082900,\n",
       "   23325000,\n",
       "   4996800,\n",
       "   18984300,\n",
       "   19464000,\n",
       "   9694800,\n",
       "   17983800,\n",
       "   21319200,\n",
       "   21116100,\n",
       "   20686200,\n",
       "   19528500,\n",
       "   17695500,\n",
       "   9372900,\n",
       "   18094500,\n",
       "   22605900,\n",
       "   21951900,\n",
       "   22182300,\n",
       "   21162600,\n",
       "   18515100,\n",
       "   17017200,\n",
       "   10061100,\n",
       "   20774700,\n",
       "   19355700,\n",
       "   21173400,\n",
       "   20515200,\n",
       "   20215200,\n",
       "   20486700,\n",
       "   20458200,\n",
       "   19699200,\n",
       "   17874900,\n",
       "   19002900,\n",
       "   14229000,\n",
       "   15439800,\n",
       "   15178200,\n",
       "   1263900,\n",
       "   18475800,\n",
       "   20017200,\n",
       "   19548900,\n",
       "   19379400,\n",
       "   19067400,\n",
       "   14892300,\n",
       "   6781500,\n",
       "   1397400,\n",
       "   3731700,\n",
       "   3660300,\n",
       "   11928000,\n",
       "   16068900,\n",
       "   12713700,\n",
       "   14263200,\n",
       "   17064900,\n",
       "   16020000,\n",
       "   5673000,\n",
       "   11145000,\n",
       "   16787100,\n",
       "   14185500,\n",
       "   8224800,\n",
       "   4938000,\n",
       "   7165800,\n",
       "   9065400,\n",
       "   14779800,\n",
       "   10543800,\n",
       "   12386400,\n",
       "   1155000,\n",
       "   10702500,\n",
       "   15086100,\n",
       "   13414500,\n",
       "   12559800,\n",
       "   6374100,\n",
       "   14383200,\n",
       "   11226600,\n",
       "   5317800,\n",
       "   1571700,\n",
       "   6075600,\n",
       "   12772800,\n",
       "   13809900,\n",
       "   5867700,\n",
       "   6563400,\n",
       "   1566000,\n",
       "   10331100,\n",
       "   13282800,\n",
       "   4209900,\n",
       "   8178900,\n",
       "   4483800,\n",
       "   1877400,\n",
       "   3474900,\n",
       "   12776100,\n",
       "   13005600,\n",
       "   12803400,\n",
       "   12643200,\n",
       "   12497400,\n",
       "   4855800,\n",
       "   5655900,\n",
       "   10750800,\n",
       "   3524400,\n",
       "   1669800,\n",
       "   9900300,\n",
       "   1317000,\n",
       "   8569800,\n",
       "   12075900,\n",
       "   6894300,\n",
       "   9343500,\n",
       "   3021300,\n",
       "   3079800,\n",
       "   2593500,\n",
       "   8978100,\n",
       "   11629500,\n",
       "   11718000,\n",
       "   10431600,\n",
       "   11551500,\n",
       "   11322900,\n",
       "   11415600,\n",
       "   9142200,\n",
       "   9344700,\n",
       "   11875800,\n",
       "   11714100,\n",
       "   9189900,\n",
       "   1836300,\n",
       "   7769400,\n",
       "   4728900,\n",
       "   2759400,\n",
       "   11638800,\n",
       "   8446200,\n",
       "   2419500,\n",
       "   10223100,\n",
       "   5783700,\n",
       "   10042200,\n",
       "   12258900,\n",
       "   11873700,\n",
       "   12007800,\n",
       "   11637300,\n",
       "   12102900,\n",
       "   8953200,\n",
       "   11190600,\n",
       "   12856500,\n",
       "   12591000,\n",
       "   6436800,\n",
       "   12836700,\n",
       "   4120200,\n",
       "   13568700,\n",
       "   13328700,\n",
       "   7196400,\n",
       "   6755700,\n",
       "   14090100,\n",
       "   12294000,\n",
       "   13171800,\n",
       "   2272200,\n",
       "   13686000,\n",
       "   3060900,\n",
       "   3281700,\n",
       "   14986200,\n",
       "   11649600,\n",
       "   14196900,\n",
       "   14638500,\n",
       "   14750700,\n",
       "   15240000,\n",
       "   15561900,\n",
       "   15117000,\n",
       "   13979100,\n",
       "   16514700,\n",
       "   15756600,\n",
       "   13722000,\n",
       "   4941300,\n",
       "   4258500,\n",
       "   3673500,\n",
       "   2652900,\n",
       "   10190100,\n",
       "   3336600,\n",
       "   17025300,\n",
       "   14037300,\n",
       "   17593200,\n",
       "   17688600,\n",
       "   18066000,\n",
       "   14121600,\n",
       "   17809500,\n",
       "   18134100,\n",
       "   11269800,\n",
       "   13512600,\n",
       "   8999400,\n",
       "   5670300,\n",
       "   3335100,\n",
       "   3901500,\n",
       "   9906300,\n",
       "   2948400,\n",
       "   3222000,\n",
       "   7547100,\n",
       "   21142200,\n",
       "   21361200,\n",
       "   21711000,\n",
       "   19547700,\n",
       "   10179900,\n",
       "   3895500,\n",
       "   3195900,\n",
       "   7480800,\n",
       "   9319200,\n",
       "   18545700,\n",
       "   20613000,\n",
       "   21089400,\n",
       "   20255100,\n",
       "   20687100,\n",
       "   21951300,\n",
       "   20767200,\n",
       "   22123200,\n",
       "   18582600,\n",
       "   5984100,\n",
       "   20608800,\n",
       "   24244200,\n",
       "   5724900,\n",
       "   10911600,\n",
       "   16576800,\n",
       "   19764900,\n",
       "   23928300,\n",
       "   24205200,\n",
       "   6724800,\n",
       "   16292700,\n",
       "   16354800,\n",
       "   19264200,\n",
       "   21360300,\n",
       "   25013100,\n",
       "   22652400,\n",
       "   4367100,\n",
       "   26521200,\n",
       "   26032200,\n",
       "   25923900,\n",
       "   23751600,\n",
       "   20006700,\n",
       "   11379000,\n",
       "   4283100,\n",
       "   26893500,\n",
       "   12894300,\n",
       "   26231700,\n",
       "   26613000,\n",
       "   1901100,\n",
       "   14388600,\n",
       "   19898100,\n",
       "   26872800,\n",
       "   21458700,\n",
       "   21837000,\n",
       "   17120400,\n",
       "   22131900,\n",
       "   8935200,\n",
       "   19382400,\n",
       "   15155100,\n",
       "   9457800,\n",
       "   27548700,\n",
       "   10790100,\n",
       "   8543100,\n",
       "   4237500,\n",
       "   25912500,\n",
       "   27738600,\n",
       "   28324200,\n",
       "   26664300,\n",
       "   13883700,\n",
       "   28163100,\n",
       "   28284600,\n",
       "   19408200,\n",
       "   9631800,\n",
       "   23997000,\n",
       "   19948200,\n",
       "   28560000,\n",
       "   25789500,\n",
       "   23130000,\n",
       "   25545000,\n",
       "   6018600,\n",
       "   5756400,\n",
       "   4888200,\n",
       "   15797400,\n",
       "   26442000,\n",
       "   28575000,\n",
       "   9979800,\n",
       "   12695100,\n",
       "   19260900,\n",
       "   27844500,\n",
       "   19807800,\n",
       "   15523200,\n",
       "   9855000,\n",
       "   22791300,\n",
       "   26081100,\n",
       "   26122500,\n",
       "   18433800,\n",
       "   8227500,\n",
       "   3114900,\n",
       "   27494700,\n",
       "   29208000,\n",
       "   28674900,\n",
       "   26582400,\n",
       "   28925700,\n",
       "   28974300,\n",
       "   25713600,\n",
       "   27640500,\n",
       "   27342900,\n",
       "   25633800,\n",
       "   27424500,\n",
       "   27273600,\n",
       "   27977100,\n",
       "   25254600,\n",
       "   28752000,\n",
       "   29009700,\n",
       "   19721100,\n",
       "   28479000,\n",
       "   21432000,\n",
       "   12062100,\n",
       "   28757400,\n",
       "   15855000,\n",
       "   24624900,\n",
       "   28672200,\n",
       "   29151000,\n",
       "   28810800,\n",
       "   28404300,\n",
       "   28475100,\n",
       "   28575300,\n",
       "   28568700,\n",
       "   28552800,\n",
       "   28148400,\n",
       "   27450900,\n",
       "   25950300,\n",
       "   26799900,\n",
       "   27184800,\n",
       "   19395900,\n",
       "   25849200,\n",
       "   14811600,\n",
       "   19250100,\n",
       "   27675600,\n",
       "   22810800,\n",
       "   15826800,\n",
       "   25006500,\n",
       "   27005700,\n",
       "   27107700,\n",
       "   27685200,\n",
       "   27774300,\n",
       "   26032500,\n",
       "   24401400,\n",
       "   10391400,\n",
       "   10551900,\n",
       "   12291000,\n",
       "   13361100,\n",
       "   19368600,\n",
       "   22162500,\n",
       "   25746000,\n",
       "   26504400,\n",
       "   24608700,\n",
       "   26658600,\n",
       "   26454300,\n",
       "   25844400,\n",
       "   25609500,\n",
       "   25702800,\n",
       "   25863000,\n",
       "   19226400,\n",
       "   21600300,\n",
       "   7074600,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   12588000,\n",
       "   22174500,\n",
       "   23222400,\n",
       "   23051400,\n",
       "   22851000,\n",
       "   18495300,\n",
       "   15765600,\n",
       "   22423500,\n",
       "   20672100,\n",
       "   15087300,\n",
       "   14507100,\n",
       "   14212500,\n",
       "   14170800,\n",
       "   8084100,\n",
       "   12435000,\n",
       "   21881400,\n",
       "   12464400,\n",
       "   3077100,\n",
       "   15755400,\n",
       "   9270300,\n",
       "   9249600,\n",
       "   8933400,\n",
       "   7437300,\n",
       "   2196300,\n",
       "   21818400,\n",
       "   22196100,\n",
       "   7923000,\n",
       "   3485400,\n",
       "   13438200,\n",
       "   15196800,\n",
       "   19957500,\n",
       "   18114000,\n",
       "   19177500,\n",
       "   21003300,\n",
       "   2819400,\n",
       "   20206800,\n",
       "   20023800,\n",
       "   20127600,\n",
       "   20338200,\n",
       "   20002500,\n",
       "   19054500,\n",
       "   16784100,\n",
       "   18621000,\n",
       "   18509700,\n",
       "   18436800,\n",
       "   17403000,\n",
       "   19081800,\n",
       "   18750300,\n",
       "   18604500,\n",
       "   18344400,\n",
       "   17430900,\n",
       "   15885000,\n",
       "   18779400,\n",
       "   18177900,\n",
       "   17150400,\n",
       "   18234000,\n",
       "   17807100,\n",
       "   16705200,\n",
       "   16075500,\n",
       "   16043100,\n",
       "   17052600,\n",
       "   11799600,\n",
       "   4476900,\n",
       "   3221400,\n",
       "   15126000,\n",
       "   8430600,\n",
       "   16055400,\n",
       "   11535000,\n",
       "   15625500,\n",
       "   7736700,\n",
       "   15237000,\n",
       "   14382900,\n",
       "   14331000,\n",
       "   12407100,\n",
       "   15802500,\n",
       "   14798100,\n",
       "   11571900,\n",
       "   13949100,\n",
       "   14046900,\n",
       "   13939500,\n",
       "   11465400,\n",
       "   14238000,\n",
       "   13506000,\n",
       "   12017400,\n",
       "   13279800,\n",
       "   11569500,\n",
       "   8440500,\n",
       "   13403700,\n",
       "   12765900,\n",
       "   11378700,\n",
       "   12900300,\n",
       "   13447800,\n",
       "   13316100,\n",
       "   13089000,\n",
       "   12752100,\n",
       "   12610200,\n",
       "   12292500,\n",
       "   6360000,\n",
       "   11767200,\n",
       "   10440900,\n",
       "   10087500,\n",
       "   5208600,\n",
       "   12792600,\n",
       "   13362000,\n",
       "   12766200,\n",
       "   5777100,\n",
       "   8212500,\n",
       "   11112000,\n",
       "   9515700,\n",
       "   1278000,\n",
       "   2028600,\n",
       "   1035900,\n",
       "   5277300,\n",
       "   2822100,\n",
       "   2565300,\n",
       "   2838300,\n",
       "   11978700,\n",
       "   12140100,\n",
       "   12225300,\n",
       "   10570500,\n",
       "   11488500,\n",
       "   11821500,\n",
       "   7014900,\n",
       "   7465200,\n",
       "   5506800,\n",
       "   822000,\n",
       "   6998700,\n",
       "   11958300,\n",
       "   11965500,\n",
       "   2789100,\n",
       "   11355600,\n",
       "   13446900,\n",
       "   12854100,\n",
       "   12792900,\n",
       "   10662000,\n",
       "   13326300,\n",
       "   12900300,\n",
       "   12773400,\n",
       "   13383600,\n",
       "   9469800,\n",
       "   11310300,\n",
       "   3250200,\n",
       "   11577900,\n",
       "   14604600,\n",
       "   14022900,\n",
       "   14249100,\n",
       "   6964800,\n",
       "   13493400,\n",
       "   14610300,\n",
       "   14285100,\n",
       "   15641700,\n",
       "   15606000,\n",
       "   12616800,\n",
       "   12141900,\n",
       "   13120500,\n",
       "   8646600,\n",
       "   7545600,\n",
       "   7412700,\n",
       "   15133200,\n",
       "   16920600,\n",
       "   15951000,\n",
       "   12057300,\n",
       "   13988400,\n",
       "   13862700,\n",
       "   16025700,\n",
       "   15752100,\n",
       "   17996400,\n",
       "   17230200,\n",
       "   17768700,\n",
       "   17277600,\n",
       "   17223600,\n",
       "   19263600,\n",
       "   18876900,\n",
       "   13911900,\n",
       "   17771400,\n",
       "   16755300,\n",
       "   17531700,\n",
       "   16734000,\n",
       "   14136300,\n",
       "   12174300,\n",
       "   9288600,\n",
       "   15692100,\n",
       "   10255800,\n",
       "   19694100,\n",
       "   7878900,\n",
       "   18352200,\n",
       "   21339000,\n",
       "   21639600,\n",
       "   12864000,\n",
       "   15048000,\n",
       "   11277000,\n",
       "   22881600,\n",
       "   22824000,\n",
       "   18653400,\n",
       "   23020800,\n",
       "   19395900,\n",
       "   22335600,\n",
       "   21747900,\n",
       "   19554000,\n",
       "   22115700,\n",
       "   22285200,\n",
       "   18729600,\n",
       "   20499000,\n",
       "   24346200,\n",
       "   25682400,\n",
       "   24799500,\n",
       "   23883300,\n",
       "   18697800,\n",
       "   16082400,\n",
       "   24943200,\n",
       "   13857000,\n",
       "   2699400,\n",
       "   9387300,\n",
       "   21798600,\n",
       "   17839800,\n",
       "   25787400,\n",
       "   25883100,\n",
       "   25925700,\n",
       "   19628400,\n",
       "   24213600,\n",
       "   10218900,\n",
       "   26698500,\n",
       "   20871600,\n",
       "   23700600,\n",
       "   25429200,\n",
       "   24438300,\n",
       "   24566400,\n",
       "   9887400,\n",
       "   26411100,\n",
       "   19389300,\n",
       "   27317700,\n",
       "   27634800,\n",
       "   26389500,\n",
       "   27096300,\n",
       "   26770800,\n",
       "   24842100,\n",
       "   12916200,\n",
       "   4887600,\n",
       "   28218300,\n",
       "   26941200,\n",
       "   22939200,\n",
       "   28478100,\n",
       "   24927300,\n",
       "   4468800,\n",
       "   21514800,\n",
       "   29399100,\n",
       "   28754700,\n",
       "   26002200,\n",
       "   25549200,\n",
       "   26879400,\n",
       "   21385200,\n",
       "   26802600,\n",
       "   22538100,\n",
       "   25273500,\n",
       "   22819800,\n",
       "   9448800,\n",
       "   16046400,\n",
       "   26114100,\n",
       "   13122300,\n",
       "   25675200,\n",
       "   26760000,\n",
       "   27743400,\n",
       "   28213200,\n",
       "   26897700,\n",
       "   24087300,\n",
       "   26456700,\n",
       "   24126900,\n",
       "   27547200,\n",
       "   26994900,\n",
       "   26275500,\n",
       "   22691400,\n",
       "   13176000,\n",
       "   29806200,\n",
       "   28315200,\n",
       "   24978900,\n",
       "   14842800,\n",
       "   18921000,\n",
       "   20390700,\n",
       "   28181100,\n",
       "   25904700,\n",
       "   27827400,\n",
       "   28189800,\n",
       "   23216700,\n",
       "   27950700,\n",
       "   28792200,\n",
       "   30448200,\n",
       "   28649700,\n",
       "   29091300,\n",
       "   18698400,\n",
       "   24875100,\n",
       "   18720600,\n",
       "   24807300,\n",
       "   22787400,\n",
       "   26480400,\n",
       "   29443200,\n",
       "   28404900,\n",
       "   28857600,\n",
       "   29971200,\n",
       "   28769100,\n",
       "   29212800,\n",
       "   26916000,\n",
       "   24231900,\n",
       "   24450600,\n",
       "   27282300,\n",
       "   26918400,\n",
       "   25914900,\n",
       "   28919400,\n",
       "   26743200,\n",
       "   24239400,\n",
       "   25949700,\n",
       "   27936600,\n",
       "   28534500,\n",
       "   29109900,\n",
       "   25849500,\n",
       "   14211900,\n",
       "   11603100,\n",
       "   3706500,\n",
       "   9566400,\n",
       "   17511900,\n",
       "   22505700,\n",
       "   23384400,\n",
       "   28444500,\n",
       "   25469700,\n",
       "   26007600,\n",
       "   27965700,\n",
       "   28198200,\n",
       "   24504900,\n",
       "   27907200,\n",
       "   26326800,\n",
       "   27148200,\n",
       "   19832400,\n",
       "   27982200,\n",
       "   10986000,\n",
       "   19271700,\n",
       "   27959100,\n",
       "   16704900,\n",
       "   7792500,\n",
       "   15494700,\n",
       "   21480900,\n",
       "   25908000,\n",
       "   25900200,\n",
       "   27015000,\n",
       "   24905100,\n",
       "   27042300,\n",
       "   25115400,\n",
       "   24550500,\n",
       "   13608000,\n",
       "   14948700,\n",
       "   20362500,\n",
       "   16618800,\n",
       "   25763700,\n",
       "   24162900,\n",
       "   11899500,\n",
       "   15970800,\n",
       "   15692100,\n",
       "   24661800,\n",
       "   25511100,\n",
       "   24817200,\n",
       "   23154600,\n",
       "   23400900,\n",
       "   22486800,\n",
       "   20285100,\n",
       "   5614200,\n",
       "   16320000,\n",
       "   11110800,\n",
       "   8311500,\n",
       "   16836000,\n",
       "   21135900,\n",
       "   12476100,\n",
       "   21132300,\n",
       "   21886200,\n",
       "   13887600,\n",
       "   19346700,\n",
       "   21130200,\n",
       "   21353100,\n",
       "   20538600,\n",
       "   23316300,\n",
       "   23743500,\n",
       "   23105400,\n",
       "   22622100,\n",
       "   16967400,\n",
       "   15777000,\n",
       "   13072800,\n",
       "   15902100,\n",
       "   16818300,\n",
       "   20723400,\n",
       "   3980100,\n",
       "   15718500,\n",
       "   16512300,\n",
       "   22661400,\n",
       "   16862100,\n",
       "   17477700,\n",
       "   4424100,\n",
       "   5251800,\n",
       "   6655500,\n",
       "   ...]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = get_split(df, 'D', split_type = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12384900,\n",
       " 11908500,\n",
       " 12470700,\n",
       " 12725400,\n",
       " 10894800,\n",
       " 6639000,\n",
       " 13244700,\n",
       " 12927900,\n",
       " 12600300,\n",
       " 6406500,\n",
       " 12743400,\n",
       " 10453500,\n",
       " 12985200,\n",
       " 13080000,\n",
       " 11826300,\n",
       " 1974000,\n",
       " 13541700,\n",
       " 13673700,\n",
       " 6796800,\n",
       " 5658900,\n",
       " 7073400,\n",
       " 3354000,\n",
       " 2579700,\n",
       " 2387700,\n",
       " 8390700,\n",
       " 7326600,\n",
       " 10743900,\n",
       " 12812100,\n",
       " 9065100,\n",
       " 3954900,\n",
       " 15324900,\n",
       " 15259800,\n",
       " 15264900,\n",
       " 14489400,\n",
       " 10506600,\n",
       " 15325500,\n",
       " 15568500,\n",
       " 9486600,\n",
       " 3012000,\n",
       " 7137000,\n",
       " 15829800,\n",
       " 16185000,\n",
       " 16245600,\n",
       " 17941800,\n",
       " 11963700,\n",
       " 16709700,\n",
       " 16625100,\n",
       " 16555200,\n",
       " 7427700,\n",
       " 2082000,\n",
       " 17161200,\n",
       " 2443500,\n",
       " 3140100,\n",
       " 18781200,\n",
       " 18937200,\n",
       " 19162500,\n",
       " 19760400,\n",
       " 9084000,\n",
       " 1798200,\n",
       " 3118800,\n",
       " 19929900,\n",
       " 17554200,\n",
       " 20229300,\n",
       " 20141400,\n",
       " 14808000,\n",
       " 8989500,\n",
       " 1375800,\n",
       " 16026600,\n",
       " 21753900,\n",
       " 20451900,\n",
       " 6376500,\n",
       " 21710400,\n",
       " 22170600,\n",
       " 21982200,\n",
       " 22007400,\n",
       " 20682600,\n",
       " 22692000,\n",
       " 20535600,\n",
       " 13570500,\n",
       " 23943000,\n",
       " 21197700,\n",
       " 21195900,\n",
       " 12319800,\n",
       " 18210000,\n",
       " 2527500,\n",
       " 18444600,\n",
       " 24110100,\n",
       " 24049800,\n",
       " 24738000,\n",
       " 24766200,\n",
       " 24579000,\n",
       " 21041400,\n",
       " 25072200,\n",
       " 15807000,\n",
       " 5219400,\n",
       " 26026500,\n",
       " 25310400,\n",
       " 22787100,\n",
       " 19146600,\n",
       " 6097800,\n",
       " 11318100,\n",
       " 26455500,\n",
       " 26019900,\n",
       " 22852200,\n",
       " 25971600,\n",
       " 27102900,\n",
       " 27312000,\n",
       " 27440100,\n",
       " 25749000,\n",
       " 19738200,\n",
       " 16588800,\n",
       " 25292400,\n",
       " 24645000,\n",
       " 16526100,\n",
       " 11119200,\n",
       " 25897200,\n",
       " 6033300,\n",
       " 22697100,\n",
       " 2339700,\n",
       " 19995000,\n",
       " 24334200,\n",
       " 3290100,\n",
       " 12036900,\n",
       " 22262100,\n",
       " 26693100,\n",
       " 22156500,\n",
       " 14926800,\n",
       " 23795400,\n",
       " 7746600,\n",
       " 14021400,\n",
       " 9796800,\n",
       " 20584500,\n",
       " 20631000,\n",
       " 17381100,\n",
       " 28557300,\n",
       " 24748500,\n",
       " 19879500,\n",
       " 25688100,\n",
       " 27304500,\n",
       " 28724400,\n",
       " 28100700,\n",
       " 27672000,\n",
       " 27557700,\n",
       " 23702100,\n",
       " 24495300,\n",
       " 22462200,\n",
       " 10118400,\n",
       " 8797200,\n",
       " 18129900,\n",
       " 25717800,\n",
       " 29058300,\n",
       " 28208400,\n",
       " 26998800,\n",
       " 25136100,\n",
       " 26884200,\n",
       " 27795000,\n",
       " 25331400,\n",
       " 26971500,\n",
       " 27443100,\n",
       " 19671900,\n",
       " 16786200,\n",
       " 9270300,\n",
       " 28360500,\n",
       " 28767300,\n",
       " 25071000,\n",
       " 11430300,\n",
       " 19471500,\n",
       " 24864900,\n",
       " 27896700,\n",
       " 24625200,\n",
       " 25742100,\n",
       " 26133300,\n",
       " 27492000,\n",
       " 26890200,\n",
       " 27198300,\n",
       " 28450200,\n",
       " 28235100,\n",
       " 28850100,\n",
       " 19938300,\n",
       " 24283500,\n",
       " 27440100,\n",
       " 26653800,\n",
       " 27022500,\n",
       " 27841800,\n",
       " 28008300,\n",
       " 27493800,\n",
       " 22945500,\n",
       " 25452600,\n",
       " 20948700,\n",
       " 13686000,\n",
       " 22726800,\n",
       " 20129100,\n",
       " 18383400,\n",
       " 26090100,\n",
       " 24473100,\n",
       " 25812600,\n",
       " 26548500,\n",
       " 25943100,\n",
       " 27840300,\n",
       " 28393500,\n",
       " 27226800,\n",
       " 16731600,\n",
       " 28234800,\n",
       " 20788200,\n",
       " 23977500,\n",
       " 18775800,\n",
       " 23494800,\n",
       " 28453200,\n",
       " 27543900,\n",
       " 24582600,\n",
       " 23734800,\n",
       " 26240400,\n",
       " 26601900,\n",
       " 22685100,\n",
       " 25404600,\n",
       " 16614000,\n",
       " 26692500,\n",
       " 25923600,\n",
       " 26162700,\n",
       " 25593600,\n",
       " 26544000,\n",
       " 24890400,\n",
       " 25350300,\n",
       " 24976200,\n",
       " 24972600,\n",
       " 20469000,\n",
       " 19839900,\n",
       " 27149700,\n",
       " 21291000,\n",
       " 23178000,\n",
       " 16622100,\n",
       " 12325200,\n",
       " 25245600,\n",
       " 25217400,\n",
       " 23240400,\n",
       " 20825400,\n",
       " 19134900,\n",
       " 22869000,\n",
       " 24476400,\n",
       " 24480900,\n",
       " 22998900,\n",
       " 22580100,\n",
       " 7375500,\n",
       " 8647200,\n",
       " 13092300,\n",
       " 17082900,\n",
       " 23325000,\n",
       " 4996800,\n",
       " 18984300,\n",
       " 19464000,\n",
       " 9694800,\n",
       " 17983800,\n",
       " 21319200,\n",
       " 21116100,\n",
       " 20686200,\n",
       " 19528500,\n",
       " 17695500,\n",
       " 9372900,\n",
       " 18094500,\n",
       " 22605900,\n",
       " 21951900,\n",
       " 22182300,\n",
       " 21162600,\n",
       " 18515100,\n",
       " 17017200,\n",
       " 10061100,\n",
       " 20774700,\n",
       " 19355700,\n",
       " 21173400,\n",
       " 20515200,\n",
       " 20215200,\n",
       " 20486700,\n",
       " 20458200,\n",
       " 19699200,\n",
       " 17874900,\n",
       " 19002900,\n",
       " 14229000,\n",
       " 15439800,\n",
       " 15178200,\n",
       " 1263900,\n",
       " 18475800,\n",
       " 20017200,\n",
       " 19548900,\n",
       " 19379400,\n",
       " 19067400,\n",
       " 14892300,\n",
       " 6781500,\n",
       " 1397400,\n",
       " 3731700,\n",
       " 3660300,\n",
       " 11928000,\n",
       " 16068900,\n",
       " 12713700,\n",
       " 14263200,\n",
       " 17064900,\n",
       " 16020000,\n",
       " 5673000,\n",
       " 11145000,\n",
       " 16787100,\n",
       " 14185500,\n",
       " 8224800,\n",
       " 4938000,\n",
       " 7165800,\n",
       " 9065400,\n",
       " 14779800,\n",
       " 10543800,\n",
       " 12386400,\n",
       " 1155000,\n",
       " 10702500,\n",
       " 15086100,\n",
       " 13414500,\n",
       " 12559800,\n",
       " 6374100,\n",
       " 14383200,\n",
       " 11226600,\n",
       " 5317800,\n",
       " 1571700,\n",
       " 6075600,\n",
       " 12772800,\n",
       " 13809900,\n",
       " 5867700,\n",
       " 6563400,\n",
       " 1566000,\n",
       " 10331100,\n",
       " 13282800,\n",
       " 4209900,\n",
       " 8178900,\n",
       " 4483800,\n",
       " 1877400,\n",
       " 3474900,\n",
       " 12776100,\n",
       " 13005600,\n",
       " 12803400,\n",
       " 12643200,\n",
       " 12497400,\n",
       " 4855800,\n",
       " 5655900,\n",
       " 10750800,\n",
       " 3524400,\n",
       " 1669800,\n",
       " 9900300,\n",
       " 1317000,\n",
       " 8569800,\n",
       " 12075900,\n",
       " 6894300,\n",
       " 9343500,\n",
       " 3021300,\n",
       " 3079800,\n",
       " 2593500,\n",
       " 8978100,\n",
       " 11629500,\n",
       " 11718000,\n",
       " 10431600,\n",
       " 11551500,\n",
       " 11322900,\n",
       " 11415600,\n",
       " 9142200,\n",
       " 9344700,\n",
       " 11875800,\n",
       " 11714100,\n",
       " 9189900,\n",
       " 1836300,\n",
       " 7769400,\n",
       " 4728900,\n",
       " 2759400,\n",
       " 11638800,\n",
       " 8446200,\n",
       " 2419500,\n",
       " 10223100,\n",
       " 5783700,\n",
       " 10042200,\n",
       " 12258900,\n",
       " 11873700,\n",
       " 12007800,\n",
       " 11637300,\n",
       " 12102900,\n",
       " 8953200,\n",
       " 11190600,\n",
       " 12856500,\n",
       " 12591000,\n",
       " 6436800,\n",
       " 12836700,\n",
       " 4120200,\n",
       " 13568700,\n",
       " 13328700,\n",
       " 7196400,\n",
       " 6755700,\n",
       " 14090100,\n",
       " 12294000,\n",
       " 13171800,\n",
       " 2272200,\n",
       " 13686000,\n",
       " 3060900,\n",
       " 3281700,\n",
       " 14986200,\n",
       " 11649600,\n",
       " 14196900,\n",
       " 14638500,\n",
       " 14750700,\n",
       " 15240000,\n",
       " 15561900,\n",
       " 15117000,\n",
       " 13979100,\n",
       " 16514700,\n",
       " 15756600,\n",
       " 13722000,\n",
       " 4941300,\n",
       " 4258500,\n",
       " 3673500,\n",
       " 2652900,\n",
       " 10190100,\n",
       " 3336600,\n",
       " 17025300,\n",
       " 14037300,\n",
       " 17593200,\n",
       " 17688600,\n",
       " 18066000,\n",
       " 14121600,\n",
       " 17809500,\n",
       " 18134100,\n",
       " 11269800,\n",
       " 13512600,\n",
       " 8999400,\n",
       " 5670300,\n",
       " 3335100,\n",
       " 3901500,\n",
       " 9906300,\n",
       " 2948400,\n",
       " 3222000,\n",
       " 7547100,\n",
       " 21142200,\n",
       " 21361200,\n",
       " 21711000,\n",
       " 19547700,\n",
       " 10179900,\n",
       " 3895500,\n",
       " 3195900,\n",
       " 7480800,\n",
       " 9319200,\n",
       " 18545700,\n",
       " 20613000,\n",
       " 21089400,\n",
       " 20255100,\n",
       " 20687100,\n",
       " 21951300,\n",
       " 20767200,\n",
       " 22123200,\n",
       " 18582600,\n",
       " 5984100,\n",
       " 20608800,\n",
       " 24244200,\n",
       " 5724900,\n",
       " 10911600,\n",
       " 16576800,\n",
       " 19764900,\n",
       " 23928300,\n",
       " 24205200,\n",
       " 6724800,\n",
       " 16292700,\n",
       " 16354800,\n",
       " 19264200,\n",
       " 21360300,\n",
       " 25013100,\n",
       " 22652400,\n",
       " 4367100,\n",
       " 26521200,\n",
       " 26032200,\n",
       " 25923900,\n",
       " 23751600,\n",
       " 20006700,\n",
       " 11379000,\n",
       " 4283100,\n",
       " 26893500,\n",
       " 12894300,\n",
       " 26231700,\n",
       " 26613000,\n",
       " 1901100,\n",
       " 14388600,\n",
       " 19898100,\n",
       " 26872800,\n",
       " 21458700,\n",
       " 21837000,\n",
       " 17120400,\n",
       " 22131900,\n",
       " 8935200,\n",
       " 19382400,\n",
       " 15155100,\n",
       " 9457800,\n",
       " 27548700,\n",
       " 10790100,\n",
       " 8543100,\n",
       " 4237500,\n",
       " 25912500,\n",
       " 27738600,\n",
       " 28324200,\n",
       " 26664300,\n",
       " 13883700,\n",
       " 28163100,\n",
       " 28284600,\n",
       " 19408200,\n",
       " 9631800,\n",
       " 23997000,\n",
       " 19948200,\n",
       " 28560000,\n",
       " 25789500,\n",
       " 23130000,\n",
       " 25545000,\n",
       " 6018600,\n",
       " 5756400,\n",
       " 4888200,\n",
       " 15797400,\n",
       " 26442000,\n",
       " 28575000,\n",
       " 9979800,\n",
       " 12695100,\n",
       " 19260900,\n",
       " 27844500,\n",
       " 19807800,\n",
       " 15523200,\n",
       " 9855000,\n",
       " 22791300,\n",
       " 26081100,\n",
       " 26122500,\n",
       " 18433800,\n",
       " 8227500,\n",
       " 3114900,\n",
       " 27494700,\n",
       " 29208000,\n",
       " 28674900,\n",
       " 26582400,\n",
       " 28925700,\n",
       " 28974300,\n",
       " 25713600,\n",
       " 27640500,\n",
       " 27342900,\n",
       " 25633800,\n",
       " 27424500,\n",
       " 27273600,\n",
       " 27977100,\n",
       " 25254600,\n",
       " 28752000,\n",
       " 29009700,\n",
       " 19721100,\n",
       " 28479000,\n",
       " 21432000,\n",
       " 12062100,\n",
       " 28757400,\n",
       " 15855000,\n",
       " 24624900,\n",
       " 28672200,\n",
       " 29151000,\n",
       " 28810800,\n",
       " 28404300,\n",
       " 28475100,\n",
       " 28575300,\n",
       " 28568700,\n",
       " 28552800,\n",
       " 28148400,\n",
       " 27450900,\n",
       " 25950300,\n",
       " 26799900,\n",
       " 27184800,\n",
       " 19395900,\n",
       " 25849200,\n",
       " 14811600,\n",
       " 19250100,\n",
       " 27675600,\n",
       " 22810800,\n",
       " 15826800,\n",
       " 25006500,\n",
       " 27005700,\n",
       " 27107700,\n",
       " 27685200,\n",
       " 27774300,\n",
       " 26032500,\n",
       " 24401400,\n",
       " 10391400,\n",
       " 10551900,\n",
       " 12291000,\n",
       " 13361100,\n",
       " 19368600,\n",
       " 22162500,\n",
       " 25746000,\n",
       " 26504400,\n",
       " 24608700,\n",
       " 26658600,\n",
       " 26454300,\n",
       " 25844400,\n",
       " 25609500,\n",
       " 25702800,\n",
       " 25863000,\n",
       " 19226400,\n",
       " 21600300,\n",
       " 7074600,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 11010048,\n",
       " 12588000,\n",
       " 22174500,\n",
       " 23222400,\n",
       " 23051400,\n",
       " 22851000,\n",
       " 18495300,\n",
       " 15765600,\n",
       " 22423500,\n",
       " 20672100,\n",
       " 15087300,\n",
       " 14507100,\n",
       " 14212500,\n",
       " 14170800,\n",
       " 8084100,\n",
       " 12435000,\n",
       " 21881400,\n",
       " 12464400,\n",
       " 3077100,\n",
       " 15755400,\n",
       " 9270300,\n",
       " 9249600,\n",
       " 8933400,\n",
       " 7437300,\n",
       " 2196300,\n",
       " 21818400,\n",
       " 22196100,\n",
       " 7923000,\n",
       " 3485400,\n",
       " 13438200,\n",
       " 15196800,\n",
       " 19957500,\n",
       " 18114000,\n",
       " 19177500,\n",
       " 21003300,\n",
       " 2819400,\n",
       " 20206800,\n",
       " 20023800,\n",
       " 20127600,\n",
       " 20338200,\n",
       " 20002500,\n",
       " 19054500,\n",
       " 16784100,\n",
       " 18621000,\n",
       " 18509700,\n",
       " 18436800,\n",
       " 17403000,\n",
       " 19081800,\n",
       " 18750300,\n",
       " 18604500,\n",
       " 18344400,\n",
       " 17430900,\n",
       " 15885000,\n",
       " 18779400,\n",
       " 18177900,\n",
       " 17150400,\n",
       " 18234000,\n",
       " 17807100,\n",
       " 16705200,\n",
       " 16075500,\n",
       " 16043100,\n",
       " 17052600,\n",
       " 11799600,\n",
       " 4476900,\n",
       " 3221400,\n",
       " 15126000,\n",
       " 8430600,\n",
       " 16055400,\n",
       " 11535000,\n",
       " 15625500,\n",
       " 7736700,\n",
       " 15237000,\n",
       " 14382900,\n",
       " 14331000,\n",
       " 12407100,\n",
       " 15802500,\n",
       " 14798100,\n",
       " 11571900,\n",
       " 13949100,\n",
       " 14046900,\n",
       " 13939500,\n",
       " 11465400,\n",
       " 14238000,\n",
       " 13506000,\n",
       " 12017400,\n",
       " 13279800,\n",
       " 11569500,\n",
       " 8440500,\n",
       " 13403700,\n",
       " 12765900,\n",
       " 11378700,\n",
       " 12900300,\n",
       " 13447800,\n",
       " 13316100,\n",
       " 13089000,\n",
       " 12752100,\n",
       " 12610200,\n",
       " 12292500,\n",
       " 6360000,\n",
       " 11767200,\n",
       " 10440900,\n",
       " 10087500,\n",
       " 5208600,\n",
       " 12792600,\n",
       " 13362000,\n",
       " 12766200,\n",
       " 5777100,\n",
       " 8212500,\n",
       " 11112000,\n",
       " 9515700,\n",
       " 1278000,\n",
       " 2028600,\n",
       " 1035900,\n",
       " 5277300,\n",
       " 2822100,\n",
       " 2565300,\n",
       " 2838300,\n",
       " 11978700,\n",
       " 12140100,\n",
       " 12225300,\n",
       " 10570500,\n",
       " 11488500,\n",
       " 11821500,\n",
       " 7014900,\n",
       " 7465200,\n",
       " 5506800,\n",
       " 822000,\n",
       " 6998700,\n",
       " 11958300,\n",
       " 11965500,\n",
       " 2789100,\n",
       " 11355600,\n",
       " 13446900,\n",
       " 12854100,\n",
       " 12792900,\n",
       " 10662000,\n",
       " 13326300,\n",
       " 12900300,\n",
       " 12773400,\n",
       " 13383600,\n",
       " 9469800,\n",
       " 11310300,\n",
       " 3250200,\n",
       " 11577900,\n",
       " 14604600,\n",
       " 14022900,\n",
       " 14249100,\n",
       " 6964800,\n",
       " 13493400,\n",
       " 14610300,\n",
       " 14285100,\n",
       " 15641700,\n",
       " 15606000,\n",
       " 12616800,\n",
       " 12141900,\n",
       " 13120500,\n",
       " 8646600,\n",
       " 7545600,\n",
       " 7412700,\n",
       " 15133200,\n",
       " 16920600,\n",
       " 15951000,\n",
       " 12057300,\n",
       " 13988400,\n",
       " 13862700,\n",
       " 16025700,\n",
       " 15752100,\n",
       " 17996400,\n",
       " 17230200,\n",
       " 17768700,\n",
       " 17277600,\n",
       " 17223600,\n",
       " 19263600,\n",
       " 18876900,\n",
       " 13911900,\n",
       " 17771400,\n",
       " 16755300,\n",
       " 17531700,\n",
       " 16734000,\n",
       " 14136300,\n",
       " 12174300,\n",
       " 9288600,\n",
       " 15692100,\n",
       " 10255800,\n",
       " 19694100,\n",
       " 7878900,\n",
       " 18352200,\n",
       " 21339000,\n",
       " 21639600,\n",
       " 12864000,\n",
       " 15048000,\n",
       " 11277000,\n",
       " 22881600,\n",
       " 22824000,\n",
       " 18653400,\n",
       " 23020800,\n",
       " 19395900,\n",
       " 22335600,\n",
       " 21747900,\n",
       " 19554000,\n",
       " 22115700,\n",
       " 22285200,\n",
       " 18729600,\n",
       " 20499000,\n",
       " 24346200,\n",
       " 25682400,\n",
       " 24799500,\n",
       " 23883300,\n",
       " 18697800,\n",
       " 16082400,\n",
       " 24943200,\n",
       " 13857000,\n",
       " 2699400,\n",
       " 9387300,\n",
       " 21798600,\n",
       " 17839800,\n",
       " 25787400,\n",
       " 25883100,\n",
       " 25925700,\n",
       " 19628400,\n",
       " 24213600,\n",
       " 10218900,\n",
       " 26698500,\n",
       " 20871600,\n",
       " 23700600,\n",
       " 25429200,\n",
       " 24438300,\n",
       " 24566400,\n",
       " 9887400,\n",
       " 26411100,\n",
       " 19389300,\n",
       " 27317700,\n",
       " 27634800,\n",
       " 26389500,\n",
       " 27096300,\n",
       " 26770800,\n",
       " 24842100,\n",
       " 12916200,\n",
       " 4887600,\n",
       " 28218300,\n",
       " 26941200,\n",
       " 22939200,\n",
       " 28478100,\n",
       " 24927300,\n",
       " 4468800,\n",
       " 21514800,\n",
       " 29399100,\n",
       " 28754700,\n",
       " 26002200,\n",
       " 25549200,\n",
       " 26879400,\n",
       " 21385200,\n",
       " 26802600,\n",
       " 22538100,\n",
       " 25273500,\n",
       " 22819800,\n",
       " 9448800,\n",
       " 16046400,\n",
       " 26114100,\n",
       " 13122300,\n",
       " 25675200,\n",
       " 26760000,\n",
       " 27743400,\n",
       " 28213200,\n",
       " 26897700,\n",
       " 24087300,\n",
       " 26456700,\n",
       " 24126900,\n",
       " 27547200,\n",
       " 26994900,\n",
       " 26275500,\n",
       " 22691400,\n",
       " 13176000,\n",
       " 29806200,\n",
       " 28315200,\n",
       " 24978900,\n",
       " 14842800,\n",
       " 18921000,\n",
       " 20390700,\n",
       " 28181100,\n",
       " 25904700,\n",
       " 27827400,\n",
       " 28189800,\n",
       " 23216700,\n",
       " 27950700,\n",
       " 28792200,\n",
       " 30448200,\n",
       " 28649700,\n",
       " 29091300,\n",
       " 18698400,\n",
       " 24875100,\n",
       " 18720600,\n",
       " 24807300,\n",
       " 22787400,\n",
       " 26480400,\n",
       " 29443200,\n",
       " 28404900,\n",
       " 28857600,\n",
       " 29971200,\n",
       " 28769100,\n",
       " 29212800,\n",
       " 26916000,\n",
       " 24231900,\n",
       " 24450600,\n",
       " 27282300,\n",
       " 26918400,\n",
       " 25914900,\n",
       " 28919400,\n",
       " 26743200,\n",
       " 24239400,\n",
       " 25949700,\n",
       " 27936600,\n",
       " 28534500,\n",
       " 29109900,\n",
       " 25849500,\n",
       " 14211900,\n",
       " 11603100,\n",
       " 3706500,\n",
       " 9566400,\n",
       " 17511900,\n",
       " 22505700,\n",
       " 23384400,\n",
       " 28444500,\n",
       " 25469700,\n",
       " 26007600,\n",
       " 27965700,\n",
       " 28198200,\n",
       " 24504900,\n",
       " 27907200,\n",
       " 26326800,\n",
       " 27148200,\n",
       " 19832400,\n",
       " 27982200,\n",
       " 10986000,\n",
       " 19271700,\n",
       " 27959100,\n",
       " 16704900,\n",
       " 7792500,\n",
       " 15494700,\n",
       " 21480900,\n",
       " 25908000,\n",
       " 25900200,\n",
       " 27015000,\n",
       " 24905100,\n",
       " 27042300,\n",
       " 25115400,\n",
       " 24550500,\n",
       " 13608000,\n",
       " 14948700,\n",
       " 20362500,\n",
       " 16618800,\n",
       " 25763700,\n",
       " 24162900,\n",
       " 11899500,\n",
       " 15970800,\n",
       " 15692100,\n",
       " 24661800,\n",
       " 25511100,\n",
       " 24817200,\n",
       " 23154600,\n",
       " 23400900,\n",
       " 22486800,\n",
       " 20285100,\n",
       " 5614200,\n",
       " 16320000,\n",
       " 11110800,\n",
       " 8311500,\n",
       " 16836000,\n",
       " 21135900,\n",
       " 12476100,\n",
       " 21132300,\n",
       " 21886200,\n",
       " 13887600,\n",
       " 19346700,\n",
       " 21130200,\n",
       " 21353100,\n",
       " 20538600,\n",
       " 23316300,\n",
       " 23743500,\n",
       " 23105400,\n",
       " 22622100,\n",
       " 16967400,\n",
       " 15777000,\n",
       " 13072800,\n",
       " 15902100,\n",
       " 16818300,\n",
       " 20723400,\n",
       " 3980100,\n",
       " 15718500,\n",
       " 16512300,\n",
       " 22661400,\n",
       " 16862100,\n",
       " 17477700,\n",
       " 4424100,\n",
       " 5251800,\n",
       " 6655500,\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].get('target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3579"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[0].get('target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set[0].get('target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rite_dicts_to_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0816985234b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrite_dicts_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwrite_dicts_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rite_dicts_to_file' is not defined"
     ]
    }
   ],
   "source": [
    "rite_dicts_to_file('train.json', train_set)\n",
    "write_dicts_to_file('test.json', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-62f5a3d38a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_dicts_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwrite_dicts_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-275d46e2b4f4>\u001b[0m in \u001b[0;36mwrite_dicts_to_file\u001b[0;34m(path, data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "write_dicts_to_file('train.json', train_set)\n",
    "write_dicts_to_file('test.json', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file('train.json', train_set)\n",
    "write_dicts_to_file('test.json', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "image = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::023375022819:role/service-role/AmazonSageMaker-ExecutionRole-20191029T112723'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_name=image,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path='s3://forecasting-do-not-delete/output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_name=image,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path='s3://forecasting-do-not-delete/output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_name=image,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    base_job_name='forecastingteamawesome',\n",
    "    output_path='s3://forecastingteamawesomedataset/morning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \n",
    "    # frequency interval is once per day\n",
    "    \"time_freq\": 'D',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \n",
    "    # let's use the last 30 days for context\n",
    "    \"context_length\": str(30),\n",
    "    \n",
    "    # let's forecast for 30 days\n",
    "    \"prediction_length\": str(30)\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://forecastingteamawesomedataset/morning/train.json\",\n",
    "    \"test\": \"s3://forecastingteamawesomedataset/morning/test.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-30 16:21:16 Starting - Starting the training job...\n",
      "2019-10-30 16:21:18 Starting - Launching requested ML instances......\n",
      "2019-10-30 16:22:24 Starting - Preparing the instances for training...\n",
      "2019-10-30 16:22:58 Downloading - Downloading input data...\n",
      "2019-10-30 16:23:44 Training - Training image download completed. Training in progress..\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:45 INFO 140089330636608] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:45 INFO 140089330636608] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'30', u'epochs': u'400', u'time_freq': u'D', u'context_length': u'30', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:45 INFO 140089330636608] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'D', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:45 INFO 140089330636608] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Training set statistics:\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Integer time series\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] number of time series: 1\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] number of observations: 3579\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] mean target length: 3579\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] min/mean/max target: 720600.0/17180642.8164/31347900.0\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] mean abs(target): 17180642.8164\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] contains missing values: no\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Test set statistics:\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Integer time series\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] number of time series: 1\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] number of observations: 767\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] mean target length: 767\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] min/mean/max target: 12000.0/16576071.4263/29639100.0\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] mean abs(target): 16576071.4263\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] contains missing values: no\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] nvidia-smi took: 0.0251541137695 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 149.69301223754883, \"sum\": 149.69301223754883, \"min\": 149.69301223754883}}, \"EndTime\": 1572452626.290474, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452626.139891}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 370.38207054138184, \"sum\": 370.38207054138184, \"min\": 370.38207054138184}}, \"EndTime\": 1572452626.510397, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452626.290555}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] Epoch[0] Batch[0] avg_epoch_loss=19.842489\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=19.8424892426\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] Epoch[0] Batch[5] avg_epoch_loss=19.244504\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=19.2445039749\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] Epoch[0] Batch [5]#011Speed: 630.13 samples/sec#011loss=19.244504\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] processed a total of 624 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 1331.5889835357666, \"sum\": 1331.5889835357666, \"min\": 1331.5889835357666}}, \"EndTime\": 1572452627.84218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452626.510457}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=468.537483239 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=0, train loss <loss>=18.979129982\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:47 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_65f38586-6aba-40a8-ba60-6891ad40fe46-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.810096740722656, \"sum\": 27.810096740722656, \"min\": 27.810096740722656}}, \"EndTime\": 1572452627.870921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452627.84234}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:48 INFO 140089330636608] Epoch[1] Batch[0] avg_epoch_loss=18.297588\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=18.2975883484\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:48 INFO 140089330636608] Epoch[1] Batch[5] avg_epoch_loss=18.065228\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=18.0652281443\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:48 INFO 140089330636608] Epoch[1] Batch [5]#011Speed: 478.00 samples/sec#011loss=18.065228\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] Epoch[1] Batch[10] avg_epoch_loss=17.867309\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=17.6298061371\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] Epoch[1] Batch [10]#011Speed: 639.17 samples/sec#011loss=17.629806\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] processed a total of 651 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1428.0309677124023, \"sum\": 1428.0309677124023, \"min\": 1428.0309677124023}}, \"EndTime\": 1572452629.299069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452627.87098}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=455.841796301 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=1, train loss <loss>=17.8673090501\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_70dc2211-743e-4ef8-9984-e5ba429fe50d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.270036697387695, \"sum\": 21.270036697387695, \"min\": 21.270036697387695}}, \"EndTime\": 1572452629.320816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452629.299131}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] Epoch[2] Batch[0] avg_epoch_loss=17.509253\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=17.5092525482\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] Epoch[2] Batch[5] avg_epoch_loss=17.409288\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=17.4092884064\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] Epoch[2] Batch [5]#011Speed: 605.31 samples/sec#011loss=17.409288\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] Epoch[2] Batch[10] avg_epoch_loss=17.366974\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=17.3161972046\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] Epoch[2] Batch [10]#011Speed: 625.83 samples/sec#011loss=17.316197\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] processed a total of 649 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1231.705904006958, \"sum\": 1231.705904006958, \"min\": 1231.705904006958}}, \"EndTime\": 1572452630.552634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452629.320872}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=526.871211394 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=2, train loss <loss>=17.3669742237\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_f4e5e779-5125-4c34-a2d0-bb37539f9a2e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.190881729125977, \"sum\": 21.190881729125977, \"min\": 21.190881729125977}}, \"EndTime\": 1572452630.574283, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452630.552697}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] Epoch[3] Batch[0] avg_epoch_loss=17.330093\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=17.3300933838\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] Epoch[3] Batch[5] avg_epoch_loss=17.333883\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=17.3338832855\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] Epoch[3] Batch [5]#011Speed: 629.36 samples/sec#011loss=17.333883\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] processed a total of 612 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.7529850006104, \"sum\": 1088.7529850006104, \"min\": 1088.7529850006104}}, \"EndTime\": 1572452631.663146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452630.574339}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=562.062478829 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=3, train loss <loss>=17.2854598999\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_d6418585-9e2a-457a-bd2d-eeb9e8367f6a-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.00396156311035, \"sum\": 21.00396156311035, \"min\": 21.00396156311035}}, \"EndTime\": 1572452631.684764, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452631.66321}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] Epoch[4] Batch[0] avg_epoch_loss=17.294378\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=17.2943782806\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] Epoch[4] Batch[5] avg_epoch_loss=17.255976\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=17.2559757233\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] Epoch[4] Batch [5]#011Speed: 649.05 samples/sec#011loss=17.255976\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] Epoch[4] Batch[10] avg_epoch_loss=17.252225\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=17.2477241516\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] Epoch[4] Batch [10]#011Speed: 633.87 samples/sec#011loss=17.247724\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] processed a total of 691 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1191.1818981170654, \"sum\": 1191.1818981170654, \"min\": 1191.1818981170654}}, \"EndTime\": 1572452632.876056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452631.684819}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=580.051308049 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=4, train loss <loss>=17.2522250089\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:52 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_38c3d1af-a76f-4096-b186-bb53be52be25-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.385047912597656, \"sum\": 23.385047912597656, \"min\": 23.385047912597656}}, \"EndTime\": 1572452632.899826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452632.876118}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] Epoch[5] Batch[0] avg_epoch_loss=17.174471\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=17.1744709015\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] Epoch[5] Batch[5] avg_epoch_loss=17.201023\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=17.2010227839\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] Epoch[5] Batch [5]#011Speed: 646.75 samples/sec#011loss=17.201023\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] processed a total of 634 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1080.7960033416748, \"sum\": 1080.7960033416748, \"min\": 1080.7960033416748}}, \"EndTime\": 1572452633.980735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452632.899886}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=586.553576399 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=5, train loss <loss>=17.2100482941\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:53 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:54 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_71232a87-2868-4304-a63a-b75bed82d2cd-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.270917892456055, \"sum\": 22.270917892456055, \"min\": 22.270917892456055}}, \"EndTime\": 1572452634.003583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452633.980803}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:54 INFO 140089330636608] Epoch[6] Batch[0] avg_epoch_loss=17.118229\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=17.1182289124\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:54 INFO 140089330636608] Epoch[6] Batch[5] avg_epoch_loss=17.175414\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=17.1754137675\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:54 INFO 140089330636608] Epoch[6] Batch [5]#011Speed: 627.08 samples/sec#011loss=17.175414\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] Epoch[6] Batch[10] avg_epoch_loss=17.161320\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=17.1444084167\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] Epoch[6] Batch [10]#011Speed: 601.38 samples/sec#011loss=17.144408\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] processed a total of 669 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1234.5709800720215, \"sum\": 1234.5709800720215, \"min\": 1234.5709800720215}}, \"EndTime\": 1572452635.238272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452634.003641}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=541.842502434 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=6, train loss <loss>=17.1613204262\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_5320e20b-a34d-4f1c-b545-1751155da796-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.116899490356445, \"sum\": 22.116899490356445, \"min\": 22.116899490356445}}, \"EndTime\": 1572452635.260902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452635.238343}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] Epoch[7] Batch[0] avg_epoch_loss=17.166733\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=17.1667327881\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] Epoch[7] Batch[5] avg_epoch_loss=17.145820\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=17.1458202998\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:55 INFO 140089330636608] Epoch[7] Batch [5]#011Speed: 652.86 samples/sec#011loss=17.145820\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] processed a total of 622 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1062.946081161499, \"sum\": 1062.946081161499, \"min\": 1062.946081161499}}, \"EndTime\": 1572452636.323975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452635.260974}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=585.113329136 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=7, train loss <loss>=17.1220985413\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_fa2f5472-411c-42f9-88f8-54face82c22d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.8950252532959, \"sum\": 23.8950252532959, \"min\": 23.8950252532959}}, \"EndTime\": 1572452636.348338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452636.32404}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] Epoch[8] Batch[0] avg_epoch_loss=17.092402\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=17.0924015045\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] Epoch[8] Batch[5] avg_epoch_loss=17.138176\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=17.1381762822\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] Epoch[8] Batch [5]#011Speed: 650.98 samples/sec#011loss=17.138176\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] processed a total of 612 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1067.6710605621338, \"sum\": 1067.6710605621338, \"min\": 1067.6710605621338}}, \"EndTime\": 1572452637.41613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452636.348399}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=573.155474577 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=8, train loss <loss>=17.1250530243\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] Epoch[9] Batch[0] avg_epoch_loss=17.110918\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=17.110918045\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] Epoch[9] Batch[5] avg_epoch_loss=17.062732\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=17.0627323786\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] Epoch[9] Batch [5]#011Speed: 653.15 samples/sec#011loss=17.062732\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] Epoch[9] Batch[10] avg_epoch_loss=17.097119\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=17.1383823395\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] Epoch[9] Batch [10]#011Speed: 634.90 samples/sec#011loss=17.138382\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] processed a total of 662 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1174.881935119629, \"sum\": 1174.881935119629, \"min\": 1174.881935119629}}, \"EndTime\": 1572452638.591526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452637.416196}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.417653946 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=9, train loss <loss>=17.0971187245\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_44abbbbf-074f-4abe-a10e-8ae58a5123b4-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.370887756347656, \"sum\": 21.370887756347656, \"min\": 21.370887756347656}}, \"EndTime\": 1572452638.613359, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452638.591586}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] Epoch[10] Batch[0] avg_epoch_loss=17.002857\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=17.0028572083\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] Epoch[10] Batch[5] avg_epoch_loss=17.017162\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=17.0171616872\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] Epoch[10] Batch [5]#011Speed: 634.21 samples/sec#011loss=17.017162\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] Epoch[10] Batch[10] avg_epoch_loss=17.003892\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=16.9879680634\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] Epoch[10] Batch [10]#011Speed: 615.44 samples/sec#011loss=16.987968\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] processed a total of 653 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1206.6340446472168, \"sum\": 1206.6340446472168, \"min\": 1206.6340446472168}}, \"EndTime\": 1572452639.82012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452638.613426}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=541.133686399 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=10, train loss <loss>=17.0038918582\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:23:59 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_8ae54a79-8854-4222-9f8e-c34e1f76a0e5-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.444124221801758, \"sum\": 27.444124221801758, \"min\": 27.444124221801758}}, \"EndTime\": 1572452639.848226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452639.820176}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] Epoch[11] Batch[0] avg_epoch_loss=16.915697\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=16.9156970978\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] Epoch[11] Batch[5] avg_epoch_loss=17.019820\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=17.0198202133\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] Epoch[11] Batch [5]#011Speed: 641.02 samples/sec#011loss=17.019820\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.5950679779053, \"sum\": 1076.5950679779053, \"min\": 1076.5950679779053}}, \"EndTime\": 1572452640.924941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452639.848291}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=576.769240955 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=11, train loss <loss>=17.0222490311\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:00 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:01 INFO 140089330636608] Epoch[12] Batch[0] avg_epoch_loss=16.940826\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=16.940826416\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:01 INFO 140089330636608] Epoch[12] Batch[5] avg_epoch_loss=16.991580\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=16.9915796916\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:01 INFO 140089330636608] Epoch[12] Batch [5]#011Speed: 644.17 samples/sec#011loss=16.991580\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] processed a total of 629 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1080.1670551300049, \"sum\": 1080.1670551300049, \"min\": 1080.1670551300049}}, \"EndTime\": 1572452642.005556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452640.925005}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=582.26502663 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=12, train loss <loss>=17.0375383377\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] Epoch[13] Batch[0] avg_epoch_loss=16.959116\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=16.9591159821\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] Epoch[13] Batch[5] avg_epoch_loss=16.993454\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=16.9934536616\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:02 INFO 140089330636608] Epoch[13] Batch [5]#011Speed: 648.47 samples/sec#011loss=16.993454\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1066.620111465454, \"sum\": 1066.620111465454, \"min\": 1066.620111465454}}, \"EndTime\": 1572452643.07266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452642.005618}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=568.098749008 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=13, train loss <loss>=17.0200508118\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] Epoch[14] Batch[0] avg_epoch_loss=17.052319\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=17.052318573\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] Epoch[14] Batch[5] avg_epoch_loss=16.943051\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=16.9430513382\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:03 INFO 140089330636608] Epoch[14] Batch [5]#011Speed: 637.08 samples/sec#011loss=16.943051\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] Epoch[14] Batch[10] avg_epoch_loss=16.976537\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=17.0167205811\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] Epoch[14] Batch [10]#011Speed: 637.43 samples/sec#011loss=17.016721\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] processed a total of 651 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1197.3369121551514, \"sum\": 1197.3369121551514, \"min\": 1197.3369121551514}}, \"EndTime\": 1572452644.270387, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452643.072723}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=543.626727521 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=14, train loss <loss>=16.9765373577\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_06fc6492-e9d2-4f13-97be-4d7d0a7a5b24-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.577877044677734, \"sum\": 27.577877044677734, \"min\": 27.577877044677734}}, \"EndTime\": 1572452644.29934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452644.270532}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] Epoch[15] Batch[0] avg_epoch_loss=16.953106\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=16.9531059265\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] Epoch[15] Batch[5] avg_epoch_loss=16.940363\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=16.9403626124\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] Epoch[15] Batch [5]#011Speed: 611.78 samples/sec#011loss=16.940363\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] processed a total of 624 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1117.6059246063232, \"sum\": 1117.6059246063232, \"min\": 1117.6059246063232}}, \"EndTime\": 1572452645.417058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452644.299396}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=558.269406999 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=15, train loss <loss>=16.9674621582\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_673692b8-6438-4f07-ab84-0f5f0137023d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.98602867126465, \"sum\": 24.98602867126465, \"min\": 24.98602867126465}}, \"EndTime\": 1572452645.442527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452645.417159}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] Epoch[16] Batch[0] avg_epoch_loss=17.011353\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=17.0113525391\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] Epoch[16] Batch[5] avg_epoch_loss=16.967785\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=16.9677845637\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] Epoch[16] Batch [5]#011Speed: 647.59 samples/sec#011loss=16.967785\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] Epoch[16] Batch[10] avg_epoch_loss=16.974731\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=16.9830673218\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] Epoch[16] Batch [10]#011Speed: 633.97 samples/sec#011loss=16.983067\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] processed a total of 651 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1177.567958831787, \"sum\": 1177.567958831787, \"min\": 1177.567958831787}}, \"EndTime\": 1572452646.620211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452645.442587}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=552.79202892 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=16, train loss <loss>=16.9747312719\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] Epoch[17] Batch[0] avg_epoch_loss=16.885796\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=16.8857955933\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] Epoch[17] Batch[5] avg_epoch_loss=16.938267\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=16.938267072\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] Epoch[17] Batch [5]#011Speed: 644.90 samples/sec#011loss=16.938267\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] Epoch[17] Batch[10] avg_epoch_loss=16.962551\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=16.9916908264\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] Epoch[17] Batch [10]#011Speed: 642.93 samples/sec#011loss=16.991691\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] processed a total of 671 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1195.3139305114746, \"sum\": 1195.3139305114746, \"min\": 1195.3139305114746}}, \"EndTime\": 1572452647.81586, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452646.620272}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=561.315140709 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=17, train loss <loss>=16.9625505968\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:07 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_50a356e8-ee76-4090-ac57-60f76294bc31-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.213956832885742, \"sum\": 25.213956832885742, \"min\": 25.213956832885742}}, \"EndTime\": 1572452647.841464, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452647.815925}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:08 INFO 140089330636608] Epoch[18] Batch[0] avg_epoch_loss=16.990242\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=16.9902420044\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:08 INFO 140089330636608] Epoch[18] Batch[5] avg_epoch_loss=16.963833\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=16.9638325373\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:08 INFO 140089330636608] Epoch[18] Batch [5]#011Speed: 643.10 samples/sec#011loss=16.963833\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] Epoch[18] Batch[10] avg_epoch_loss=16.929647\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=16.8886234283\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] Epoch[18] Batch [10]#011Speed: 600.14 samples/sec#011loss=16.888623\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] processed a total of 649 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1208.2347869873047, \"sum\": 1208.2347869873047, \"min\": 1208.2347869873047}}, \"EndTime\": 1572452649.049811, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452647.841521}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=537.104543373 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=18, train loss <loss>=16.9296465787\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_3f342353-28fb-4d6e-8f42-c6f159d3a248-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.92690086364746, \"sum\": 24.92690086364746, \"min\": 24.92690086364746}}, \"EndTime\": 1572452649.075145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452649.049877}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] Epoch[19] Batch[0] avg_epoch_loss=16.990238\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=16.9902381897\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] Epoch[19] Batch[5] avg_epoch_loss=16.958419\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=16.958419164\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:09 INFO 140089330636608] Epoch[19] Batch [5]#011Speed: 643.90 samples/sec#011loss=16.958419\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] Epoch[19] Batch[10] avg_epoch_loss=16.915726\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=16.8644931793\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] Epoch[19] Batch [10]#011Speed: 622.82 samples/sec#011loss=16.864493\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] processed a total of 643 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1187.9470348358154, \"sum\": 1187.9470348358154, \"min\": 1187.9470348358154}}, \"EndTime\": 1572452650.263213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452649.075208}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=541.223866981 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=19, train loss <loss>=16.9157255346\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_33677e93-e202-4a60-9540-0ca51e861087-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.519899368286133, \"sum\": 21.519899368286133, \"min\": 21.519899368286133}}, \"EndTime\": 1572452650.285239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452650.263282}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] Epoch[20] Batch[0] avg_epoch_loss=16.996405\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=16.9964046478\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] Epoch[20] Batch[5] avg_epoch_loss=16.970678\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=16.9706776937\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:10 INFO 140089330636608] Epoch[20] Batch [5]#011Speed: 647.52 samples/sec#011loss=16.970678\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] Epoch[20] Batch[10] avg_epoch_loss=16.953509\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=16.9329055786\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] Epoch[20] Batch [10]#011Speed: 632.61 samples/sec#011loss=16.932906\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] processed a total of 655 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1185.3699684143066, \"sum\": 1185.3699684143066, \"min\": 1185.3699684143066}}, \"EndTime\": 1572452651.47073, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452650.285305}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=552.528201418 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=20, train loss <loss>=16.9535085505\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] Epoch[21] Batch[0] avg_epoch_loss=16.944107\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=16.9441070557\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] Epoch[21] Batch[5] avg_epoch_loss=16.883553\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=16.8835528692\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] Epoch[21] Batch [5]#011Speed: 636.81 samples/sec#011loss=16.883553\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] processed a total of 619 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.2890434265137, \"sum\": 1091.2890434265137, \"min\": 1091.2890434265137}}, \"EndTime\": 1572452652.56235, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452651.47079}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.170288658 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=21, train loss <loss>=16.91524086\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_327f8b4c-66c7-4e01-abbf-11bb3408589b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.124839782714844, \"sum\": 21.124839782714844, \"min\": 21.124839782714844}}, \"EndTime\": 1572452652.583897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452652.562413}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] Epoch[22] Batch[0] avg_epoch_loss=16.977228\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=16.9772281647\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] Epoch[22] Batch[5] avg_epoch_loss=16.955618\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=16.9556175868\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] Epoch[22] Batch [5]#011Speed: 653.17 samples/sec#011loss=16.955618\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] Epoch[22] Batch[10] avg_epoch_loss=16.972489\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=16.992735672\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] Epoch[22] Batch [10]#011Speed: 623.20 samples/sec#011loss=16.992736\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] processed a total of 666 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1184.9091053009033, \"sum\": 1184.9091053009033, \"min\": 1184.9091053009033}}, \"EndTime\": 1572452653.768916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452652.583956}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=562.025795906 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=22, train loss <loss>=16.9724894437\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] Epoch[23] Batch[0] avg_epoch_loss=16.968464\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=16.9684638977\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] Epoch[23] Batch[5] avg_epoch_loss=16.953435\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=16.9534346263\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] Epoch[23] Batch [5]#011Speed: 638.48 samples/sec#011loss=16.953435\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] processed a total of 623 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1096.2929725646973, \"sum\": 1096.2929725646973, \"min\": 1096.2929725646973}}, \"EndTime\": 1572452654.865559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452653.768978}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=568.230060584 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=23, train loss <loss>=16.9530580521\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:14 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] Epoch[24] Batch[0] avg_epoch_loss=16.885649\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=16.8856487274\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] Epoch[24] Batch[5] avg_epoch_loss=16.895805\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=16.8958053589\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] Epoch[24] Batch [5]#011Speed: 640.44 samples/sec#011loss=16.895805\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] processed a total of 561 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 993.9780235290527, \"sum\": 993.9780235290527, \"min\": 993.9780235290527}}, \"EndTime\": 1572452655.85998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452654.865626}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=564.343161232 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=24, train loss <loss>=16.9175374773\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:15 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:16 INFO 140089330636608] Epoch[25] Batch[0] avg_epoch_loss=16.899855\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=16.89985466\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:16 INFO 140089330636608] Epoch[25] Batch[5] avg_epoch_loss=16.919297\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=16.9192972183\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:16 INFO 140089330636608] Epoch[25] Batch [5]#011Speed: 638.60 samples/sec#011loss=16.919297\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] Epoch[25] Batch[10] avg_epoch_loss=16.911985\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=16.9032100677\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] Epoch[25] Batch [10]#011Speed: 637.08 samples/sec#011loss=16.903210\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] processed a total of 660 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1186.2070560455322, \"sum\": 1186.2070560455322, \"min\": 1186.2070560455322}}, \"EndTime\": 1572452657.046687, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452655.860044}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=556.350321512 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=25, train loss <loss>=16.9119848772\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_6bc37106-e8fd-4414-b668-9a3ef046621e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.00992202758789, \"sum\": 21.00992202758789, \"min\": 21.00992202758789}}, \"EndTime\": 1572452657.068161, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452657.046753}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] Epoch[26] Batch[0] avg_epoch_loss=16.903538\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=16.9035377502\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] Epoch[26] Batch[5] avg_epoch_loss=16.932741\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=16.9327411652\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:17 INFO 140089330636608] Epoch[26] Batch [5]#011Speed: 641.49 samples/sec#011loss=16.932741\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] Epoch[26] Batch[10] avg_epoch_loss=16.934376\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=16.9363388062\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] Epoch[26] Batch [10]#011Speed: 640.94 samples/sec#011loss=16.936339\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] processed a total of 657 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1183.7198734283447, \"sum\": 1183.7198734283447, \"min\": 1183.7198734283447}}, \"EndTime\": 1572452658.251982, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452657.068216}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=554.98860541 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=26, train loss <loss>=16.9343764565\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] Epoch[27] Batch[0] avg_epoch_loss=16.938951\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=16.9389514923\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] Epoch[27] Batch[5] avg_epoch_loss=16.905935\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=16.9059352875\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:18 INFO 140089330636608] Epoch[27] Batch [5]#011Speed: 624.57 samples/sec#011loss=16.905935\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] Epoch[27] Batch[10] avg_epoch_loss=16.898677\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=16.8899669647\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] Epoch[27] Batch [10]#011Speed: 629.40 samples/sec#011loss=16.889967\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] processed a total of 662 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1224.776029586792, \"sum\": 1224.776029586792, \"min\": 1224.776029586792}}, \"EndTime\": 1572452659.477089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452658.252043}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=540.465112766 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=27, train loss <loss>=16.898676959\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_5fdff00f-b552-48a5-a813-bc2e3290e470-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.291017532348633, \"sum\": 21.291017532348633, \"min\": 21.291017532348633}}, \"EndTime\": 1572452659.499012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452659.477151}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] Epoch[28] Batch[0] avg_epoch_loss=16.950623\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=16.9506225586\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] Epoch[28] Batch[5] avg_epoch_loss=16.886582\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=16.8865820567\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] Epoch[28] Batch [5]#011Speed: 584.64 samples/sec#011loss=16.886582\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] processed a total of 595 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1122.6708889007568, \"sum\": 1122.6708889007568, \"min\": 1122.6708889007568}}, \"EndTime\": 1572452660.621868, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452659.499086}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=529.908143427 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=28, train loss <loss>=16.9193864822\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] Epoch[29] Batch[0] avg_epoch_loss=16.937426\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=16.9374256134\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] Epoch[29] Batch[5] avg_epoch_loss=16.916616\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=16.916615804\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] Epoch[29] Batch [5]#011Speed: 641.28 samples/sec#011loss=16.916616\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.9580764770508, \"sum\": 1084.9580764770508, \"min\": 1084.9580764770508}}, \"EndTime\": 1572452661.707232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452660.621948}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=546.508573178 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=29, train loss <loss>=16.8741371155\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_bad6a870-511f-4ca8-8fb7-d55be9991a5d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.578166961669922, \"sum\": 23.578166961669922, \"min\": 23.578166961669922}}, \"EndTime\": 1572452661.731275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452661.707311}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] Epoch[30] Batch[0] avg_epoch_loss=16.900288\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=16.9002876282\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] Epoch[30] Batch[5] avg_epoch_loss=16.903668\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=16.9036680857\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] Epoch[30] Batch [5]#011Speed: 649.06 samples/sec#011loss=16.903668\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] Epoch[30] Batch[10] avg_epoch_loss=16.885801\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=16.8643604279\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] Epoch[30] Batch [10]#011Speed: 646.37 samples/sec#011loss=16.864360\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] processed a total of 648 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1181.1141967773438, \"sum\": 1181.1141967773438, \"min\": 1181.1141967773438}}, \"EndTime\": 1572452662.912511, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452661.73134}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=548.592311417 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=30, train loss <loss>=16.8858009685\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:22 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:23 INFO 140089330636608] Epoch[31] Batch[0] avg_epoch_loss=16.915821\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=16.9158210754\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:23 INFO 140089330636608] Epoch[31] Batch[5] avg_epoch_loss=16.920723\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=16.9207226435\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:23 INFO 140089330636608] Epoch[31] Batch [5]#011Speed: 640.07 samples/sec#011loss=16.920723\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] processed a total of 632 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1109.1010570526123, \"sum\": 1109.1010570526123, \"min\": 1109.1010570526123}}, \"EndTime\": 1572452664.021951, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452662.912572}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=569.777449341 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=31, train loss <loss>=16.8930757523\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] Epoch[32] Batch[0] avg_epoch_loss=16.849516\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=16.8495159149\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] Epoch[32] Batch[5] avg_epoch_loss=16.901307\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=16.901307106\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:24 INFO 140089330636608] Epoch[32] Batch [5]#011Speed: 647.86 samples/sec#011loss=16.901307\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] processed a total of 634 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1077.544927597046, \"sum\": 1077.544927597046, \"min\": 1077.544927597046}}, \"EndTime\": 1572452665.099959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452664.022011}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=588.318825375 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=32, train loss <loss>=16.8970541\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] Epoch[33] Batch[0] avg_epoch_loss=16.846937\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=16.8469371796\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] Epoch[33] Batch[5] avg_epoch_loss=16.865419\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=16.865418752\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:25 INFO 140089330636608] Epoch[33] Batch [5]#011Speed: 645.85 samples/sec#011loss=16.865419\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] processed a total of 635 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.1519527435303, \"sum\": 1091.1519527435303, \"min\": 1091.1519527435303}}, \"EndTime\": 1572452666.191579, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452665.100028}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=581.901011091 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=33, train loss <loss>=16.8497701645\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_e185121f-7920-4b27-8c46-b8718546ba30-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.46014976501465, \"sum\": 23.46014976501465, \"min\": 23.46014976501465}}, \"EndTime\": 1572452666.2155, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452666.191646}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] Epoch[34] Batch[0] avg_epoch_loss=16.887518\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=16.8875179291\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] Epoch[34] Batch[5] avg_epoch_loss=16.877249\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=16.8772493998\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:26 INFO 140089330636608] Epoch[34] Batch [5]#011Speed: 635.22 samples/sec#011loss=16.877249\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] Epoch[34] Batch[10] avg_epoch_loss=16.849914\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=16.8171115875\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] Epoch[34] Batch [10]#011Speed: 643.57 samples/sec#011loss=16.817112\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] processed a total of 659 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1186.3679885864258, \"sum\": 1186.3679885864258, \"min\": 1186.3679885864258}}, \"EndTime\": 1572452667.401987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452666.215562}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=555.434361077 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=34, train loss <loss>=16.8499140306\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] Epoch[35] Batch[0] avg_epoch_loss=16.917612\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=16.9176120758\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] Epoch[35] Batch[5] avg_epoch_loss=16.886727\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=16.8867273331\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] Epoch[35] Batch [5]#011Speed: 656.96 samples/sec#011loss=16.886727\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1067.4879550933838, \"sum\": 1067.4879550933838, \"min\": 1067.4879550933838}}, \"EndTime\": 1572452668.469801, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452667.402047}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=580.752090331 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=35, train loss <loss>=16.9013828278\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] Epoch[36] Batch[0] avg_epoch_loss=16.837072\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=16.8370723724\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] Epoch[36] Batch[5] avg_epoch_loss=16.872918\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=16.8729184469\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] Epoch[36] Batch [5]#011Speed: 633.74 samples/sec#011loss=16.872918\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] processed a total of 626 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1081.7539691925049, \"sum\": 1081.7539691925049, \"min\": 1081.7539691925049}}, \"EndTime\": 1572452669.55199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452668.469866}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=578.637395496 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=36, train loss <loss>=16.8782730103\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] Epoch[37] Batch[0] avg_epoch_loss=16.860317\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=16.8603172302\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] Epoch[37] Batch[5] avg_epoch_loss=16.921856\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=16.9218562444\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] Epoch[37] Batch [5]#011Speed: 595.87 samples/sec#011loss=16.921856\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] processed a total of 618 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1116.4789199829102, \"sum\": 1116.4789199829102, \"min\": 1116.4789199829102}}, \"EndTime\": 1572452670.668984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452669.552053}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=553.476851096 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=37, train loss <loss>=16.9143341064\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] Epoch[38] Batch[0] avg_epoch_loss=16.714533\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=16.7145328522\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] Epoch[38] Batch[5] avg_epoch_loss=16.817715\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=16.8177146912\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] Epoch[38] Batch [5]#011Speed: 646.24 samples/sec#011loss=16.817715\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] Epoch[38] Batch[10] avg_epoch_loss=16.854696\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=16.89907341\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] Epoch[38] Batch [10]#011Speed: 633.08 samples/sec#011loss=16.899073\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] processed a total of 660 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1193.809986114502, \"sum\": 1193.809986114502, \"min\": 1193.809986114502}}, \"EndTime\": 1572452671.863265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452670.669052}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=552.810067937 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=38, train loss <loss>=16.854695927\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:31 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:32 INFO 140089330636608] Epoch[39] Batch[0] avg_epoch_loss=16.917189\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=16.9171886444\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:24:32 INFO 140089330636608] Epoch[39] Batch[5] avg_epoch_loss=16.855399\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=16.8553991318\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:32 INFO 140089330636608] Epoch[39] Batch [5]#011Speed: 645.18 samples/sec#011loss=16.855399\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] Epoch[39] Batch[10] avg_epoch_loss=16.894807\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=16.9420955658\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] Epoch[39] Batch [10]#011Speed: 643.98 samples/sec#011loss=16.942096\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] processed a total of 667 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1199.8989582061768, \"sum\": 1199.8989582061768, \"min\": 1199.8989582061768}}, \"EndTime\": 1572452673.06352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452671.863327}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=555.837507776 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=39, train loss <loss>=16.8948066018\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] Epoch[40] Batch[0] avg_epoch_loss=16.895060\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=16.8950595856\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] Epoch[40] Batch[5] avg_epoch_loss=16.829933\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=16.8299331665\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:33 INFO 140089330636608] Epoch[40] Batch [5]#011Speed: 623.63 samples/sec#011loss=16.829933\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] processed a total of 592 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1112.2779846191406, \"sum\": 1112.2779846191406, \"min\": 1112.2779846191406}}, \"EndTime\": 1572452674.176141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452673.063583}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=532.192793509 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=40, train loss <loss>=16.8217580795\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_dea63828-ebc5-4fca-a3ce-a5d67e0c3093-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.763967514038086, \"sum\": 22.763967514038086, \"min\": 22.763967514038086}}, \"EndTime\": 1572452674.199386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452674.17621}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] Epoch[41] Batch[0] avg_epoch_loss=16.967094\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=16.9670944214\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] Epoch[41] Batch[5] avg_epoch_loss=16.865056\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=16.8650560379\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:34 INFO 140089330636608] Epoch[41] Batch [5]#011Speed: 631.23 samples/sec#011loss=16.865056\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] Epoch[41] Batch[10] avg_epoch_loss=16.843810\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=16.818314743\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] Epoch[41] Batch [10]#011Speed: 594.46 samples/sec#011loss=16.818315\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] processed a total of 641 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1228.1808853149414, \"sum\": 1228.1808853149414, \"min\": 1228.1808853149414}}, \"EndTime\": 1572452675.427923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452674.199684}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=521.871393504 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=41, train loss <loss>=16.8438099948\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] Epoch[42] Batch[0] avg_epoch_loss=16.906096\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=16.9060955048\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] Epoch[42] Batch[5] avg_epoch_loss=16.788932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=16.7889315287\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] Epoch[42] Batch [5]#011Speed: 643.02 samples/sec#011loss=16.788932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] Epoch[42] Batch[10] avg_epoch_loss=16.795888\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=16.8042362213\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] Epoch[42] Batch [10]#011Speed: 639.27 samples/sec#011loss=16.804236\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] processed a total of 658 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1202.5368213653564, \"sum\": 1202.5368213653564, \"min\": 1202.5368213653564}}, \"EndTime\": 1572452676.630791, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452675.427984}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=547.134177974 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=42, train loss <loss>=16.7958882072\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_df2bbecc-251f-4128-b8ca-cd1ba102eba6-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.723861694335938, \"sum\": 26.723861694335938, \"min\": 26.723861694335938}}, \"EndTime\": 1572452676.657928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452676.630855}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] Epoch[43] Batch[0] avg_epoch_loss=16.868456\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=16.8684558868\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] Epoch[43] Batch[5] avg_epoch_loss=16.873848\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=16.8738479614\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] Epoch[43] Batch [5]#011Speed: 639.01 samples/sec#011loss=16.873848\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] Epoch[43] Batch[10] avg_epoch_loss=16.841463\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=16.8026016235\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] Epoch[43] Batch [10]#011Speed: 636.43 samples/sec#011loss=16.802602\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] processed a total of 658 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1192.2719478607178, \"sum\": 1192.2719478607178, \"min\": 1192.2719478607178}}, \"EndTime\": 1572452677.850318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452676.657992}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=551.845358242 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=43, train loss <loss>=16.8414632624\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:37 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] Epoch[44] Batch[0] avg_epoch_loss=16.865868\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=16.8658676147\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] Epoch[44] Batch[5] avg_epoch_loss=16.797514\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=16.7975142797\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] Epoch[44] Batch [5]#011Speed: 651.62 samples/sec#011loss=16.797514\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] processed a total of 613 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.7020149230957, \"sum\": 1084.7020149230957, \"min\": 1084.7020149230957}}, \"EndTime\": 1572452678.935412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452677.850378}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=565.074287764 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=44, train loss <loss>=16.8077220917\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:38 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:39 INFO 140089330636608] Epoch[45] Batch[0] avg_epoch_loss=16.837000\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=16.8369998932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:39 INFO 140089330636608] Epoch[45] Batch[5] avg_epoch_loss=16.795446\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=16.795446078\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:39 INFO 140089330636608] Epoch[45] Batch [5]#011Speed: 644.54 samples/sec#011loss=16.795446\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] Epoch[45] Batch[10] avg_epoch_loss=16.794227\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=16.7927646637\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] Epoch[45] Batch [10]#011Speed: 625.87 samples/sec#011loss=16.792765\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] processed a total of 688 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1216.4490222930908, \"sum\": 1216.4490222930908, \"min\": 1216.4490222930908}}, \"EndTime\": 1572452680.152267, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452678.935492}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=565.535070184 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=45, train loss <loss>=16.7942272533\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_a3f3af5d-a006-46ac-b27f-688d357b0547-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.991968154907227, \"sum\": 21.991968154907227, \"min\": 21.991968154907227}}, \"EndTime\": 1572452680.17476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452680.152333}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] Epoch[46] Batch[0] avg_epoch_loss=16.811516\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=16.8115158081\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] Epoch[46] Batch[5] avg_epoch_loss=16.840542\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=16.8405424754\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:40 INFO 140089330636608] Epoch[46] Batch [5]#011Speed: 633.62 samples/sec#011loss=16.840542\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] Epoch[46] Batch[10] avg_epoch_loss=16.856080\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=16.8747241974\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] Epoch[46] Batch [10]#011Speed: 643.43 samples/sec#011loss=16.874724\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] processed a total of 665 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1233.0238819122314, \"sum\": 1233.0238819122314, \"min\": 1233.0238819122314}}, \"EndTime\": 1572452681.407906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452680.174823}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=539.279462208 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=46, train loss <loss>=16.8560796217\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] Epoch[47] Batch[0] avg_epoch_loss=16.701809\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=16.7018089294\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] Epoch[47] Batch[5] avg_epoch_loss=16.810040\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=16.8100398382\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] Epoch[47] Batch [5]#011Speed: 639.06 samples/sec#011loss=16.810040\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] Epoch[47] Batch[10] avg_epoch_loss=16.822269\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=16.8369445801\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] Epoch[47] Batch [10]#011Speed: 646.69 samples/sec#011loss=16.836945\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] processed a total of 646 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1174.0140914916992, \"sum\": 1174.0140914916992, \"min\": 1174.0140914916992}}, \"EndTime\": 1572452682.58232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452681.407975}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=550.205802023 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=47, train loss <loss>=16.8222692663\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] Epoch[48] Batch[0] avg_epoch_loss=16.805574\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=16.8055744171\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] Epoch[48] Batch[5] avg_epoch_loss=16.843024\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=16.8430242538\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] Epoch[48] Batch [5]#011Speed: 643.75 samples/sec#011loss=16.843024\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] processed a total of 605 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.902006149292, \"sum\": 1078.902006149292, \"min\": 1078.902006149292}}, \"EndTime\": 1572452683.661556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452682.582381}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=560.705844607 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=48, train loss <loss>=16.8207454681\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] Epoch[49] Batch[0] avg_epoch_loss=16.752365\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=16.7523651123\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] Epoch[49] Batch[5] avg_epoch_loss=16.768321\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=16.7683207194\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] Epoch[49] Batch [5]#011Speed: 633.31 samples/sec#011loss=16.768321\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] Epoch[49] Batch[10] avg_epoch_loss=16.790511\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=16.8171401978\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] Epoch[49] Batch [10]#011Speed: 632.58 samples/sec#011loss=16.817140\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] processed a total of 687 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1217.7209854125977, \"sum\": 1217.7209854125977, \"min\": 1217.7209854125977}}, \"EndTime\": 1572452684.879654, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452683.661622}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=564.126012407 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=49, train loss <loss>=16.7905113914\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:44 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_d2893130-2cd2-403c-bf0e-fcaa8a38f029-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.1789608001709, \"sum\": 21.1789608001709, \"min\": 21.1789608001709}}, \"EndTime\": 1572452684.90129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452684.879719}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:45 INFO 140089330636608] Epoch[50] Batch[0] avg_epoch_loss=16.866032\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=16.8660316467\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:45 INFO 140089330636608] Epoch[50] Batch[5] avg_epoch_loss=16.827182\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=16.827182134\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:45 INFO 140089330636608] Epoch[50] Batch [5]#011Speed: 627.84 samples/sec#011loss=16.827182\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] processed a total of 619 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1100.0909805297852, \"sum\": 1100.0909805297852, \"min\": 1100.0909805297852}}, \"EndTime\": 1572452686.001504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452684.901351}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=562.631593022 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=50, train loss <loss>=16.8300914764\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] Epoch[51] Batch[0] avg_epoch_loss=16.775976\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=16.775976181\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] Epoch[51] Batch[5] avg_epoch_loss=16.820470\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=16.8204698563\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:46 INFO 140089330636608] Epoch[51] Batch [5]#011Speed: 653.55 samples/sec#011loss=16.820470\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1092.0839309692383, \"sum\": 1092.0839309692383, \"min\": 1092.0839309692383}}, \"EndTime\": 1572452687.094049, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452686.001563}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.674095018 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=51, train loss <loss>=16.79880867\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] Epoch[52] Batch[0] avg_epoch_loss=16.830601\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=16.8306007385\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] Epoch[52] Batch[5] avg_epoch_loss=16.758932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=16.7589321136\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:47 INFO 140089330636608] Epoch[52] Batch [5]#011Speed: 645.29 samples/sec#011loss=16.758932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] processed a total of 624 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.8750553131104, \"sum\": 1088.8750553131104, \"min\": 1088.8750553131104}}, \"EndTime\": 1572452688.183308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452687.094113}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=573.018568919 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=52, train loss <loss>=16.7691062927\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_aea2dc75-724f-4b3e-a970-ec9c6b612950-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.63498306274414, \"sum\": 22.63498306274414, \"min\": 22.63498306274414}}, \"EndTime\": 1572452688.206383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452688.183374}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] Epoch[53] Batch[0] avg_epoch_loss=16.854338\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=16.8543376923\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] Epoch[53] Batch[5] avg_epoch_loss=16.834373\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=16.8343734741\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:48 INFO 140089330636608] Epoch[53] Batch [5]#011Speed: 636.08 samples/sec#011loss=16.834373\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1095.046043395996, \"sum\": 1095.046043395996, \"min\": 1095.046043395996}}, \"EndTime\": 1572452689.301549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452688.206447}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=570.703796318 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=53, train loss <loss>=16.8088657379\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] Epoch[54] Batch[0] avg_epoch_loss=16.878496\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=16.87849617\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] Epoch[54] Batch[5] avg_epoch_loss=16.848769\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=16.84876887\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:49 INFO 140089330636608] Epoch[54] Batch [5]#011Speed: 628.94 samples/sec#011loss=16.848769\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] Epoch[54] Batch[10] avg_epoch_loss=16.871442\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=16.8986488342\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] Epoch[54] Batch [10]#011Speed: 629.99 samples/sec#011loss=16.898649\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] processed a total of 642 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1203.6499977111816, \"sum\": 1203.6499977111816, \"min\": 1203.6499977111816}}, \"EndTime\": 1572452690.505573, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452689.301612}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=533.33771083 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=54, train loss <loss>=16.871441581\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] Epoch[55] Batch[0] avg_epoch_loss=16.829823\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=16.8298225403\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] Epoch[55] Batch[5] avg_epoch_loss=16.793528\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=16.7935276031\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] Epoch[55] Batch [5]#011Speed: 634.35 samples/sec#011loss=16.793528\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] processed a total of 629 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1079.9939632415771, \"sum\": 1079.9939632415771, \"min\": 1079.9939632415771}}, \"EndTime\": 1572452691.585896, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452690.505635}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=582.359881136 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=55, train loss <loss>=16.7869300842\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] Epoch[56] Batch[0] avg_epoch_loss=16.773741\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=16.7737407684\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] Epoch[56] Batch[5] avg_epoch_loss=16.770272\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=16.7702719371\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] Epoch[56] Batch [5]#011Speed: 650.64 samples/sec#011loss=16.770272\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] processed a total of 629 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.282024383545, \"sum\": 1076.282024383545, \"min\": 1076.282024383545}}, \"EndTime\": 1572452692.662568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452691.585962}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=584.359506999 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=56, train loss <loss>=16.7346141815\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_4cfdeb81-f6ca-4386-8e6b-4da6441b9058-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.188974380493164, \"sum\": 21.188974380493164, \"min\": 21.188974380493164}}, \"EndTime\": 1572452692.684283, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452692.662641}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] Epoch[57] Batch[0] avg_epoch_loss=16.777138\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=16.7771377563\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] Epoch[57] Batch[5] avg_epoch_loss=16.728469\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=16.728468895\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] Epoch[57] Batch [5]#011Speed: 653.12 samples/sec#011loss=16.728469\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] Epoch[57] Batch[10] avg_epoch_loss=16.776556\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=16.8342605591\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] Epoch[57] Batch [10]#011Speed: 641.84 samples/sec#011loss=16.834261\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] processed a total of 684 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1178.2848834991455, \"sum\": 1178.2848834991455, \"min\": 1178.2848834991455}}, \"EndTime\": 1572452693.862689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452692.684339}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=580.457910638 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=57, train loss <loss>=16.776556015\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:53 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] Epoch[58] Batch[0] avg_epoch_loss=16.791281\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=16.7912807465\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] Epoch[58] Batch[5] avg_epoch_loss=16.803013\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=16.8030131658\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] Epoch[58] Batch [5]#011Speed: 641.98 samples/sec#011loss=16.803013\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] processed a total of 617 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1071.336030960083, \"sum\": 1071.336030960083, \"min\": 1071.336030960083}}, \"EndTime\": 1572452694.934506, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452693.862752}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=575.860021553 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=58, train loss <loss>=16.8131996155\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:54 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:55 INFO 140089330636608] Epoch[59] Batch[0] avg_epoch_loss=16.703167\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=16.7031669617\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:55 INFO 140089330636608] Epoch[59] Batch[5] avg_epoch_loss=16.772448\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=16.772447904\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:55 INFO 140089330636608] Epoch[59] Batch [5]#011Speed: 621.20 samples/sec#011loss=16.772448\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] Epoch[59] Batch[10] avg_epoch_loss=16.807472\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=16.8495006561\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] Epoch[59] Batch [10]#011Speed: 629.53 samples/sec#011loss=16.849501\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] processed a total of 670 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1204.4298648834229, \"sum\": 1204.4298648834229, \"min\": 1204.4298648834229}}, \"EndTime\": 1572452696.139432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452694.934572}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=556.222982146 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=59, train loss <loss>=16.8074718822\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] Epoch[60] Batch[0] avg_epoch_loss=16.751942\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=16.7519416809\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] Epoch[60] Batch[5] avg_epoch_loss=16.815392\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=16.8153921763\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:56 INFO 140089330636608] Epoch[60] Batch [5]#011Speed: 645.91 samples/sec#011loss=16.815392\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.055061340332, \"sum\": 1089.055061340332, \"min\": 1089.055061340332}}, \"EndTime\": 1572452697.22888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452696.139525}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=573.83146065 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=60, train loss <loss>=16.8038431168\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] Epoch[61] Batch[0] avg_epoch_loss=16.848923\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=16.8489227295\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] Epoch[61] Batch[5] avg_epoch_loss=16.793690\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=16.7936900457\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:57 INFO 140089330636608] Epoch[61] Batch [5]#011Speed: 641.25 samples/sec#011loss=16.793690\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] Epoch[61] Batch[10] avg_epoch_loss=16.820102\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=16.851795578\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] Epoch[61] Batch [10]#011Speed: 644.36 samples/sec#011loss=16.851796\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] processed a total of 659 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1175.3270626068115, \"sum\": 1175.3270626068115, \"min\": 1175.3270626068115}}, \"EndTime\": 1572452698.404705, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452697.228946}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=560.645862943 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=61, train loss <loss>=16.8201016513\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] Epoch[62] Batch[0] avg_epoch_loss=16.761133\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=16.761133194\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] Epoch[62] Batch[5] avg_epoch_loss=16.778879\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=16.7788791656\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] Epoch[62] Batch [5]#011Speed: 640.92 samples/sec#011loss=16.778879\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] Epoch[62] Batch[10] avg_epoch_loss=16.804287\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=16.8347763062\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] Epoch[62] Batch [10]#011Speed: 637.12 samples/sec#011loss=16.834776\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] processed a total of 660 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1180.5729866027832, \"sum\": 1180.5729866027832, \"min\": 1180.5729866027832}}, \"EndTime\": 1572452699.585759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452698.404774}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=559.0065411 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=62, train loss <loss>=16.8042869568\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] Epoch[63] Batch[0] avg_epoch_loss=16.800728\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:24:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=16.8007278442\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] Epoch[63] Batch[5] avg_epoch_loss=16.813216\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=16.8132158915\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] Epoch[63] Batch [5]#011Speed: 645.46 samples/sec#011loss=16.813216\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] processed a total of 627 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1097.2092151641846, \"sum\": 1097.2092151641846, \"min\": 1097.2092151641846}}, \"EndTime\": 1572452700.683381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452699.585819}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=571.396919253 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=63, train loss <loss>=16.783877182\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] Epoch[64] Batch[0] avg_epoch_loss=16.853994\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=16.8539943695\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] Epoch[64] Batch[5] avg_epoch_loss=16.778261\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=16.7782605489\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] Epoch[64] Batch [5]#011Speed: 647.18 samples/sec#011loss=16.778261\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] Epoch[64] Batch[10] avg_epoch_loss=16.805185\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=16.837494278\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] Epoch[64] Batch [10]#011Speed: 642.66 samples/sec#011loss=16.837494\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] processed a total of 671 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1186.189889907837, \"sum\": 1186.189889907837, \"min\": 1186.189889907837}}, \"EndTime\": 1572452701.87005, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452700.683452}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=565.494514998 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=64, train loss <loss>=16.8051849712\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:01 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] Epoch[65] Batch[0] avg_epoch_loss=16.744802\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=16.744802475\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] Epoch[65] Batch[5] avg_epoch_loss=16.753890\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=16.7538897196\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] Epoch[65] Batch [5]#011Speed: 639.63 samples/sec#011loss=16.753890\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] processed a total of 640 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1090.0211334228516, \"sum\": 1090.0211334228516, \"min\": 1090.0211334228516}}, \"EndTime\": 1572452702.960973, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452701.870395}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=587.089747036 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=65, train loss <loss>=16.732220459\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:02 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_ba6e199c-9bed-4381-b2b8-6a4824a48f5e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.20685577392578, \"sum\": 21.20685577392578, \"min\": 21.20685577392578}}, \"EndTime\": 1572452702.982709, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452702.961043}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:03 INFO 140089330636608] Epoch[66] Batch[0] avg_epoch_loss=16.800348\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=16.8003482819\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:03 INFO 140089330636608] Epoch[66] Batch[5] avg_epoch_loss=16.760664\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=16.7606643041\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:03 INFO 140089330636608] Epoch[66] Batch [5]#011Speed: 652.82 samples/sec#011loss=16.760664\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] processed a total of 633 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.3557796478271, \"sum\": 1088.3557796478271, \"min\": 1088.3557796478271}}, \"EndTime\": 1572452704.071183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452702.982773}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=581.561468876 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=66, train loss <loss>=16.7676689148\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] Epoch[67] Batch[0] avg_epoch_loss=16.679882\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=16.6798820496\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] Epoch[67] Batch[5] avg_epoch_loss=16.734089\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=16.7340885798\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:04 INFO 140089330636608] Epoch[67] Batch [5]#011Speed: 649.99 samples/sec#011loss=16.734089\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] Epoch[67] Batch[10] avg_epoch_loss=16.715020\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=16.692137146\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] Epoch[67] Batch [10]#011Speed: 630.38 samples/sec#011loss=16.692137\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] processed a total of 662 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1198.8818645477295, \"sum\": 1198.8818645477295, \"min\": 1198.8818645477295}}, \"EndTime\": 1572452705.270525, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452704.071247}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=552.140111584 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=67, train loss <loss>=16.7150197463\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_a28d8c27-c37a-435d-89c4-ec95039a7454-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.261972427368164, \"sum\": 27.261972427368164, \"min\": 27.261972427368164}}, \"EndTime\": 1572452705.298178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452705.270584}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] Epoch[68] Batch[0] avg_epoch_loss=16.800293\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=16.8002929688\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] Epoch[68] Batch[5] avg_epoch_loss=16.792927\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=16.7929271062\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] Epoch[68] Batch [5]#011Speed: 617.22 samples/sec#011loss=16.792927\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] Epoch[68] Batch[10] avg_epoch_loss=16.814995\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=16.8414756775\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] Epoch[68] Batch [10]#011Speed: 642.85 samples/sec#011loss=16.841476\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] processed a total of 647 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1215.0311470031738, \"sum\": 1215.0311470031738, \"min\": 1215.0311470031738}}, \"EndTime\": 1572452706.513323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452705.298236}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=532.455054028 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=68, train loss <loss>=16.8149946386\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] Epoch[69] Batch[0] avg_epoch_loss=16.699718\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=16.6997184753\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] Epoch[69] Batch[5] avg_epoch_loss=16.751565\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=16.7515652974\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] Epoch[69] Batch [5]#011Speed: 642.69 samples/sec#011loss=16.751565\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] Epoch[69] Batch[10] avg_epoch_loss=16.776825\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=16.8071369171\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] Epoch[69] Batch [10]#011Speed: 636.42 samples/sec#011loss=16.807137\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] processed a total of 681 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1183.9630603790283, \"sum\": 1183.9630603790283, \"min\": 1183.9630603790283}}, \"EndTime\": 1572452707.697722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452706.513387}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=575.137757597 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=69, train loss <loss>=16.7768251246\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] Epoch[70] Batch[0] avg_epoch_loss=16.812881\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=16.8128814697\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] Epoch[70] Batch[5] avg_epoch_loss=16.759011\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=16.7590109507\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] Epoch[70] Batch [5]#011Speed: 640.75 samples/sec#011loss=16.759011\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] processed a total of 635 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1095.1740741729736, \"sum\": 1095.1740741729736, \"min\": 1095.1740741729736}}, \"EndTime\": 1572452708.793332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452707.697792}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=579.76462721 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=70, train loss <loss>=16.7873960495\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] Epoch[71] Batch[0] avg_epoch_loss=16.805634\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=16.8056335449\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] Epoch[71] Batch[5] avg_epoch_loss=16.775645\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=16.7756446203\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] Epoch[71] Batch [5]#011Speed: 642.07 samples/sec#011loss=16.775645\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] processed a total of 630 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.0939235687256, \"sum\": 1089.0939235687256, \"min\": 1089.0939235687256}}, \"EndTime\": 1572452709.882815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452708.793401}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=578.41350256 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=71, train loss <loss>=16.7664201736\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:09 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:10 INFO 140089330636608] Epoch[72] Batch[0] avg_epoch_loss=16.676741\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=16.6767406464\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:10 INFO 140089330636608] Epoch[72] Batch[5] avg_epoch_loss=16.745869\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=16.7458693186\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:10 INFO 140089330636608] Epoch[72] Batch [5]#011Speed: 640.65 samples/sec#011loss=16.745869\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] Epoch[72] Batch[10] avg_epoch_loss=16.715658\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=16.6794048309\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] Epoch[72] Batch [10]#011Speed: 601.68 samples/sec#011loss=16.679405\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] processed a total of 643 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1221.595048904419, \"sum\": 1221.595048904419, \"min\": 1221.595048904419}}, \"EndTime\": 1572452711.104788, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452709.882878}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=526.320123634 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=72, train loss <loss>=16.7156581879\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] Epoch[73] Batch[0] avg_epoch_loss=16.707905\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=16.7079048157\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] Epoch[73] Batch[5] avg_epoch_loss=16.844277\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=16.8442773819\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:11 INFO 140089330636608] Epoch[73] Batch [5]#011Speed: 653.78 samples/sec#011loss=16.844277\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] Epoch[73] Batch[10] avg_epoch_loss=16.822721\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=16.7968544006\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] Epoch[73] Batch [10]#011Speed: 633.64 samples/sec#011loss=16.796854\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] processed a total of 645 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1174.1070747375488, \"sum\": 1174.1070747375488, \"min\": 1174.1070747375488}}, \"EndTime\": 1572452712.279358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452711.104852}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=549.306349297 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=73, train loss <loss>=16.8227214813\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] Epoch[74] Batch[0] avg_epoch_loss=16.808561\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=16.8085613251\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] Epoch[74] Batch[5] avg_epoch_loss=16.765389\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=16.7653891246\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:12 INFO 140089330636608] Epoch[74] Batch [5]#011Speed: 645.51 samples/sec#011loss=16.765389\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:13 INFO 140089330636608] processed a total of 635 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.0880870819092, \"sum\": 1084.0880870819092, \"min\": 1084.0880870819092}}, \"EndTime\": 1572452713.363889, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452712.279426}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:13 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=585.693849538 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:13 INFO 140089330636608] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=74, train loss <loss>=16.7818159103\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:13 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:13 INFO 140089330636608] Epoch[75] Batch[0] avg_epoch_loss=16.862604\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=16.8626041412\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] Epoch[75] Batch[5] avg_epoch_loss=16.825228\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=16.8252280553\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] Epoch[75] Batch [5]#011Speed: 627.22 samples/sec#011loss=16.825228\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] processed a total of 608 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1095.595121383667, \"sum\": 1095.595121383667, \"min\": 1095.595121383667}}, \"EndTime\": 1572452714.459957, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452713.363955}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=554.902920482 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=75, train loss <loss>=16.7895112991\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] Epoch[76] Batch[0] avg_epoch_loss=16.608055\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=16.6080551147\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] Epoch[76] Batch[5] avg_epoch_loss=16.749125\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=16.7491251628\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] Epoch[76] Batch [5]#011Speed: 637.90 samples/sec#011loss=16.749125\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] processed a total of 626 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.2560062408447, \"sum\": 1083.2560062408447, \"min\": 1083.2560062408447}}, \"EndTime\": 1572452715.543681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452714.460021}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=577.837167191 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=76, train loss <loss>=16.7627574921\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] Epoch[77] Batch[0] avg_epoch_loss=16.708019\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=16.7080192566\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] Epoch[77] Batch[5] avg_epoch_loss=16.786933\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=16.7869326274\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] Epoch[77] Batch [5]#011Speed: 636.47 samples/sec#011loss=16.786933\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] Epoch[77] Batch[10] avg_epoch_loss=16.769929\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=16.7495246887\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] Epoch[77] Batch [10]#011Speed: 637.61 samples/sec#011loss=16.749525\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] processed a total of 641 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1206.4080238342285, \"sum\": 1206.4080238342285, \"min\": 1206.4080238342285}}, \"EndTime\": 1572452716.750567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452715.543745}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=531.287043728 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=77, train loss <loss>=16.7699290189\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] Epoch[78] Batch[0] avg_epoch_loss=16.827698\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=16.8276977539\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] Epoch[78] Batch[5] avg_epoch_loss=16.745044\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=16.7450443904\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] Epoch[78] Batch [5]#011Speed: 631.80 samples/sec#011loss=16.745044\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1110.9318733215332, \"sum\": 1110.9318733215332, \"min\": 1110.9318733215332}}, \"EndTime\": 1572452717.861927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452716.75063}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=558.942790539 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=78, train loss <loss>=16.7645252228\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:17 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] Epoch[79] Batch[0] avg_epoch_loss=16.780561\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=16.7805614471\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] Epoch[79] Batch[5] avg_epoch_loss=16.781712\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=16.7817122142\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] Epoch[79] Batch [5]#011Speed: 644.57 samples/sec#011loss=16.781712\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] processed a total of 615 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.4550857543945, \"sum\": 1083.4550857543945, \"min\": 1083.4550857543945}}, \"EndTime\": 1572452718.94585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452717.861992}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.576181707 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=79, train loss <loss>=16.7912252426\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:18 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:19 INFO 140089330636608] Epoch[80] Batch[0] avg_epoch_loss=16.793718\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=16.793718338\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:19 INFO 140089330636608] Epoch[80] Batch[5] avg_epoch_loss=16.775309\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=16.7753092448\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:19 INFO 140089330636608] Epoch[80] Batch [5]#011Speed: 632.49 samples/sec#011loss=16.775309\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] Epoch[80] Batch[10] avg_epoch_loss=16.733175\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=16.6826129913\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] Epoch[80] Batch [10]#011Speed: 628.00 samples/sec#011loss=16.682613\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] processed a total of 645 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1212.8219604492188, \"sum\": 1212.8219604492188, \"min\": 1212.8219604492188}}, \"EndTime\": 1572452720.159106, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452718.945918}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=531.753886397 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=80, train loss <loss>=16.7331745841\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] Epoch[81] Batch[0] avg_epoch_loss=16.609962\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=16.6099624634\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] Epoch[81] Batch[5] avg_epoch_loss=16.727405\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=16.7274052302\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:20 INFO 140089330636608] Epoch[81] Batch [5]#011Speed: 633.42 samples/sec#011loss=16.727405\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] Epoch[81] Batch[10] avg_epoch_loss=16.728529\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=16.7298782349\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] Epoch[81] Batch [10]#011Speed: 628.98 samples/sec#011loss=16.729878\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] processed a total of 662 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1208.509922027588, \"sum\": 1208.509922027588, \"min\": 1208.509922027588}}, \"EndTime\": 1572452721.368122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452720.159171}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=547.740735889 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=81, train loss <loss>=16.7285293232\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] Epoch[82] Batch[0] avg_epoch_loss=16.761786\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=16.7617855072\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] Epoch[82] Batch[5] avg_epoch_loss=16.779312\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=16.7793124517\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] Epoch[82] Batch [5]#011Speed: 639.92 samples/sec#011loss=16.779312\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1092.1790599822998, \"sum\": 1092.1790599822998, \"min\": 1092.1790599822998}}, \"EndTime\": 1572452722.460746, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452721.36818}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=568.537075818 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=82, train loss <loss>=16.7808050156\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] Epoch[83] Batch[0] avg_epoch_loss=16.734339\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=16.7343387604\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] Epoch[83] Batch[5] avg_epoch_loss=16.752948\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=16.7529478073\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] Epoch[83] Batch [5]#011Speed: 634.03 samples/sec#011loss=16.752948\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.0578804016113, \"sum\": 1083.0578804016113, \"min\": 1083.0578804016113}}, \"EndTime\": 1572452723.544301, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452722.460812}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=577.020774544 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=83, train loss <loss>=16.7462818146\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] Epoch[84] Batch[0] avg_epoch_loss=16.753590\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=16.7535896301\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] Epoch[84] Batch[5] avg_epoch_loss=16.746002\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=16.7460021973\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] Epoch[84] Batch [5]#011Speed: 636.61 samples/sec#011loss=16.746002\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] processed a total of 607 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1077.9480934143066, \"sum\": 1077.9480934143066, \"min\": 1077.9480934143066}}, \"EndTime\": 1572452724.622728, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452723.544366}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.057824203 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=84, train loss <loss>=16.7485980988\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] Epoch[85] Batch[0] avg_epoch_loss=16.840435\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=16.8404350281\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] Epoch[85] Batch[5] avg_epoch_loss=16.752079\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=16.7520793279\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] Epoch[85] Batch [5]#011Speed: 642.74 samples/sec#011loss=16.752079\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1073.2090473175049, \"sum\": 1073.2090473175049, \"min\": 1073.2090473175049}}, \"EndTime\": 1572452725.696399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452724.622793}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=577.656085099 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=85, train loss <loss>=16.7406633377\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] Epoch[86] Batch[0] avg_epoch_loss=16.713478\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=16.7134780884\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] Epoch[86] Batch[5] avg_epoch_loss=16.733875\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=16.7338746389\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] Epoch[86] Batch [5]#011Speed: 635.92 samples/sec#011loss=16.733875\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] processed a total of 610 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1118.7329292297363, \"sum\": 1118.7329292297363, \"min\": 1118.7329292297363}}, \"EndTime\": 1572452726.8156, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452725.696463}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=545.21393654 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=86, train loss <loss>=16.7214265823\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] Epoch[87] Batch[0] avg_epoch_loss=16.735487\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=16.7354869843\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:27 INFO 140089330636608] Epoch[87] Batch[5] avg_epoch_loss=16.757097\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=16.7570966085\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:27 INFO 140089330636608] Epoch[87] Batch [5]#011Speed: 639.69 samples/sec#011loss=16.757097\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] Epoch[87] Batch[10] avg_epoch_loss=16.760386\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=16.7643341064\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] Epoch[87] Batch [10]#011Speed: 638.32 samples/sec#011loss=16.764334\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] processed a total of 674 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1185.171127319336, \"sum\": 1185.171127319336, \"min\": 1185.171127319336}}, \"EndTime\": 1572452728.001233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452726.815658}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=568.651576305 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=87, train loss <loss>=16.7603863803\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] Epoch[88] Batch[0] avg_epoch_loss=16.816275\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=16.8162746429\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] Epoch[88] Batch[5] avg_epoch_loss=16.754354\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=16.7543544769\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:28 INFO 140089330636608] Epoch[88] Batch [5]#011Speed: 635.05 samples/sec#011loss=16.754354\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] Epoch[88] Batch[10] avg_epoch_loss=16.753519\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=16.7525154114\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] Epoch[88] Batch [10]#011Speed: 611.69 samples/sec#011loss=16.752515\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] processed a total of 653 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1212.2399806976318, \"sum\": 1212.2399806976318, \"min\": 1212.2399806976318}}, \"EndTime\": 1572452729.213882, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452728.001292}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=538.630903738 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=88, train loss <loss>=16.753518538\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] Epoch[89] Batch[0] avg_epoch_loss=16.707447\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=16.707447052\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] Epoch[89] Batch[5] avg_epoch_loss=16.761455\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=16.7614549001\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:29 INFO 140089330636608] Epoch[89] Batch [5]#011Speed: 640.62 samples/sec#011loss=16.761455\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] Epoch[89] Batch[10] avg_epoch_loss=16.788709\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=16.8214134216\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] Epoch[89] Batch [10]#011Speed: 630.74 samples/sec#011loss=16.821413\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] processed a total of 646 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1201.4999389648438, \"sum\": 1201.4999389648438, \"min\": 1201.4999389648438}}, \"EndTime\": 1572452730.415725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452729.213944}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=537.620104757 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=89, train loss <loss>=16.7887087735\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] Epoch[90] Batch[0] avg_epoch_loss=16.771784\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=16.7717838287\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] Epoch[90] Batch[5] avg_epoch_loss=16.734934\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=16.7349335353\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] Epoch[90] Batch [5]#011Speed: 616.55 samples/sec#011loss=16.734934\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] processed a total of 613 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1110.8410358428955, \"sum\": 1110.8410358428955, \"min\": 1110.8410358428955}}, \"EndTime\": 1572452731.526968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452730.415789}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=551.786524486 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=90, train loss <loss>=16.7582567215\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] Epoch[91] Batch[0] avg_epoch_loss=16.717155\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=16.7171554565\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] Epoch[91] Batch[5] avg_epoch_loss=16.739199\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=16.7391993205\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] Epoch[91] Batch [5]#011Speed: 617.17 samples/sec#011loss=16.739199\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] processed a total of 635 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1126.1260509490967, \"sum\": 1126.1260509490967, \"min\": 1126.1260509490967}}, \"EndTime\": 1572452732.653482, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452731.527032}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.83157261 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=91, train loss <loss>=16.758152771\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] Epoch[92] Batch[0] avg_epoch_loss=16.582613\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=16.5826129913\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] Epoch[92] Batch[5] avg_epoch_loss=16.703812\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=16.7038122813\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] Epoch[92] Batch [5]#011Speed: 630.51 samples/sec#011loss=16.703812\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] Epoch[92] Batch[10] avg_epoch_loss=16.721139\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=16.7419300079\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] Epoch[92] Batch [10]#011Speed: 620.61 samples/sec#011loss=16.741930\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] processed a total of 655 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1215.6169414520264, \"sum\": 1215.6169414520264, \"min\": 1215.6169414520264}}, \"EndTime\": 1572452733.869575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452732.653549}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=538.781637328 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=92, train loss <loss>=16.7211385207\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:33 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:34 INFO 140089330636608] Epoch[93] Batch[0] avg_epoch_loss=16.705109\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=16.7051086426\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:34 INFO 140089330636608] Epoch[93] Batch[5] avg_epoch_loss=16.728467\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=16.7284669876\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:34 INFO 140089330636608] Epoch[93] Batch [5]#011Speed: 628.70 samples/sec#011loss=16.728467\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] Epoch[93] Batch[10] avg_epoch_loss=16.757433\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=16.7921924591\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] Epoch[93] Batch [10]#011Speed: 639.87 samples/sec#011loss=16.792192\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] processed a total of 661 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1201.5459537506104, \"sum\": 1201.5459537506104, \"min\": 1201.5459537506104}}, \"EndTime\": 1572452735.071443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452733.869635}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=550.069163322 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=93, train loss <loss>=16.757433111\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] Epoch[94] Batch[0] avg_epoch_loss=16.708958\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=16.7089576721\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] Epoch[94] Batch[5] avg_epoch_loss=16.718016\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=16.7180163066\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:35 INFO 140089330636608] Epoch[94] Batch [5]#011Speed: 636.29 samples/sec#011loss=16.718016\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] processed a total of 629 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1108.626127243042, \"sum\": 1108.626127243042, \"min\": 1108.626127243042}}, \"EndTime\": 1572452736.180449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452735.071535}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.321451673 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=94, train loss <loss>=16.7145359039\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_d415cf2b-1016-4332-be12-9158340ec13a-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.68993377685547, \"sum\": 27.68993377685547, \"min\": 27.68993377685547}}, \"EndTime\": 1572452736.208661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452736.180514}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] Epoch[95] Batch[0] avg_epoch_loss=16.777782\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=16.7777824402\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] Epoch[95] Batch[5] avg_epoch_loss=16.650587\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=16.650586764\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:36 INFO 140089330636608] Epoch[95] Batch [5]#011Speed: 641.48 samples/sec#011loss=16.650587\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1094.877004623413, \"sum\": 1094.877004623413, \"min\": 1094.877004623413}}, \"EndTime\": 1572452737.303658, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452736.208723}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=580.835181718 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=95, train loss <loss>=16.6877185822\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_a1bfb2c4-e52b-4b74-b8cf-b52f63e0bbcd-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.118921279907227, \"sum\": 27.118921279907227, \"min\": 27.118921279907227}}, \"EndTime\": 1572452737.331204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452737.303725}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] Epoch[96] Batch[0] avg_epoch_loss=16.727190\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=16.7271900177\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] Epoch[96] Batch[5] avg_epoch_loss=16.721089\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=16.7210887273\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] Epoch[96] Batch [5]#011Speed: 632.47 samples/sec#011loss=16.721089\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1093.5111045837402, \"sum\": 1093.5111045837402, \"min\": 1093.5111045837402}}, \"EndTime\": 1572452738.424837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452737.331267}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.837402635 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=96, train loss <loss>=16.7140399933\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] Epoch[97] Batch[0] avg_epoch_loss=16.807676\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=16.8076763153\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] Epoch[97] Batch[5] avg_epoch_loss=16.663854\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=16.6638536453\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] Epoch[97] Batch [5]#011Speed: 640.32 samples/sec#011loss=16.663854\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] Epoch[97] Batch[10] avg_epoch_loss=16.700413\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=16.7442832947\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] Epoch[97] Batch [10]#011Speed: 626.17 samples/sec#011loss=16.744283\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] processed a total of 651 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1191.183090209961, \"sum\": 1191.183090209961, \"min\": 1191.183090209961}}, \"EndTime\": 1572452739.616512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452738.424913}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=546.471511548 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=97, train loss <loss>=16.7004125768\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] Epoch[98] Batch[0] avg_epoch_loss=16.772047\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=16.7720470428\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] Epoch[98] Batch[5] avg_epoch_loss=16.725244\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=16.7252442042\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] Epoch[98] Batch [5]#011Speed: 645.46 samples/sec#011loss=16.725244\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] Epoch[98] Batch[10] avg_epoch_loss=16.720521\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=16.7148525238\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] Epoch[98] Batch [10]#011Speed: 643.11 samples/sec#011loss=16.714853\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] processed a total of 666 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1176.4061450958252, \"sum\": 1176.4061450958252, \"min\": 1176.4061450958252}}, \"EndTime\": 1572452740.793358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452739.616577}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=566.086840852 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=98, train loss <loss>=16.7205207131\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] Epoch[99] Batch[0] avg_epoch_loss=16.757504\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=16.7575035095\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] Epoch[99] Batch[5] avg_epoch_loss=16.732421\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=16.7324212392\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] Epoch[99] Batch [5]#011Speed: 640.46 samples/sec#011loss=16.732421\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] processed a total of 614 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.8588638305664, \"sum\": 1091.8588638305664, \"min\": 1091.8588638305664}}, \"EndTime\": 1572452741.885636, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452740.793419}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=562.292788018 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=99, train loss <loss>=16.7428699493\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:41 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:42 INFO 140089330636608] Epoch[100] Batch[0] avg_epoch_loss=16.613283\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=16.6132831573\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:25:42 INFO 140089330636608] Epoch[100] Batch[5] avg_epoch_loss=16.707391\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=16.7073907852\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:42 INFO 140089330636608] Epoch[100] Batch [5]#011Speed: 627.97 samples/sec#011loss=16.707391\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] Epoch[100] Batch[10] avg_epoch_loss=16.698054\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=16.6868492126\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] Epoch[100] Batch [10]#011Speed: 632.30 samples/sec#011loss=16.686849\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] processed a total of 649 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1204.200029373169, \"sum\": 1204.200029373169, \"min\": 1204.200029373169}}, \"EndTime\": 1572452743.090218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452741.885702}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=538.905820378 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=100, train loss <loss>=16.6980537068\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] Epoch[101] Batch[0] avg_epoch_loss=16.769709\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=16.7697086334\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] Epoch[101] Batch[5] avg_epoch_loss=16.680442\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=16.6804424922\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:43 INFO 140089330636608] Epoch[101] Batch [5]#011Speed: 642.20 samples/sec#011loss=16.680442\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1107.6741218566895, \"sum\": 1107.6741218566895, \"min\": 1107.6741218566895}}, \"EndTime\": 1572452744.198241, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452743.090282}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=574.127404163 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=101, train loss <loss>=16.7190973282\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] Epoch[102] Batch[0] avg_epoch_loss=16.661501\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=16.6615009308\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] Epoch[102] Batch[5] avg_epoch_loss=16.693838\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=16.6938378016\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:44 INFO 140089330636608] Epoch[102] Batch [5]#011Speed: 646.73 samples/sec#011loss=16.693838\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] Epoch[102] Batch[10] avg_epoch_loss=16.699468\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=16.7062232971\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] Epoch[102] Batch [10]#011Speed: 627.83 samples/sec#011loss=16.706223\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] processed a total of 663 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1215.6040668487549, \"sum\": 1215.6040668487549, \"min\": 1215.6040668487549}}, \"EndTime\": 1572452745.414252, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452744.198304}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=545.350839533 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=102, train loss <loss>=16.6994675723\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] Epoch[103] Batch[0] avg_epoch_loss=16.593079\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=16.5930786133\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] Epoch[103] Batch[5] avg_epoch_loss=16.712337\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=16.712337176\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] Epoch[103] Batch [5]#011Speed: 616.32 samples/sec#011loss=16.712337\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] Epoch[103] Batch[10] avg_epoch_loss=16.719555\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=16.7282157898\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] Epoch[103] Batch [10]#011Speed: 640.51 samples/sec#011loss=16.728216\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] processed a total of 677 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1213.212013244629, \"sum\": 1213.212013244629, \"min\": 1213.212013244629}}, \"EndTime\": 1572452746.627937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452745.414314}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=557.981489724 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=103, train loss <loss>=16.7195547277\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] Epoch[104] Batch[0] avg_epoch_loss=16.764645\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=16.7646446228\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] Epoch[104] Batch[5] avg_epoch_loss=16.719130\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=16.7191298803\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] Epoch[104] Batch [5]#011Speed: 635.68 samples/sec#011loss=16.719130\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] processed a total of 618 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1111.407995223999, \"sum\": 1111.407995223999, \"min\": 1111.407995223999}}, \"EndTime\": 1572452747.739673, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452746.627998}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=555.996321786 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=104, train loss <loss>=16.7017578125\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] Epoch[105] Batch[0] avg_epoch_loss=16.734921\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=16.7349205017\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] Epoch[105] Batch[5] avg_epoch_loss=16.744302\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=16.7443021139\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] Epoch[105] Batch [5]#011Speed: 646.06 samples/sec#011loss=16.744302\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] processed a total of 610 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1069.728136062622, \"sum\": 1069.728136062622, \"min\": 1069.728136062622}}, \"EndTime\": 1572452748.809809, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452747.739752}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=570.184079487 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=105, train loss <loss>=16.7479372025\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] Epoch[106] Batch[0] avg_epoch_loss=16.749508\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=16.7495079041\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] Epoch[106] Batch[5] avg_epoch_loss=16.728914\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=16.7289142609\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] Epoch[106] Batch [5]#011Speed: 622.01 samples/sec#011loss=16.728914\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] processed a total of 637 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1093.4200286865234, \"sum\": 1093.4200286865234, \"min\": 1093.4200286865234}}, \"EndTime\": 1572452749.903715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452748.80988}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=582.525075683 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=106, train loss <loss>=16.7420124054\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:49 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:50 INFO 140089330636608] Epoch[107] Batch[0] avg_epoch_loss=16.761272\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=16.7612724304\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:50 INFO 140089330636608] Epoch[107] Batch[5] avg_epoch_loss=16.674922\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=16.6749223073\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:50 INFO 140089330636608] Epoch[107] Batch [5]#011Speed: 634.96 samples/sec#011loss=16.674922\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] Epoch[107] Batch[10] avg_epoch_loss=16.653394\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=16.6275608063\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] Epoch[107] Batch [10]#011Speed: 623.71 samples/sec#011loss=16.627561\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] processed a total of 662 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1205.8680057525635, \"sum\": 1205.8680057525635, \"min\": 1205.8680057525635}}, \"EndTime\": 1572452751.109958, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452749.903779}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=548.934386959 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=107, train loss <loss>=16.6533943523\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_e5429261-acee-40cc-9809-3cc7b8d78687-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.348953247070312, \"sum\": 21.348953247070312, \"min\": 21.348953247070312}}, \"EndTime\": 1572452751.13178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452751.110029}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] Epoch[108] Batch[0] avg_epoch_loss=16.793369\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=16.7933692932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] Epoch[108] Batch[5] avg_epoch_loss=16.793076\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=16.7930761973\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:51 INFO 140089330636608] Epoch[108] Batch [5]#011Speed: 643.19 samples/sec#011loss=16.793076\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] Epoch[108] Batch[10] avg_epoch_loss=16.747555\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=16.6929298401\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] Epoch[108] Batch [10]#011Speed: 631.19 samples/sec#011loss=16.692930\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] processed a total of 663 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1184.997797012329, \"sum\": 1184.997797012329, \"min\": 1184.997797012329}}, \"EndTime\": 1572452752.316897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452751.131845}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=559.45205065 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=108, train loss <loss>=16.7475551258\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] Epoch[109] Batch[0] avg_epoch_loss=16.810448\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=16.8104476929\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] Epoch[109] Batch[5] avg_epoch_loss=16.772695\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=16.7726952235\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] Epoch[109] Batch [5]#011Speed: 641.96 samples/sec#011loss=16.772695\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] Epoch[109] Batch[10] avg_epoch_loss=16.792540\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=16.8163528442\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] Epoch[109] Batch [10]#011Speed: 628.60 samples/sec#011loss=16.816353\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] processed a total of 670 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1211.057186126709, \"sum\": 1211.057186126709, \"min\": 1211.057186126709}}, \"EndTime\": 1572452753.528284, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452752.316958}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=553.194148809 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=109, train loss <loss>=16.7925395966\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] Epoch[110] Batch[0] avg_epoch_loss=16.689404\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=16.6894035339\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] Epoch[110] Batch[5] avg_epoch_loss=16.691153\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=16.6911528905\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] Epoch[110] Batch [5]#011Speed: 597.28 samples/sec#011loss=16.691153\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] Epoch[110] Batch[10] avg_epoch_loss=16.699151\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=16.7087482452\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] Epoch[110] Batch [10]#011Speed: 631.37 samples/sec#011loss=16.708748\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] processed a total of 641 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1229.4349670410156, \"sum\": 1229.4349670410156, \"min\": 1229.4349670410156}}, \"EndTime\": 1572452754.758062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452753.528346}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=521.337380888 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=110, train loss <loss>=16.699150779\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] Epoch[111] Batch[0] avg_epoch_loss=16.719414\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=16.7194137573\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] Epoch[111] Batch[5] avg_epoch_loss=16.721152\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=16.7211519877\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] Epoch[111] Batch [5]#011Speed: 645.88 samples/sec#011loss=16.721152\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1073.8229751586914, \"sum\": 1073.8229751586914, \"min\": 1073.8229751586914}}, \"EndTime\": 1572452755.832307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452754.758126}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=547.53033689 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=111, train loss <loss>=16.7247083664\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:55 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] Epoch[112] Batch[0] avg_epoch_loss=16.768452\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=16.7684516907\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] Epoch[112] Batch[5] avg_epoch_loss=16.724908\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=16.7249078751\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] Epoch[112] Batch [5]#011Speed: 626.03 samples/sec#011loss=16.724908\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] processed a total of 631 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.2940502166748, \"sum\": 1091.2940502166748, \"min\": 1091.2940502166748}}, \"EndTime\": 1572452756.924105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452755.832368}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=578.16172327 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=112, train loss <loss>=16.7386617661\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:56 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:57 INFO 140089330636608] Epoch[113] Batch[0] avg_epoch_loss=16.782278\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=16.7822780609\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:25:57 INFO 140089330636608] Epoch[113] Batch[5] avg_epoch_loss=16.728001\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=16.7280009588\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:57 INFO 140089330636608] Epoch[113] Batch [5]#011Speed: 640.87 samples/sec#011loss=16.728001\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] Epoch[113] Batch[10] avg_epoch_loss=16.714339\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=16.6979438782\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] Epoch[113] Batch [10]#011Speed: 630.88 samples/sec#011loss=16.697944\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] processed a total of 689 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1194.883108139038, \"sum\": 1194.883108139038, \"min\": 1194.883108139038}}, \"EndTime\": 1572452758.119512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452756.924165}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=576.570450269 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=113, train loss <loss>=16.7143386494\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] Epoch[114] Batch[0] avg_epoch_loss=16.744854\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=16.7448539734\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] Epoch[114] Batch[5] avg_epoch_loss=16.709249\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=16.7092491786\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:58 INFO 140089330636608] Epoch[114] Batch [5]#011Speed: 635.28 samples/sec#011loss=16.709249\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] Epoch[114] Batch[10] avg_epoch_loss=16.712239\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=16.7158267975\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] Epoch[114] Batch [10]#011Speed: 640.80 samples/sec#011loss=16.715827\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] processed a total of 646 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1196.3210105895996, \"sum\": 1196.3210105895996, \"min\": 1196.3210105895996}}, \"EndTime\": 1572452759.316245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452758.119576}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=539.947415356 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=114, train loss <loss>=16.7122390053\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] Epoch[115] Batch[0] avg_epoch_loss=16.787338\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:25:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=16.7873382568\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] Epoch[115] Batch[5] avg_epoch_loss=16.715758\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=16.7157583237\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] Epoch[115] Batch [5]#011Speed: 647.34 samples/sec#011loss=16.715758\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] Epoch[115] Batch[10] avg_epoch_loss=16.686161\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=16.6506439209\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] Epoch[115] Batch [10]#011Speed: 636.69 samples/sec#011loss=16.650644\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] processed a total of 645 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1204.7719955444336, \"sum\": 1204.7719955444336, \"min\": 1204.7719955444336}}, \"EndTime\": 1572452760.521395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452759.316307}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=535.33181229 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=115, train loss <loss>=16.6861608679\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] Epoch[116] Batch[0] avg_epoch_loss=16.779358\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=16.7793579102\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] Epoch[116] Batch[5] avg_epoch_loss=16.713019\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=16.713019371\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] Epoch[116] Batch [5]#011Speed: 623.28 samples/sec#011loss=16.713019\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] Epoch[116] Batch[10] avg_epoch_loss=16.761860\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=16.8204685211\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] Epoch[116] Batch [10]#011Speed: 638.52 samples/sec#011loss=16.820469\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] processed a total of 656 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1213.0279541015625, \"sum\": 1213.0279541015625, \"min\": 1213.0279541015625}}, \"EndTime\": 1572452761.734748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452760.521456}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=540.757082558 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=116, train loss <loss>=16.7618598938\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] Epoch[117] Batch[0] avg_epoch_loss=16.574871\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=16.5748710632\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] Epoch[117] Batch[5] avg_epoch_loss=16.691275\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=16.6912752787\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] Epoch[117] Batch [5]#011Speed: 635.14 samples/sec#011loss=16.691275\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] Epoch[117] Batch[10] avg_epoch_loss=16.691139\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=16.6909744263\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] Epoch[117] Batch [10]#011Speed: 622.03 samples/sec#011loss=16.690974\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] processed a total of 649 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1232.8219413757324, \"sum\": 1232.8219413757324, \"min\": 1232.8219413757324}}, \"EndTime\": 1572452762.967897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452761.734808}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=526.395202985 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=117, train loss <loss>=16.6911385276\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:02 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:03 INFO 140089330636608] Epoch[118] Batch[0] avg_epoch_loss=16.773310\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=16.7733097076\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:03 INFO 140089330636608] Epoch[118] Batch[5] avg_epoch_loss=16.725196\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=16.7251955668\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:03 INFO 140089330636608] Epoch[118] Batch [5]#011Speed: 636.21 samples/sec#011loss=16.725196\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] Epoch[118] Batch[10] avg_epoch_loss=16.717660\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=16.7086162567\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] Epoch[118] Batch [10]#011Speed: 627.29 samples/sec#011loss=16.708616\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] processed a total of 650 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1213.1578922271729, \"sum\": 1213.1578922271729, \"min\": 1213.1578922271729}}, \"EndTime\": 1572452764.181391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452762.967961}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=535.752485003 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=118, train loss <loss>=16.7176595168\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] Epoch[119] Batch[0] avg_epoch_loss=16.779955\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=16.7799549103\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] Epoch[119] Batch[5] avg_epoch_loss=16.723371\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=16.7233705521\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:04 INFO 140089330636608] Epoch[119] Batch [5]#011Speed: 627.09 samples/sec#011loss=16.723371\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] processed a total of 639 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1106.537103652954, \"sum\": 1106.537103652954, \"min\": 1106.537103652954}}, \"EndTime\": 1572452765.288252, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452764.181452}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=577.43034215 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=119, train loss <loss>=16.7093637466\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] Epoch[120] Batch[0] avg_epoch_loss=16.674850\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=16.6748504639\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] Epoch[120] Batch[5] avg_epoch_loss=16.729955\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=16.7299547195\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:05 INFO 140089330636608] Epoch[120] Batch [5]#011Speed: 636.94 samples/sec#011loss=16.729955\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:06 INFO 140089330636608] processed a total of 615 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1101.6499996185303, \"sum\": 1101.6499996185303, \"min\": 1101.6499996185303}}, \"EndTime\": 1572452766.390318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452765.288314}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:06 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=558.199287443 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:06 INFO 140089330636608] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=120, train loss <loss>=16.7128194809\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:06 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:06 INFO 140089330636608] Epoch[121] Batch[0] avg_epoch_loss=16.758636\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=16.7586364746\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] Epoch[121] Batch[5] avg_epoch_loss=16.726807\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=16.7268066406\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] Epoch[121] Batch [5]#011Speed: 642.97 samples/sec#011loss=16.726807\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] Epoch[121] Batch[10] avg_epoch_loss=16.766324\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=16.8137454987\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] Epoch[121] Batch [10]#011Speed: 630.79 samples/sec#011loss=16.813745\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] processed a total of 644 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1191.2388801574707, \"sum\": 1191.2388801574707, \"min\": 1191.2388801574707}}, \"EndTime\": 1572452767.582139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452766.390385}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=540.572754221 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=121, train loss <loss>=16.7663243034\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] Epoch[122] Batch[0] avg_epoch_loss=16.774031\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=16.7740306854\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] Epoch[122] Batch[5] avg_epoch_loss=16.656130\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=16.6561304728\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] Epoch[122] Batch [5]#011Speed: 631.54 samples/sec#011loss=16.656130\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1113.4400367736816, \"sum\": 1113.4400367736816, \"min\": 1113.4400367736816}}, \"EndTime\": 1572452768.695906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452767.5822}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=526.251452251 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=122, train loss <loss>=16.6520626068\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_9a12435b-d327-43b3-919c-2ed55a71d2f1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.87215805053711, \"sum\": 26.87215805053711, \"min\": 26.87215805053711}}, \"EndTime\": 1572452768.723203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452768.695972}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] Epoch[123] Batch[0] avg_epoch_loss=16.699730\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=16.6997299194\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] Epoch[123] Batch[5] avg_epoch_loss=16.701042\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=16.7010421753\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] Epoch[123] Batch [5]#011Speed: 640.90 samples/sec#011loss=16.701042\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] Epoch[123] Batch[10] avg_epoch_loss=16.725669\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=16.7552204132\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] Epoch[123] Batch [10]#011Speed: 632.43 samples/sec#011loss=16.755220\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] processed a total of 669 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1202.8629779815674, \"sum\": 1202.8629779815674, \"min\": 1202.8629779815674}}, \"EndTime\": 1572452769.926181, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452768.723263}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=556.131076191 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=123, train loss <loss>=16.7256686471\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:09 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:10 INFO 140089330636608] Epoch[124] Batch[0] avg_epoch_loss=16.730957\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=16.7309570312\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:10 INFO 140089330636608] Epoch[124] Batch[5] avg_epoch_loss=16.685096\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=16.685095787\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:10 INFO 140089330636608] Epoch[124] Batch [5]#011Speed: 637.08 samples/sec#011loss=16.685096\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] Epoch[124] Batch[10] avg_epoch_loss=16.712729\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=16.7458881378\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] Epoch[124] Batch [10]#011Speed: 635.68 samples/sec#011loss=16.745888\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] processed a total of 660 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1193.1180953979492, \"sum\": 1193.1180953979492, \"min\": 1193.1180953979492}}, \"EndTime\": 1572452771.119632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452769.926241}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=553.128408547 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=124, train loss <loss>=16.7127286738\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] Epoch[125] Batch[0] avg_epoch_loss=16.649008\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=16.6490077972\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] Epoch[125] Batch[5] avg_epoch_loss=16.707570\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=16.7075697581\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:11 INFO 140089330636608] Epoch[125] Batch [5]#011Speed: 629.38 samples/sec#011loss=16.707570\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] processed a total of 627 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1126.8460750579834, \"sum\": 1126.8460750579834, \"min\": 1126.8460750579834}}, \"EndTime\": 1572452772.246818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452771.119696}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=556.373891543 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=125, train loss <loss>=16.7167032242\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] Epoch[126] Batch[0] avg_epoch_loss=16.711023\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=16.7110233307\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] Epoch[126] Batch[5] avg_epoch_loss=16.704159\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=16.704158783\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:12 INFO 140089330636608] Epoch[126] Batch [5]#011Speed: 638.11 samples/sec#011loss=16.704159\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:13 INFO 140089330636608] processed a total of 622 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1092.1189785003662, \"sum\": 1092.1189785003662, \"min\": 1092.1189785003662}}, \"EndTime\": 1572452773.339318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452772.246881}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:13 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=569.483919329 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:13 INFO 140089330636608] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=126, train loss <loss>=16.7245779037\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:13 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:13 INFO 140089330636608] Epoch[127] Batch[0] avg_epoch_loss=16.671741\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=16.6717414856\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] Epoch[127] Batch[5] avg_epoch_loss=16.686610\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=16.6866095861\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] Epoch[127] Batch [5]#011Speed: 624.24 samples/sec#011loss=16.686610\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] processed a total of 637 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1108.483076095581, \"sum\": 1108.483076095581, \"min\": 1108.483076095581}}, \"EndTime\": 1572452774.448204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452773.339385}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=574.611496094 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=127, train loss <loss>=16.6872612\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] Epoch[128] Batch[0] avg_epoch_loss=16.746674\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=16.746673584\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] Epoch[128] Batch[5] avg_epoch_loss=16.695335\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=16.6953353882\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] Epoch[128] Batch [5]#011Speed: 639.36 samples/sec#011loss=16.695335\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] Epoch[128] Batch[10] avg_epoch_loss=16.697566\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=16.7002426147\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] Epoch[128] Batch [10]#011Speed: 637.58 samples/sec#011loss=16.700243\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] processed a total of 644 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1196.861982345581, \"sum\": 1196.861982345581, \"min\": 1196.861982345581}}, \"EndTime\": 1572452775.645439, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452774.448267}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=538.031938137 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=128, train loss <loss>=16.6975659457\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] Epoch[129] Batch[0] avg_epoch_loss=16.666096\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=16.6660957336\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] Epoch[129] Batch[5] avg_epoch_loss=16.694267\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=16.6942672729\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] Epoch[129] Batch [5]#011Speed: 620.31 samples/sec#011loss=16.694267\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] processed a total of 619 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1097.8670120239258, \"sum\": 1097.8670120239258, \"min\": 1097.8670120239258}}, \"EndTime\": 1572452776.743779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452775.6455}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.764615346 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=129, train loss <loss>=16.7027181625\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] Epoch[130] Batch[0] avg_epoch_loss=16.734013\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=16.7340126038\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] Epoch[130] Batch[5] avg_epoch_loss=16.695081\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=16.6950807571\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] Epoch[130] Batch [5]#011Speed: 641.19 samples/sec#011loss=16.695081\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] processed a total of 629 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1094.933032989502, \"sum\": 1094.933032989502, \"min\": 1094.933032989502}}, \"EndTime\": 1572452777.839185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452776.743854}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=574.408322356 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=130, train loss <loss>=16.6996700287\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:17 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:18 INFO 140089330636608] Epoch[131] Batch[0] avg_epoch_loss=16.684593\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=16.6845932007\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:18 INFO 140089330636608] Epoch[131] Batch[5] avg_epoch_loss=16.745953\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=16.7459529241\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:18 INFO 140089330636608] Epoch[131] Batch [5]#011Speed: 645.24 samples/sec#011loss=16.745953\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] Epoch[131] Batch[10] avg_epoch_loss=16.711233\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=16.6695690155\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] Epoch[131] Batch [10]#011Speed: 632.64 samples/sec#011loss=16.669569\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] processed a total of 684 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1188.7941360473633, \"sum\": 1188.7941360473633, \"min\": 1188.7941360473633}}, \"EndTime\": 1572452779.028538, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452777.839258}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=575.324613267 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=131, train loss <loss>=16.7112329656\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] Epoch[132] Batch[0] avg_epoch_loss=16.767189\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=16.7671890259\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] Epoch[132] Batch[5] avg_epoch_loss=16.723148\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=16.7231483459\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:19 INFO 140089330636608] Epoch[132] Batch [5]#011Speed: 631.08 samples/sec#011loss=16.723148\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] Epoch[132] Batch[10] avg_epoch_loss=16.699431\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=16.6709701538\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] Epoch[132] Batch [10]#011Speed: 640.32 samples/sec#011loss=16.670970\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] processed a total of 672 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1195.9059238433838, \"sum\": 1195.9059238433838, \"min\": 1195.9059238433838}}, \"EndTime\": 1572452780.224905, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452779.028605}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=561.875775313 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=132, train loss <loss>=16.6994309859\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] Epoch[133] Batch[0] avg_epoch_loss=16.721497\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=16.721496582\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] Epoch[133] Batch[5] avg_epoch_loss=16.704495\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=16.7044951121\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:20 INFO 140089330636608] Epoch[133] Batch [5]#011Speed: 642.79 samples/sec#011loss=16.704495\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:21 INFO 140089330636608] processed a total of 639 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1079.3800354003906, \"sum\": 1079.3800354003906, \"min\": 1079.3800354003906}}, \"EndTime\": 1572452781.304682, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452780.224965}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:21 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=591.955508811 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:21 INFO 140089330636608] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=133, train loss <loss>=16.6990989685\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:21 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:21 INFO 140089330636608] Epoch[134] Batch[0] avg_epoch_loss=16.653475\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=16.6534748077\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] Epoch[134] Batch[5] avg_epoch_loss=16.664784\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=16.6647837957\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] Epoch[134] Batch [5]#011Speed: 633.88 samples/sec#011loss=16.664784\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] processed a total of 630 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1105.970859527588, \"sum\": 1105.970859527588, \"min\": 1105.970859527588}}, \"EndTime\": 1572452782.411096, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452781.304744}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=569.575465079 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=134, train loss <loss>=16.6779508591\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] Epoch[135] Batch[0] avg_epoch_loss=16.621685\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=16.6216850281\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] Epoch[135] Batch[5] avg_epoch_loss=16.674258\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=16.6742579142\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] Epoch[135] Batch [5]#011Speed: 646.43 samples/sec#011loss=16.674258\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] Epoch[135] Batch[10] avg_epoch_loss=16.673519\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=16.6726329803\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] Epoch[135] Batch [10]#011Speed: 644.20 samples/sec#011loss=16.672633\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] processed a total of 674 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1173.017978668213, \"sum\": 1173.017978668213, \"min\": 1173.017978668213}}, \"EndTime\": 1572452783.584654, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452782.411179}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=574.534867453 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=135, train loss <loss>=16.6735193079\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] Epoch[136] Batch[0] avg_epoch_loss=16.714155\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=16.7141551971\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] Epoch[136] Batch[5] avg_epoch_loss=16.678037\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=16.6780366898\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] Epoch[136] Batch [5]#011Speed: 642.41 samples/sec#011loss=16.678037\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] Epoch[136] Batch[10] avg_epoch_loss=16.677433\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=16.67670784\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] Epoch[136] Batch [10]#011Speed: 637.14 samples/sec#011loss=16.676708\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] processed a total of 648 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1182.772159576416, \"sum\": 1182.772159576416, \"min\": 1182.772159576416}}, \"EndTime\": 1572452784.767877, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452783.584724}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=547.824369447 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=136, train loss <loss>=16.6774326671\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] Epoch[137] Batch[0] avg_epoch_loss=16.616505\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=16.6165046692\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] Epoch[137] Batch[5] avg_epoch_loss=16.693370\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=16.6933701833\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] Epoch[137] Batch [5]#011Speed: 652.65 samples/sec#011loss=16.693370\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] Epoch[137] Batch[10] avg_epoch_loss=16.685139\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=16.6752613068\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] Epoch[137] Batch [10]#011Speed: 641.20 samples/sec#011loss=16.675261\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] processed a total of 648 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1170.5892086029053, \"sum\": 1170.5892086029053, \"min\": 1170.5892086029053}}, \"EndTime\": 1572452785.938885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452784.767939}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=553.522058226 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=137, train loss <loss>=16.6851388758\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:25 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:26 INFO 140089330636608] Epoch[138] Batch[0] avg_epoch_loss=16.697847\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=16.6978473663\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:26 INFO 140089330636608] Epoch[138] Batch[5] avg_epoch_loss=16.657081\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=16.6570806503\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:26 INFO 140089330636608] Epoch[138] Batch [5]#011Speed: 631.63 samples/sec#011loss=16.657081\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] Epoch[138] Batch[10] avg_epoch_loss=16.704674\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=16.7617862701\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] Epoch[138] Batch [10]#011Speed: 626.77 samples/sec#011loss=16.761786\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] processed a total of 654 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1196.8400478363037, \"sum\": 1196.8400478363037, \"min\": 1196.8400478363037}}, \"EndTime\": 1572452787.13616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452785.938952}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=546.392892584 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=138, train loss <loss>=16.7046741139\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] Epoch[139] Batch[0] avg_epoch_loss=16.702724\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=16.7027244568\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] Epoch[139] Batch[5] avg_epoch_loss=16.705099\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=16.7050987879\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:27 INFO 140089330636608] Epoch[139] Batch [5]#011Speed: 647.48 samples/sec#011loss=16.705099\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] processed a total of 628 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.5828666687012, \"sum\": 1070.5828666687012, \"min\": 1070.5828666687012}}, \"EndTime\": 1572452788.207211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452787.136227}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=586.545927438 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=139, train loss <loss>=16.7008245468\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] Epoch[140] Batch[0] avg_epoch_loss=16.782434\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=16.7824344635\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] Epoch[140] Batch[5] avg_epoch_loss=16.684284\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=16.6842835744\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:28 INFO 140089330636608] Epoch[140] Batch [5]#011Speed: 639.35 samples/sec#011loss=16.684284\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] Epoch[140] Batch[10] avg_epoch_loss=16.710749\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=16.742508316\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] Epoch[140] Batch [10]#011Speed: 632.69 samples/sec#011loss=16.742508\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] processed a total of 666 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1186.6061687469482, \"sum\": 1186.6061687469482, \"min\": 1186.6061687469482}}, \"EndTime\": 1572452789.394314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452788.207275}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=561.21923844 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=140, train loss <loss>=16.7107493661\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] Epoch[141] Batch[0] avg_epoch_loss=16.674221\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=16.6742210388\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] Epoch[141] Batch[5] avg_epoch_loss=16.716066\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=16.7160657247\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] Epoch[141] Batch [5]#011Speed: 629.81 samples/sec#011loss=16.716066\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] Epoch[141] Batch[10] avg_epoch_loss=16.698690\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=16.6778400421\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] Epoch[141] Batch [10]#011Speed: 642.66 samples/sec#011loss=16.677840\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] processed a total of 667 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1190.5977725982666, \"sum\": 1190.5977725982666, \"min\": 1190.5977725982666}}, \"EndTime\": 1572452790.585365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452789.394378}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=560.174661675 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=141, train loss <loss>=16.6986904144\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] Epoch[142] Batch[0] avg_epoch_loss=16.738840\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=16.7388401031\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] Epoch[142] Batch[5] avg_epoch_loss=16.705358\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=16.7053575516\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] Epoch[142] Batch [5]#011Speed: 644.56 samples/sec#011loss=16.705358\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] Epoch[142] Batch[10] avg_epoch_loss=16.718527\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=16.734331131\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] Epoch[142] Batch [10]#011Speed: 619.73 samples/sec#011loss=16.734331\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] processed a total of 681 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1199.4378566741943, \"sum\": 1199.4378566741943, \"min\": 1199.4378566741943}}, \"EndTime\": 1572452791.785264, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452790.585436}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.720944839 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=142, train loss <loss>=16.7185273604\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] Epoch[143] Batch[0] avg_epoch_loss=16.715500\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=16.7154998779\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] Epoch[143] Batch[5] avg_epoch_loss=16.689176\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=16.6891759237\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] Epoch[143] Batch [5]#011Speed: 627.41 samples/sec#011loss=16.689176\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1103.6438941955566, \"sum\": 1103.6438941955566, \"min\": 1103.6438941955566}}, \"EndTime\": 1572452792.889359, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452791.785326}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=561.715827748 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=143, train loss <loss>=16.7250356674\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:32 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:33 INFO 140089330636608] Epoch[144] Batch[0] avg_epoch_loss=16.682785\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=16.6827850342\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:33 INFO 140089330636608] Epoch[144] Batch[5] avg_epoch_loss=16.690286\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=16.6902863185\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:33 INFO 140089330636608] Epoch[144] Batch [5]#011Speed: 647.33 samples/sec#011loss=16.690286\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] Epoch[144] Batch[10] avg_epoch_loss=16.727201\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=16.7714981079\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] Epoch[144] Batch [10]#011Speed: 634.17 samples/sec#011loss=16.771498\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] processed a total of 656 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1180.042028427124, \"sum\": 1180.042028427124, \"min\": 1180.042028427124}}, \"EndTime\": 1572452794.069914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452792.889434}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=555.869613458 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=144, train loss <loss>=16.7272007682\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] Epoch[145] Batch[0] avg_epoch_loss=16.654060\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=16.6540603638\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] Epoch[145] Batch[5] avg_epoch_loss=16.698102\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=16.6981019974\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:34 INFO 140089330636608] Epoch[145] Batch [5]#011Speed: 645.81 samples/sec#011loss=16.698102\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] Epoch[145] Batch[10] avg_epoch_loss=16.702847\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=16.7085399628\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] Epoch[145] Batch [10]#011Speed: 636.72 samples/sec#011loss=16.708540\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] processed a total of 685 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1196.8438625335693, \"sum\": 1196.8438625335693, \"min\": 1196.8438625335693}}, \"EndTime\": 1572452795.267094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452794.069975}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=572.29464378 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=145, train loss <loss>=16.7028465271\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] Epoch[146] Batch[0] avg_epoch_loss=16.619816\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=16.6198158264\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] Epoch[146] Batch[5] avg_epoch_loss=16.677645\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=16.6776453654\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:35 INFO 140089330636608] Epoch[146] Batch [5]#011Speed: 628.62 samples/sec#011loss=16.677645\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:36 INFO 140089330636608] processed a total of 627 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.926076889038, \"sum\": 1088.926076889038, \"min\": 1088.926076889038}}, \"EndTime\": 1572452796.356505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452795.267155}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:36 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=575.745986209 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:36 INFO 140089330636608] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=146, train loss <loss>=16.6792108536\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:36 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:36 INFO 140089330636608] Epoch[147] Batch[0] avg_epoch_loss=16.777422\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=16.7774219513\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] Epoch[147] Batch[5] avg_epoch_loss=16.662059\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=16.662059466\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] Epoch[147] Batch [5]#011Speed: 637.75 samples/sec#011loss=16.662059\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1109.7819805145264, \"sum\": 1109.7819805145264, \"min\": 1109.7819805145264}}, \"EndTime\": 1572452797.466793, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452796.35657}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=573.029767899 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=147, train loss <loss>=16.6685714722\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] Epoch[148] Batch[0] avg_epoch_loss=16.550564\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=16.5505638123\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] Epoch[148] Batch[5] avg_epoch_loss=16.667860\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=16.667860349\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] Epoch[148] Batch [5]#011Speed: 635.14 samples/sec#011loss=16.667860\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] Epoch[148] Batch[10] avg_epoch_loss=16.652961\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=16.6350826263\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] Epoch[148] Batch [10]#011Speed: 640.02 samples/sec#011loss=16.635083\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] processed a total of 686 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1190.0949478149414, \"sum\": 1190.0949478149414, \"min\": 1190.0949478149414}}, \"EndTime\": 1572452798.657371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452797.466866}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=576.380487027 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=148, train loss <loss>=16.6529613842\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] Epoch[149] Batch[0] avg_epoch_loss=16.665312\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=16.6653118134\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] Epoch[149] Batch[5] avg_epoch_loss=16.659966\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=16.6599655151\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] Epoch[149] Batch [5]#011Speed: 634.16 samples/sec#011loss=16.659966\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] Epoch[149] Batch[10] avg_epoch_loss=16.680030\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=16.7041065216\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] Epoch[149] Batch [10]#011Speed: 633.75 samples/sec#011loss=16.704107\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] processed a total of 677 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1197.9210376739502, \"sum\": 1197.9210376739502, \"min\": 1197.9210376739502}}, \"EndTime\": 1572452799.855632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452798.657433}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=565.100888306 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=149, train loss <loss>=16.680029609\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:39 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:40 INFO 140089330636608] Epoch[150] Batch[0] avg_epoch_loss=16.721603\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=16.7216033936\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:40 INFO 140089330636608] Epoch[150] Batch[5] avg_epoch_loss=16.691475\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=16.6914752324\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:40 INFO 140089330636608] Epoch[150] Batch [5]#011Speed: 643.44 samples/sec#011loss=16.691475\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] Epoch[150] Batch[10] avg_epoch_loss=16.648233\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=16.5963428497\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] Epoch[150] Batch [10]#011Speed: 632.32 samples/sec#011loss=16.596343\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] processed a total of 644 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1203.2978534698486, \"sum\": 1203.2978534698486, \"min\": 1203.2978534698486}}, \"EndTime\": 1572452801.059351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452799.855695}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=535.146848345 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=150, train loss <loss>=16.6482332403\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_95f855c8-752e-4303-bd0b-702465d1bd96-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.927906036376953, \"sum\": 20.927906036376953, \"min\": 20.927906036376953}}, \"EndTime\": 1572452801.080816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452801.059426}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] Epoch[151] Batch[0] avg_epoch_loss=16.742374\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=16.7423744202\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] Epoch[151] Batch[5] avg_epoch_loss=16.732623\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=16.7326227824\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:41 INFO 140089330636608] Epoch[151] Batch [5]#011Speed: 608.54 samples/sec#011loss=16.732623\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] Epoch[151] Batch[10] avg_epoch_loss=16.697239\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=16.6547775269\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] Epoch[151] Batch [10]#011Speed: 619.60 samples/sec#011loss=16.654778\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] processed a total of 674 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1233.8368892669678, \"sum\": 1233.8368892669678, \"min\": 1233.8368892669678}}, \"EndTime\": 1572452802.314764, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452801.080873}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=546.223133787 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=151, train loss <loss>=16.6972385753\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] Epoch[152] Batch[0] avg_epoch_loss=16.607273\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=16.6072731018\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] Epoch[152] Batch[5] avg_epoch_loss=16.707677\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=16.7076765696\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] Epoch[152] Batch [5]#011Speed: 629.54 samples/sec#011loss=16.707677\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] processed a total of 640 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1097.0079898834229, \"sum\": 1097.0079898834229, \"min\": 1097.0079898834229}}, \"EndTime\": 1572452803.412106, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452802.314826}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=583.354589062 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=152, train loss <loss>=16.7030548096\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] Epoch[153] Batch[0] avg_epoch_loss=16.687048\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=16.6870479584\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] Epoch[153] Batch[5] avg_epoch_loss=16.637250\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=16.6372496287\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] Epoch[153] Batch [5]#011Speed: 627.12 samples/sec#011loss=16.637250\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] processed a total of 638 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1121.009111404419, \"sum\": 1121.009111404419, \"min\": 1121.009111404419}}, \"EndTime\": 1572452804.533492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452803.412171}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=569.080330666 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=153, train loss <loss>=16.6652147293\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] Epoch[154] Batch[0] avg_epoch_loss=16.673735\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=16.6737346649\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] Epoch[154] Batch[5] avg_epoch_loss=16.665624\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=16.6656243006\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] Epoch[154] Batch [5]#011Speed: 631.37 samples/sec#011loss=16.665624\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] Epoch[154] Batch[10] avg_epoch_loss=16.685707\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=16.7098056793\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] Epoch[154] Batch [10]#011Speed: 635.93 samples/sec#011loss=16.709806\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] processed a total of 665 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1213.4361267089844, \"sum\": 1213.4361267089844, \"min\": 1213.4361267089844}}, \"EndTime\": 1572452805.74732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452804.533559}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=547.988502744 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=154, train loss <loss>=16.6857067455\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] Epoch[155] Batch[0] avg_epoch_loss=16.639286\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=16.6392860413\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] Epoch[155] Batch[5] avg_epoch_loss=16.655627\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=16.6556272507\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] Epoch[155] Batch [5]#011Speed: 627.64 samples/sec#011loss=16.655627\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] processed a total of 624 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1106.4469814300537, \"sum\": 1106.4469814300537, \"min\": 1106.4469814300537}}, \"EndTime\": 1572452806.85425, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452805.74738}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.911844097 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=155, train loss <loss>=16.6896709442\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:46 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] Epoch[156] Batch[0] avg_epoch_loss=16.683105\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=16.6831054688\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] Epoch[156] Batch[5] avg_epoch_loss=16.650747\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=16.6507472992\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] Epoch[156] Batch [5]#011Speed: 630.68 samples/sec#011loss=16.650747\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] processed a total of 626 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1092.782974243164, \"sum\": 1092.782974243164, \"min\": 1092.782974243164}}, \"EndTime\": 1572452807.94758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452806.854322}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=572.800096338 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=156, train loss <loss>=16.6486190796\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:47 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:48 INFO 140089330636608] Epoch[157] Batch[0] avg_epoch_loss=16.784483\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=16.7844829559\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:48 INFO 140089330636608] Epoch[157] Batch[5] avg_epoch_loss=16.714903\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=16.7149028778\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:48 INFO 140089330636608] Epoch[157] Batch [5]#011Speed: 642.43 samples/sec#011loss=16.714903\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.7329788208008, \"sum\": 1091.7329788208008, \"min\": 1091.7329788208008}}, \"EndTime\": 1572452809.039806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452807.947641}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.852970046 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=157, train loss <loss>=16.7150016785\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] Epoch[158] Batch[0] avg_epoch_loss=16.467739\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=16.4677391052\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] Epoch[158] Batch[5] avg_epoch_loss=16.653239\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=16.6532386144\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:49 INFO 140089330636608] Epoch[158] Batch [5]#011Speed: 638.15 samples/sec#011loss=16.653239\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] Epoch[158] Batch[10] avg_epoch_loss=16.621519\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=16.583454895\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] Epoch[158] Batch [10]#011Speed: 618.71 samples/sec#011loss=16.583455\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] processed a total of 647 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1202.4707794189453, \"sum\": 1202.4707794189453, \"min\": 1202.4707794189453}}, \"EndTime\": 1572452810.242814, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452809.039871}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=538.011663035 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=158, train loss <loss>=16.621518742\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_37a248fd-54ab-41de-8a95-ae10b0454c1b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.270057678222656, \"sum\": 24.270057678222656, \"min\": 24.270057678222656}}, \"EndTime\": 1572452810.267598, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452810.242886}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] Epoch[159] Batch[0] avg_epoch_loss=16.591835\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=16.591835022\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] Epoch[159] Batch[5] avg_epoch_loss=16.709903\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=16.7099027634\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:50 INFO 140089330636608] Epoch[159] Batch [5]#011Speed: 641.49 samples/sec#011loss=16.709903\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] Epoch[159] Batch[10] avg_epoch_loss=16.702443\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=16.6934913635\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] Epoch[159] Batch [10]#011Speed: 633.68 samples/sec#011loss=16.693491\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] processed a total of 654 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1186.0768795013428, \"sum\": 1186.0768795013428, \"min\": 1186.0768795013428}}, \"EndTime\": 1572452811.453792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452810.267657}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=551.352538826 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=159, train loss <loss>=16.7024430362\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] Epoch[160] Batch[0] avg_epoch_loss=16.700636\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=16.70063591\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] Epoch[160] Batch[5] avg_epoch_loss=16.694983\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=16.6949828466\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] Epoch[160] Batch [5]#011Speed: 551.22 samples/sec#011loss=16.694983\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] Epoch[160] Batch[10] avg_epoch_loss=16.703798\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=16.7143768311\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] Epoch[160] Batch [10]#011Speed: 635.70 samples/sec#011loss=16.714377\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] processed a total of 642 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1287.1038913726807, \"sum\": 1287.1038913726807, \"min\": 1287.1038913726807}}, \"EndTime\": 1572452812.741315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452811.453857}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=498.757463877 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=160, train loss <loss>=16.7037982941\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] Epoch[161] Batch[0] avg_epoch_loss=16.708704\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=16.7087039948\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] Epoch[161] Batch[5] avg_epoch_loss=16.688604\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=16.6886043549\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] Epoch[161] Batch [5]#011Speed: 630.85 samples/sec#011loss=16.688604\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] Epoch[161] Batch[10] avg_epoch_loss=16.692727\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=16.6976737976\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] Epoch[161] Batch [10]#011Speed: 624.02 samples/sec#011loss=16.697674\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] processed a total of 664 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1205.1441669464111, \"sum\": 1205.1441669464111, \"min\": 1205.1441669464111}}, \"EndTime\": 1572452813.946902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452812.741377}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=550.927175016 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=161, train loss <loss>=16.6927268288\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:53 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:54 INFO 140089330636608] Epoch[162] Batch[0] avg_epoch_loss=16.716251\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=16.7162513733\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:54 INFO 140089330636608] Epoch[162] Batch[5] avg_epoch_loss=16.664932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=16.664932251\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:54 INFO 140089330636608] Epoch[162] Batch [5]#011Speed: 627.35 samples/sec#011loss=16.664932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] Epoch[162] Batch[10] avg_epoch_loss=16.672816\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=16.6822757721\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] Epoch[162] Batch [10]#011Speed: 634.27 samples/sec#011loss=16.682276\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] processed a total of 656 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1207.3478698730469, \"sum\": 1207.3478698730469, \"min\": 1207.3478698730469}}, \"EndTime\": 1572452815.154696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452813.946968}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=543.295044625 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=162, train loss <loss>=16.6728156697\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] Epoch[163] Batch[0] avg_epoch_loss=16.684677\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=16.684677124\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] Epoch[163] Batch[5] avg_epoch_loss=16.715146\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=16.7151457469\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:55 INFO 140089330636608] Epoch[163] Batch [5]#011Speed: 641.83 samples/sec#011loss=16.715146\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] processed a total of 614 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1101.0968685150146, \"sum\": 1101.0968685150146, \"min\": 1101.0968685150146}}, \"EndTime\": 1572452816.256133, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452815.154757}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=557.578090947 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=163, train loss <loss>=16.7063461304\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] Epoch[164] Batch[0] avg_epoch_loss=16.642982\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=16.6429824829\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] Epoch[164] Batch[5] avg_epoch_loss=16.720936\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=16.7209361394\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:56 INFO 140089330636608] Epoch[164] Batch [5]#011Speed: 632.46 samples/sec#011loss=16.720936\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:57 INFO 140089330636608] processed a total of 622 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.8381214141846, \"sum\": 1091.8381214141846, \"min\": 1091.8381214141846}}, \"EndTime\": 1572452817.348406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452816.256195}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:57 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=569.633505805 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:57 INFO 140089330636608] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=164, train loss <loss>=16.7102939606\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:57 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:57 INFO 140089330636608] Epoch[165] Batch[0] avg_epoch_loss=16.730436\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=16.7304363251\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] Epoch[165] Batch[5] avg_epoch_loss=16.684685\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=16.6846850713\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] Epoch[165] Batch [5]#011Speed: 630.84 samples/sec#011loss=16.684685\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.005947113037, \"sum\": 1089.005947113037, \"min\": 1089.005947113037}}, \"EndTime\": 1572452818.437863, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452817.348468}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=573.869900289 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=165, train loss <loss>=16.6875833511\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] Epoch[166] Batch[0] avg_epoch_loss=16.766487\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:58 INFO 140089330636608] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=16.7664871216\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] Epoch[166] Batch[5] avg_epoch_loss=16.685605\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=16.6856050491\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] Epoch[166] Batch [5]#011Speed: 632.19 samples/sec#011loss=16.685605\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] processed a total of 640 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.831880569458, \"sum\": 1085.831880569458, \"min\": 1085.831880569458}}, \"EndTime\": 1572452819.524196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452818.437924}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=589.338673272 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=166, train loss <loss>=16.676042366\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] Epoch[167] Batch[0] avg_epoch_loss=16.547001\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:26:59 INFO 140089330636608] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=16.547000885\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] Epoch[167] Batch[5] avg_epoch_loss=16.649321\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=16.6493206024\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] Epoch[167] Batch [5]#011Speed: 636.84 samples/sec#011loss=16.649321\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] processed a total of 617 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1105.0770282745361, \"sum\": 1105.0770282745361, \"min\": 1105.0770282745361}}, \"EndTime\": 1572452820.629685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452819.524275}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=558.282615674 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=167, train loss <loss>=16.6459621429\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] Epoch[168] Batch[0] avg_epoch_loss=16.739426\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:00 INFO 140089330636608] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=16.7394256592\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] Epoch[168] Batch[5] avg_epoch_loss=16.712346\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=16.712346077\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] Epoch[168] Batch [5]#011Speed: 645.04 samples/sec#011loss=16.712346\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] Epoch[168] Batch[10] avg_epoch_loss=16.726501\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=16.7434871674\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] Epoch[168] Batch [10]#011Speed: 608.70 samples/sec#011loss=16.743487\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] processed a total of 676 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1221.27103805542, \"sum\": 1221.27103805542, \"min\": 1221.27103805542}}, \"EndTime\": 1572452821.851415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452820.629753}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=553.475004948 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] #quality_metric: host=algo-1, epoch=168, train loss <loss>=16.7265011181\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:01 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:02 INFO 140089330636608] Epoch[169] Batch[0] avg_epoch_loss=16.768570\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=16.7685699463\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:27:02 INFO 140089330636608] Epoch[169] Batch[5] avg_epoch_loss=16.691997\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:02 INFO 140089330636608] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=16.6919968923\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:02 INFO 140089330636608] Epoch[169] Batch [5]#011Speed: 631.43 samples/sec#011loss=16.691997\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] Epoch[169] Batch[10] avg_epoch_loss=16.673555\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=16.6514240265\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] Epoch[169] Batch [10]#011Speed: 631.16 samples/sec#011loss=16.651424\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] processed a total of 661 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1232.733964920044, \"sum\": 1232.733964920044, \"min\": 1232.733964920044}}, \"EndTime\": 1572452823.084511, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452821.851473}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=536.162560954 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=169, train loss <loss>=16.6735546806\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] Epoch[170] Batch[0] avg_epoch_loss=16.568693\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=16.568693161\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] Epoch[170] Batch[5] avg_epoch_loss=16.667045\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=16.6670449575\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:03 INFO 140089330636608] Epoch[170] Batch [5]#011Speed: 631.00 samples/sec#011loss=16.667045\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] processed a total of 619 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1129.9200057983398, \"sum\": 1129.9200057983398, \"min\": 1129.9200057983398}}, \"EndTime\": 1572452824.214782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452823.084572}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=547.780389423 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=170, train loss <loss>=16.6712608337\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] Epoch[171] Batch[0] avg_epoch_loss=16.705101\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=16.7051010132\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] Epoch[171] Batch[5] avg_epoch_loss=16.670439\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=16.6704387665\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:04 INFO 140089330636608] Epoch[171] Batch [5]#011Speed: 642.09 samples/sec#011loss=16.670439\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] Epoch[171] Batch[10] avg_epoch_loss=16.654756\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=16.6359363556\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] Epoch[171] Batch [10]#011Speed: 637.70 samples/sec#011loss=16.635936\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] processed a total of 679 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1191.6139125823975, \"sum\": 1191.6139125823975, \"min\": 1191.6139125823975}}, \"EndTime\": 1572452825.406905, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452824.214844}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=569.773824834 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=171, train loss <loss>=16.6547558524\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] Epoch[172] Batch[0] avg_epoch_loss=16.669214\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:05 INFO 140089330636608] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=16.6692142487\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] Epoch[172] Batch[5] avg_epoch_loss=16.715142\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=16.7151416143\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] Epoch[172] Batch [5]#011Speed: 649.56 samples/sec#011loss=16.715142\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1071.9330310821533, \"sum\": 1071.9330310821533, \"min\": 1071.9330310821533}}, \"EndTime\": 1572452826.479239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452825.406963}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=583.006684214 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=172, train loss <loss>=16.6904201508\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] Epoch[173] Batch[0] avg_epoch_loss=16.703772\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:06 INFO 140089330636608] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=16.7037715912\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] Epoch[173] Batch[5] avg_epoch_loss=16.688928\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=16.6889282862\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] Epoch[173] Batch [5]#011Speed: 623.86 samples/sec#011loss=16.688928\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] processed a total of 619 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1103.6219596862793, \"sum\": 1103.6219596862793, \"min\": 1103.6219596862793}}, \"EndTime\": 1572452827.583372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452826.479303}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=560.791543223 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=173, train loss <loss>=16.7006557465\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] Epoch[174] Batch[0] avg_epoch_loss=16.759789\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:07 INFO 140089330636608] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=16.7597885132\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] Epoch[174] Batch[5] avg_epoch_loss=16.676247\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=16.6762472788\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] Epoch[174] Batch [5]#011Speed: 638.88 samples/sec#011loss=16.676247\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] processed a total of 633 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1100.304126739502, \"sum\": 1100.304126739502, \"min\": 1100.304126739502}}, \"EndTime\": 1572452828.684148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452827.583511}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=575.247873203 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=174, train loss <loss>=16.663177681\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] Epoch[175] Batch[0] avg_epoch_loss=16.518867\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:08 INFO 140089330636608] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=16.5188674927\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] Epoch[175] Batch[5] avg_epoch_loss=16.636116\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=16.6361163457\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] Epoch[175] Batch [5]#011Speed: 623.93 samples/sec#011loss=16.636116\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1103.7938594818115, \"sum\": 1103.7938594818115, \"min\": 1103.7938594818115}}, \"EndTime\": 1572452829.788325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452828.684209}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=562.559118795 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=175, train loss <loss>=16.6508876801\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] Epoch[176] Batch[0] avg_epoch_loss=16.706348\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:09 INFO 140089330636608] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=16.7063484192\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] Epoch[176] Batch[5] avg_epoch_loss=16.695852\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=16.6958522797\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] Epoch[176] Batch [5]#011Speed: 640.00 samples/sec#011loss=16.695852\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1074.3980407714844, \"sum\": 1074.3980407714844, \"min\": 1074.3980407714844}}, \"EndTime\": 1572452830.863196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452829.788384}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.981201723 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] #quality_metric: host=algo-1, epoch=176, train loss <loss>=16.7191123962\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:10 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:11 INFO 140089330636608] Epoch[177] Batch[0] avg_epoch_loss=16.742424\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=16.7424240112\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:11 INFO 140089330636608] Epoch[177] Batch[5] avg_epoch_loss=16.597929\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:11 INFO 140089330636608] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=16.5979290009\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:11 INFO 140089330636608] Epoch[177] Batch [5]#011Speed: 639.37 samples/sec#011loss=16.597929\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] Epoch[177] Batch[10] avg_epoch_loss=16.563904\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=16.5230731964\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] Epoch[177] Batch [10]#011Speed: 614.21 samples/sec#011loss=16.523073\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] processed a total of 643 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1201.794147491455, \"sum\": 1201.794147491455, \"min\": 1201.794147491455}}, \"EndTime\": 1572452832.065488, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452830.863268}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=534.993804283 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=177, train loss <loss>=16.5639036352\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/state_57e64368-60b5-4fc2-9d8f-e337a3511995-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.837038040161133, \"sum\": 27.837038040161133, \"min\": 27.837038040161133}}, \"EndTime\": 1572452832.093718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452832.065548}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] Epoch[178] Batch[0] avg_epoch_loss=16.595785\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=16.595785141\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] Epoch[178] Batch[5] avg_epoch_loss=16.706000\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=16.7060003281\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:12 INFO 140089330636608] Epoch[178] Batch [5]#011Speed: 634.40 samples/sec#011loss=16.706000\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] Epoch[178] Batch[10] avg_epoch_loss=16.689157\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=16.6689441681\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] Epoch[178] Batch [10]#011Speed: 636.43 samples/sec#011loss=16.668944\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] processed a total of 672 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1210.204839706421, \"sum\": 1210.204839706421, \"min\": 1210.204839706421}}, \"EndTime\": 1572452833.304087, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452832.093825}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=555.235675691 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=178, train loss <loss>=16.689156619\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] Epoch[179] Batch[0] avg_epoch_loss=16.745899\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:13 INFO 140089330636608] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=16.7458992004\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] Epoch[179] Batch[5] avg_epoch_loss=16.716993\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=16.7169926961\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] Epoch[179] Batch [5]#011Speed: 635.69 samples/sec#011loss=16.716993\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] Epoch[179] Batch[10] avg_epoch_loss=16.715210\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=16.7130718231\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] Epoch[179] Batch [10]#011Speed: 618.46 samples/sec#011loss=16.713072\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] processed a total of 677 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1216.3209915161133, \"sum\": 1216.3209915161133, \"min\": 1216.3209915161133}}, \"EndTime\": 1572452834.520759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452833.304149}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=556.555693323 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=179, train loss <loss>=16.7152104811\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] Epoch[180] Batch[0] avg_epoch_loss=16.681810\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:14 INFO 140089330636608] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=16.681810379\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] Epoch[180] Batch[5] avg_epoch_loss=16.702754\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=16.7027537028\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] Epoch[180] Batch [5]#011Speed: 637.29 samples/sec#011loss=16.702754\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] Epoch[180] Batch[10] avg_epoch_loss=16.710881\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=16.7206348419\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] Epoch[180] Batch [10]#011Speed: 628.22 samples/sec#011loss=16.720635\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] processed a total of 654 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1203.8259506225586, \"sum\": 1203.8259506225586, \"min\": 1203.8259506225586}}, \"EndTime\": 1572452835.724913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452834.520818}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=543.226374852 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=180, train loss <loss>=16.7108814933\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] Epoch[181] Batch[0] avg_epoch_loss=16.690344\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:15 INFO 140089330636608] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=16.6903438568\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] Epoch[181] Batch[5] avg_epoch_loss=16.697004\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=16.6970043182\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] Epoch[181] Batch [5]#011Speed: 620.03 samples/sec#011loss=16.697004\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] Epoch[181] Batch[10] avg_epoch_loss=16.719915\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=16.7474075317\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] Epoch[181] Batch [10]#011Speed: 615.95 samples/sec#011loss=16.747408\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] processed a total of 642 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1232.4979305267334, \"sum\": 1232.4979305267334, \"min\": 1232.4979305267334}}, \"EndTime\": 1572452836.957753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452835.724974}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=520.8540748 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] #quality_metric: host=algo-1, epoch=181, train loss <loss>=16.7199148698\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:16 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:17 INFO 140089330636608] Epoch[182] Batch[0] avg_epoch_loss=16.727402\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=16.7274017334\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:27:17 INFO 140089330636608] Epoch[182] Batch[5] avg_epoch_loss=16.601723\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:17 INFO 140089330636608] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=16.6017227173\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:17 INFO 140089330636608] Epoch[182] Batch [5]#011Speed: 633.49 samples/sec#011loss=16.601723\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] Epoch[182] Batch[10] avg_epoch_loss=16.645795\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=16.698682785\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] Epoch[182] Batch [10]#011Speed: 637.92 samples/sec#011loss=16.698683\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] processed a total of 658 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1190.201997756958, \"sum\": 1190.201997756958, \"min\": 1190.201997756958}}, \"EndTime\": 1572452838.148373, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452836.957816}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=552.804588524 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=182, train loss <loss>=16.6457954754\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] Epoch[183] Batch[0] avg_epoch_loss=16.673771\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=16.6737709045\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] Epoch[183] Batch[5] avg_epoch_loss=16.674852\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=16.6748520533\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:18 INFO 140089330636608] Epoch[183] Batch [5]#011Speed: 645.92 samples/sec#011loss=16.674852\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] processed a total of 608 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.887939453125, \"sum\": 1078.887939453125, \"min\": 1078.887939453125}}, \"EndTime\": 1572452839.227677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452838.148434}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.494546439 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=183, train loss <loss>=16.6720031738\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] Epoch[184] Batch[0] avg_epoch_loss=16.755114\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=16.7551136017\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] Epoch[184] Batch[5] avg_epoch_loss=16.666628\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=16.666627566\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:19 INFO 140089330636608] Epoch[184] Batch [5]#011Speed: 629.53 samples/sec#011loss=16.666628\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:20 INFO 140089330636608] processed a total of 631 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1100.795030593872, \"sum\": 1100.795030593872, \"min\": 1100.795030593872}}, \"EndTime\": 1572452840.328935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452839.22774}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:20 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=573.171538737 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:20 INFO 140089330636608] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=184, train loss <loss>=16.6859807968\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:20 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:20 INFO 140089330636608] Epoch[185] Batch[0] avg_epoch_loss=16.711920\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:20 INFO 140089330636608] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=16.7119197845\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] Epoch[185] Batch[5] avg_epoch_loss=16.631337\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=16.63133653\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] Epoch[185] Batch [5]#011Speed: 642.75 samples/sec#011loss=16.631337\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] processed a total of 639 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1087.3780250549316, \"sum\": 1087.3780250549316, \"min\": 1087.3780250549316}}, \"EndTime\": 1572452841.416692, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452840.328999}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=587.600712357 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=185, train loss <loss>=16.6441850662\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] Epoch[186] Batch[0] avg_epoch_loss=16.628399\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:21 INFO 140089330636608] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=16.6283988953\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] Epoch[186] Batch[5] avg_epoch_loss=16.598330\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=16.5983295441\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] Epoch[186] Batch [5]#011Speed: 610.26 samples/sec#011loss=16.598330\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 995.0389862060547, \"sum\": 995.0389862060547, \"min\": 995.0389862060547}}, \"EndTime\": 1572452842.412178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452841.416758}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=578.808272444 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=186, train loss <loss>=16.5759048462\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] Epoch[187] Batch[0] avg_epoch_loss=16.725721\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:22 INFO 140089330636608] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=16.7257213593\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] Epoch[187] Batch[5] avg_epoch_loss=16.683981\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=16.6839806239\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] Epoch[187] Batch [5]#011Speed: 648.27 samples/sec#011loss=16.683981\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] processed a total of 614 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1101.5310287475586, \"sum\": 1101.5310287475586, \"min\": 1101.5310287475586}}, \"EndTime\": 1572452843.514107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452842.412255}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=557.359912634 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=187, train loss <loss>=16.657677269\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] Epoch[188] Batch[0] avg_epoch_loss=16.731590\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:23 INFO 140089330636608] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=16.731590271\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] Epoch[188] Batch[5] avg_epoch_loss=16.675202\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=16.6752023697\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] Epoch[188] Batch [5]#011Speed: 645.24 samples/sec#011loss=16.675202\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] Epoch[188] Batch[10] avg_epoch_loss=16.738149\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=16.8136859894\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] Epoch[188] Batch [10]#011Speed: 613.88 samples/sec#011loss=16.813686\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] processed a total of 651 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1214.2140865325928, \"sum\": 1214.2140865325928, \"min\": 1214.2140865325928}}, \"EndTime\": 1572452844.728798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452843.514168}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=536.108639631 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=188, train loss <loss>=16.7381494695\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] Epoch[189] Batch[0] avg_epoch_loss=16.695673\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:24 INFO 140089330636608] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=16.6956729889\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] Epoch[189] Batch[5] avg_epoch_loss=16.697920\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=16.6979198456\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] Epoch[189] Batch [5]#011Speed: 638.29 samples/sec#011loss=16.697920\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] processed a total of 633 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1115.7710552215576, \"sum\": 1115.7710552215576, \"min\": 1115.7710552215576}}, \"EndTime\": 1572452845.84492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452844.728859}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.261654955 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] #quality_metric: host=algo-1, epoch=189, train loss <loss>=16.6523498535\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:25 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] Epoch[190] Batch[0] avg_epoch_loss=16.615313\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=16.6153125763\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] Epoch[190] Batch[5] avg_epoch_loss=16.662241\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=16.6622412999\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] Epoch[190] Batch [5]#011Speed: 645.72 samples/sec#011loss=16.662241\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] processed a total of 590 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1099.147081375122, \"sum\": 1099.147081375122, \"min\": 1099.147081375122}}, \"EndTime\": 1572452846.944554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452845.845003}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=536.732576911 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] #quality_metric: host=algo-1, epoch=190, train loss <loss>=16.6853630066\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:26 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:27 INFO 140089330636608] Epoch[191] Batch[0] avg_epoch_loss=16.649935\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=16.6499347687\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:27:27 INFO 140089330636608] Epoch[191] Batch[5] avg_epoch_loss=16.669368\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:27 INFO 140089330636608] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=16.669368426\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:27 INFO 140089330636608] Epoch[191] Batch [5]#011Speed: 630.27 samples/sec#011loss=16.669368\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] Epoch[191] Batch[10] avg_epoch_loss=16.665675\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=16.6612422943\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] Epoch[191] Batch [10]#011Speed: 630.01 samples/sec#011loss=16.661242\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] processed a total of 684 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1220.4267978668213, \"sum\": 1220.4267978668213, \"min\": 1220.4267978668213}}, \"EndTime\": 1572452848.165466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452846.944618}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=560.416862529 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=191, train loss <loss>=16.6656747298\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] Epoch[192] Batch[0] avg_epoch_loss=16.717075\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=16.7170753479\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] Epoch[192] Batch[5] avg_epoch_loss=16.688611\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=16.6886113485\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:28 INFO 140089330636608] Epoch[192] Batch [5]#011Speed: 621.13 samples/sec#011loss=16.688611\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] Epoch[192] Batch[10] avg_epoch_loss=16.649323\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=16.6021762848\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] Epoch[192] Batch [10]#011Speed: 626.95 samples/sec#011loss=16.602176\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] processed a total of 668 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1212.104082107544, \"sum\": 1212.104082107544, \"min\": 1212.104082107544}}, \"EndTime\": 1572452849.377987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452848.165526}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=551.066923929 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=192, train loss <loss>=16.6493226832\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] Epoch[193] Batch[0] avg_epoch_loss=16.709137\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:29 INFO 140089330636608] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=16.7091369629\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] Epoch[193] Batch[5] avg_epoch_loss=16.665875\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=16.6658747991\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] Epoch[193] Batch [5]#011Speed: 638.21 samples/sec#011loss=16.665875\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] Epoch[193] Batch[10] avg_epoch_loss=16.658215\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=193, batch=10 train loss <loss>=16.649023056\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] Epoch[193] Batch [10]#011Speed: 634.06 samples/sec#011loss=16.649023\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] processed a total of 644 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1227.8060913085938, \"sum\": 1227.8060913085938, \"min\": 1227.8060913085938}}, \"EndTime\": 1572452850.606119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452849.378049}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=524.474803429 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=193, train loss <loss>=16.6582149159\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] Epoch[194] Batch[0] avg_epoch_loss=16.734554\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:30 INFO 140089330636608] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=16.7345542908\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] Epoch[194] Batch[5] avg_epoch_loss=16.687490\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=16.6874901454\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] Epoch[194] Batch [5]#011Speed: 645.24 samples/sec#011loss=16.687490\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] processed a total of 584 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.357126235962, \"sum\": 1076.357126235962, \"min\": 1076.357126235962}}, \"EndTime\": 1572452851.682807, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452850.60618}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=542.522540233 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=194, train loss <loss>=16.6568822861\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] Epoch[195] Batch[0] avg_epoch_loss=16.496254\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:31 INFO 140089330636608] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=16.4962539673\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] Epoch[195] Batch[5] avg_epoch_loss=16.653840\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=16.6538403829\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] Epoch[195] Batch [5]#011Speed: 623.43 samples/sec#011loss=16.653840\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] Epoch[195] Batch[10] avg_epoch_loss=16.642973\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=16.6299316406\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] Epoch[195] Batch [10]#011Speed: 638.92 samples/sec#011loss=16.629932\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] processed a total of 667 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1197.026014328003, \"sum\": 1197.026014328003, \"min\": 1197.026014328003}}, \"EndTime\": 1572452852.880353, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452851.682872}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=557.171009191 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] #quality_metric: host=algo-1, epoch=195, train loss <loss>=16.6429727728\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:32 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] Epoch[196] Batch[0] avg_epoch_loss=16.553915\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=16.5539150238\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] Epoch[196] Batch[5] avg_epoch_loss=16.688667\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=16.6886672974\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] Epoch[196] Batch [5]#011Speed: 638.99 samples/sec#011loss=16.688667\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1080.2628993988037, \"sum\": 1080.2628993988037, \"min\": 1080.2628993988037}}, \"EndTime\": 1572452853.961042, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452852.880414}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=578.511411821 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] #quality_metric: host=algo-1, epoch=196, train loss <loss>=16.6714025497\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:33 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:34 INFO 140089330636608] Epoch[197] Batch[0] avg_epoch_loss=16.678427\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=16.6784267426\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:34 INFO 140089330636608] Epoch[197] Batch[5] avg_epoch_loss=16.677316\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:34 INFO 140089330636608] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=16.6773163478\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:34 INFO 140089330636608] Epoch[197] Batch [5]#011Speed: 633.77 samples/sec#011loss=16.677316\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] processed a total of 640 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.2630863189697, \"sum\": 1084.2630863189697, \"min\": 1084.2630863189697}}, \"EndTime\": 1572452855.045756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452853.961107}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=590.211513629 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=197, train loss <loss>=16.6616487503\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] Epoch[198] Batch[0] avg_epoch_loss=16.641706\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=16.6417064667\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] Epoch[198] Batch[5] avg_epoch_loss=16.572091\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=16.5720907847\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:35 INFO 140089330636608] Epoch[198] Batch [5]#011Speed: 642.72 samples/sec#011loss=16.572091\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] processed a total of 602 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.9338722229004, \"sum\": 1089.9338722229004, \"min\": 1089.9338722229004}}, \"EndTime\": 1572452856.13617, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452855.04582}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=552.276346946 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=198, train loss <loss>=16.5722465515\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] Epoch[199] Batch[0] avg_epoch_loss=16.624680\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=16.6246795654\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] Epoch[199] Batch[5] avg_epoch_loss=16.642896\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=16.6428963343\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:36 INFO 140089330636608] Epoch[199] Batch [5]#011Speed: 644.10 samples/sec#011loss=16.642896\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] processed a total of 610 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1105.2749156951904, \"sum\": 1105.2749156951904, \"min\": 1105.2749156951904}}, \"EndTime\": 1572452857.241966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452856.136237}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=551.847346975 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=199, train loss <loss>=16.6505664825\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] Epoch[200] Batch[0] avg_epoch_loss=16.664869\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=16.6648693085\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] Epoch[200] Batch[5] avg_epoch_loss=16.645809\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=16.6458091736\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:37 INFO 140089330636608] Epoch[200] Batch [5]#011Speed: 618.78 samples/sec#011loss=16.645809\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:38 INFO 140089330636608] processed a total of 634 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1099.7109413146973, \"sum\": 1099.7109413146973, \"min\": 1099.7109413146973}}, \"EndTime\": 1572452858.342155, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452857.242032}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:38 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=576.465267018 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:38 INFO 140089330636608] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=200, train loss <loss>=16.6355724335\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:38 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:38 INFO 140089330636608] Epoch[201] Batch[0] avg_epoch_loss=16.659023\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:38 INFO 140089330636608] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=16.6590232849\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] Epoch[201] Batch[5] avg_epoch_loss=16.644393\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=16.6443932851\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] Epoch[201] Batch [5]#011Speed: 633.76 samples/sec#011loss=16.644393\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] Epoch[201] Batch[10] avg_epoch_loss=16.695223\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=201, batch=10 train loss <loss>=16.7562194824\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] Epoch[201] Batch [10]#011Speed: 631.31 samples/sec#011loss=16.756219\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] processed a total of 657 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1211.4851474761963, \"sum\": 1211.4851474761963, \"min\": 1211.4851474761963}}, \"EndTime\": 1572452859.554016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452858.342221}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=542.270733363 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=201, train loss <loss>=16.6952233748\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] Epoch[202] Batch[0] avg_epoch_loss=16.716917\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:39 INFO 140089330636608] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=16.716917038\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] Epoch[202] Batch[5] avg_epoch_loss=16.688683\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=16.688682874\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] Epoch[202] Batch [5]#011Speed: 640.53 samples/sec#011loss=16.688683\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] processed a total of 632 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1098.388910293579, \"sum\": 1098.388910293579, \"min\": 1098.388910293579}}, \"EndTime\": 1572452860.652735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452859.554076}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=575.337855354 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=202, train loss <loss>=16.6835588455\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] Epoch[203] Batch[0] avg_epoch_loss=16.664419\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:40 INFO 140089330636608] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=16.6644191742\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] Epoch[203] Batch[5] avg_epoch_loss=16.648376\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=16.6483758291\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] Epoch[203] Batch [5]#011Speed: 642.35 samples/sec#011loss=16.648376\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] processed a total of 640 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1106.3721179962158, \"sum\": 1106.3721179962158, \"min\": 1106.3721179962158}}, \"EndTime\": 1572452861.759512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452860.652802}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=578.391933748 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=203, train loss <loss>=16.6515098572\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] Epoch[204] Batch[0] avg_epoch_loss=16.299061\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:41 INFO 140089330636608] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=16.2990608215\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] Epoch[204] Batch[5] avg_epoch_loss=16.572509\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=16.5725091298\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] Epoch[204] Batch [5]#011Speed: 627.89 samples/sec#011loss=16.572509\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] processed a total of 637 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1118.1581020355225, \"sum\": 1118.1581020355225, \"min\": 1118.1581020355225}}, \"EndTime\": 1572452862.878126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452861.759597}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=569.625786127 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] #quality_metric: host=algo-1, epoch=204, train loss <loss>=16.6043426514\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:42 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:43 INFO 140089330636608] Epoch[205] Batch[0] avg_epoch_loss=16.651260\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=16.651260376\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:43 INFO 140089330636608] Epoch[205] Batch[5] avg_epoch_loss=16.662255\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:43 INFO 140089330636608] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=16.6622546514\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:43 INFO 140089330636608] Epoch[205] Batch [5]#011Speed: 638.49 samples/sec#011loss=16.662255\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1131.3090324401855, \"sum\": 1131.3090324401855, \"min\": 1131.3090324401855}}, \"EndTime\": 1572452864.009888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452862.878213}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=562.127417051 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=205, train loss <loss>=16.641534996\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] Epoch[206] Batch[0] avg_epoch_loss=16.698692\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=16.6986923218\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] Epoch[206] Batch[5] avg_epoch_loss=16.656688\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=16.6566883723\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:44 INFO 140089330636608] Epoch[206] Batch [5]#011Speed: 639.13 samples/sec#011loss=16.656688\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] processed a total of 615 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.0318927764893, \"sum\": 1083.0318927764893, \"min\": 1083.0318927764893}}, \"EndTime\": 1572452865.093423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452864.009963}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=567.798815622 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=206, train loss <loss>=16.6210487366\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] Epoch[207] Batch[0] avg_epoch_loss=16.612888\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=16.6128883362\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] Epoch[207] Batch[5] avg_epoch_loss=16.631365\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=16.6313645045\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:45 INFO 140089330636608] Epoch[207] Batch [5]#011Speed: 647.60 samples/sec#011loss=16.631365\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] Epoch[207] Batch[10] avg_epoch_loss=16.664392\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=207, batch=10 train loss <loss>=16.70402565\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] Epoch[207] Batch [10]#011Speed: 638.38 samples/sec#011loss=16.704026\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] processed a total of 661 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1185.3110790252686, \"sum\": 1185.3110790252686, \"min\": 1185.3110790252686}}, \"EndTime\": 1572452866.279141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452865.093492}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=557.612518489 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=207, train loss <loss>=16.6643922979\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] Epoch[208] Batch[0] avg_epoch_loss=16.670225\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=16.6702251434\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] Epoch[208] Batch[5] avg_epoch_loss=16.662337\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=16.6623369853\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:46 INFO 140089330636608] Epoch[208] Batch [5]#011Speed: 641.27 samples/sec#011loss=16.662337\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] Epoch[208] Batch[10] avg_epoch_loss=16.650845\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=208, batch=10 train loss <loss>=16.6370544434\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] Epoch[208] Batch [10]#011Speed: 638.82 samples/sec#011loss=16.637054\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] processed a total of 660 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1181.879997253418, \"sum\": 1181.879997253418, \"min\": 1181.879997253418}}, \"EndTime\": 1572452867.461455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452866.279209}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=558.383668213 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=208, train loss <loss>=16.6508449208\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] Epoch[209] Batch[0] avg_epoch_loss=16.650152\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:47 INFO 140089330636608] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=16.6501522064\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] Epoch[209] Batch[5] avg_epoch_loss=16.604768\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=16.6047677994\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] Epoch[209] Batch [5]#011Speed: 641.12 samples/sec#011loss=16.604768\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] Epoch[209] Batch[10] avg_epoch_loss=16.644860\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=16.6929706573\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] Epoch[209] Batch [10]#011Speed: 630.34 samples/sec#011loss=16.692971\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] processed a total of 663 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1192.669153213501, \"sum\": 1192.669153213501, \"min\": 1192.669153213501}}, \"EndTime\": 1572452868.654589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452867.461525}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=555.854543565 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=209, train loss <loss>=16.6448600075\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] Epoch[210] Batch[0] avg_epoch_loss=16.706863\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:48 INFO 140089330636608] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=16.7068634033\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] Epoch[210] Batch[5] avg_epoch_loss=16.647401\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=16.6474014918\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] Epoch[210] Batch [5]#011Speed: 646.94 samples/sec#011loss=16.647401\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] processed a total of 638 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1117.5649166107178, \"sum\": 1117.5649166107178, \"min\": 1117.5649166107178}}, \"EndTime\": 1572452869.772507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452868.654651}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=570.8334022 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=210, train loss <loss>=16.6506441116\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] Epoch[211] Batch[0] avg_epoch_loss=16.674776\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:49 INFO 140089330636608] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=16.6747760773\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] Epoch[211] Batch[5] avg_epoch_loss=16.643632\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=16.6436319351\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] Epoch[211] Batch [5]#011Speed: 644.49 samples/sec#011loss=16.643632\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] Epoch[211] Batch[10] avg_epoch_loss=16.609703\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=211, batch=10 train loss <loss>=16.5689876556\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] Epoch[211] Batch [10]#011Speed: 632.50 samples/sec#011loss=16.568988\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] processed a total of 646 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1185.9068870544434, \"sum\": 1185.9068870544434, \"min\": 1185.9068870544434}}, \"EndTime\": 1572452870.958911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452869.77257}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=544.687648559 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] #quality_metric: host=algo-1, epoch=211, train loss <loss>=16.6097027172\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:50 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:51 INFO 140089330636608] Epoch[212] Batch[0] avg_epoch_loss=16.689007\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=16.6890068054\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:51 INFO 140089330636608] Epoch[212] Batch[5] avg_epoch_loss=16.683596\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:51 INFO 140089330636608] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=16.6835962931\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:51 INFO 140089330636608] Epoch[212] Batch [5]#011Speed: 645.01 samples/sec#011loss=16.683596\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] processed a total of 622 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.0988159179688, \"sum\": 1084.0988159179688, \"min\": 1084.0988159179688}}, \"EndTime\": 1572452872.043347, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452870.958973}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=573.69708498 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=212, train loss <loss>=16.6775016785\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] Epoch[213] Batch[0] avg_epoch_loss=16.689207\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=16.689207077\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] Epoch[213] Batch[5] avg_epoch_loss=16.660714\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=16.6607141495\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:52 INFO 140089330636608] Epoch[213] Batch [5]#011Speed: 639.69 samples/sec#011loss=16.660714\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] processed a total of 639 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1094.4108963012695, \"sum\": 1094.4108963012695, \"min\": 1094.4108963012695}}, \"EndTime\": 1572452873.138162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452872.043414}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=583.822983257 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=213, train loss <loss>=16.6442590714\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] Epoch[214] Batch[0] avg_epoch_loss=16.702885\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=16.7028846741\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] Epoch[214] Batch[5] avg_epoch_loss=16.673136\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=16.6731360753\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:53 INFO 140089330636608] Epoch[214] Batch [5]#011Speed: 646.83 samples/sec#011loss=16.673136\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] processed a total of 639 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1080.7900428771973, \"sum\": 1080.7900428771973, \"min\": 1080.7900428771973}}, \"EndTime\": 1572452874.219333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452873.13823}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=591.183305276 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=214, train loss <loss>=16.6594409943\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] Epoch[215] Batch[0] avg_epoch_loss=16.627451\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=16.627450943\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] Epoch[215] Batch[5] avg_epoch_loss=16.571202\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=16.5712016424\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:54 INFO 140089330636608] Epoch[215] Batch [5]#011Speed: 634.83 samples/sec#011loss=16.571202\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1079.4391632080078, \"sum\": 1079.4391632080078, \"min\": 1079.4391632080078}}, \"EndTime\": 1572452875.299226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452874.219399}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=589.140851929 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=215, train loss <loss>=16.6118120193\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] Epoch[216] Batch[0] avg_epoch_loss=16.534105\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=16.5341053009\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] Epoch[216] Batch[5] avg_epoch_loss=16.607307\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=16.6073071162\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:55 INFO 140089330636608] Epoch[216] Batch [5]#011Speed: 627.73 samples/sec#011loss=16.607307\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:56 INFO 140089330636608] processed a total of 612 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.9301090240479, \"sum\": 1085.9301090240479, \"min\": 1085.9301090240479}}, \"EndTime\": 1572452876.385699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452875.299287}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:56 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=563.523929055 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:56 INFO 140089330636608] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=216, train loss <loss>=16.6111257553\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:56 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:56 INFO 140089330636608] Epoch[217] Batch[0] avg_epoch_loss=16.593031\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:56 INFO 140089330636608] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=16.5930309296\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Epoch[217] Batch[5] avg_epoch_loss=16.623035\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=16.6230347951\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Epoch[217] Batch [5]#011Speed: 642.25 samples/sec#011loss=16.623035\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1087.9840850830078, \"sum\": 1087.9840850830078, \"min\": 1087.9840850830078}}, \"EndTime\": 1572452877.474063, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452876.385761}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] #throughput_metric: host=algo-1, train throughput=584.513064666 records/second\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] #quality_metric: host=algo-1, epoch=217, train loss <loss>=16.6482097626\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] loss did not improve\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Loading parameters from best epoch (177)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 9.074926376342773, \"sum\": 9.074926376342773, \"min\": 9.074926376342773}}, \"EndTime\": 1572452877.48369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452877.474132}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] stopping training now\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Final loss: 16.5639036352 (occurred at epoch 177)\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] #quality_metric: host=algo-1, train final_loss <loss>=16.5639036352\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 WARNING 140089330636608] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] All workers finished. Serializing model for prediction.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 201.80988311767578, \"sum\": 201.80988311767578, \"min\": 201.80988311767578}}, \"EndTime\": 1572452877.686186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452877.483743}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 279.0660858154297, \"sum\": 279.0660858154297, \"min\": 279.0660858154297}}, \"EndTime\": 1572452877.763389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452877.686311}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 12.113094329833984, \"sum\": 12.113094329833984, \"min\": 12.113094329833984}}, \"EndTime\": 1572452877.775618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452877.763451}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:57 INFO 140089330636608] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.029802322387695312, \"sum\": 0.029802322387695312, \"min\": 0.029802322387695312}}, \"EndTime\": 1572452877.776298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452877.775662}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 1693.4640407562256, \"sum\": 1693.4640407562256, \"min\": 1693.4640407562256}}, \"EndTime\": 1572452879.469736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452877.776349}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, RMSE): 8413440.24537\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, mean_wQuantileLoss): 0.7692178\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.1]): 1.0432718\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.2]): 1.0568787\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.3]): 1.013169\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.4]): 0.9472106\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.5]): 0.85658157\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.6]): 0.73991597\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.7]): 0.5951887\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.8]): 0.43148303\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #test_score (algo-1, wQuantileLoss[0.9]): 0.23926091\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.769217789173\u001b[0m\n",
      "\u001b[31m[10/30/2019 16:27:59 INFO 140089330636608] #quality_metric: host=algo-1, test RMSE <loss>=8413440.24537\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 253510.5390548706, \"sum\": 253510.5390548706, \"min\": 253510.5390548706}, \"setuptime\": {\"count\": 1, \"max\": 7.61103630065918, \"sum\": 7.61103630065918, \"min\": 7.61103630065918}}, \"EndTime\": 1572452879.495898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1572452879.469801}\n",
      "\u001b[0m\n",
      "\n",
      "2019-10-30 16:28:47 Uploading - Uploading generated training model\n",
      "2019-10-30 16:28:54 Completed - Training job completed\n",
      "Training seconds: 356\n",
      "Billable seconds: 356\n"
     ]
    }
   ],
   "source": [
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-78cc7d857018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# predictor.predict(ts=timeseries[120], quantiles=[0.10, 0.5, 0.90]).head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# predictor.predict(ts=timeseries[120], quantiles=[0.10, 0.5, 0.90]).head()\n",
    "test_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '1994-01-01 00:00:00',\n",
       "  'target': [7437900,\n",
       "   8199600,\n",
       "   10694400,\n",
       "   8704800,\n",
       "   12803100,\n",
       "   12027300,\n",
       "   12519000,\n",
       "   11570100,\n",
       "   9417300,\n",
       "   9941700,\n",
       "   11968200,\n",
       "   12077400,\n",
       "   4561500,\n",
       "   13197300,\n",
       "   12252600,\n",
       "   11796900,\n",
       "   11721300,\n",
       "   7685400,\n",
       "   5451600,\n",
       "   9810900,\n",
       "   12423600,\n",
       "   11590800,\n",
       "   1609800,\n",
       "   3166500,\n",
       "   3104700,\n",
       "   2655300,\n",
       "   6268200,\n",
       "   11357100,\n",
       "   10005900,\n",
       "   11276400,\n",
       "   11739600,\n",
       "   11870100,\n",
       "   11089500,\n",
       "   8790000,\n",
       "   11895600,\n",
       "   12165300,\n",
       "   7313100,\n",
       "   8252700,\n",
       "   11886600,\n",
       "   12072600,\n",
       "   10689000,\n",
       "   12416400,\n",
       "   12456000,\n",
       "   12309900,\n",
       "   9021300,\n",
       "   9142200,\n",
       "   12788700,\n",
       "   13036200,\n",
       "   11170200,\n",
       "   13284000,\n",
       "   11163600,\n",
       "   8958300,\n",
       "   7813500,\n",
       "   13773600,\n",
       "   11419800,\n",
       "   11549700,\n",
       "   11442600,\n",
       "   12351300,\n",
       "   10773000,\n",
       "   13398300,\n",
       "   14266200,\n",
       "   13605900,\n",
       "   7520400,\n",
       "   3917700,\n",
       "   8274000,\n",
       "   13047900,\n",
       "   15183000,\n",
       "   14596800,\n",
       "   14742600,\n",
       "   14959500,\n",
       "   14130000,\n",
       "   14401800,\n",
       "   14026500,\n",
       "   15988200,\n",
       "   16366200,\n",
       "   16184100,\n",
       "   14604600,\n",
       "   13388400,\n",
       "   17387400,\n",
       "   17657100,\n",
       "   17504400,\n",
       "   17134200,\n",
       "   16662600,\n",
       "   14343600,\n",
       "   8371200,\n",
       "   6264900,\n",
       "   4595700,\n",
       "   2858700,\n",
       "   3267300,\n",
       "   5874300,\n",
       "   11259300,\n",
       "   9503700,\n",
       "   10217100,\n",
       "   18846900,\n",
       "   18447300,\n",
       "   18719100,\n",
       "   16971600,\n",
       "   10020000,\n",
       "   18915300,\n",
       "   14541900,\n",
       "   19118400,\n",
       "   19968900,\n",
       "   15298500,\n",
       "   10589100,\n",
       "   12572100,\n",
       "   18650700,\n",
       "   19277400,\n",
       "   14770800,\n",
       "   22081800,\n",
       "   22152000,\n",
       "   19551000,\n",
       "   22544400,\n",
       "   5637600,\n",
       "   1752900,\n",
       "   1836300,\n",
       "   13347000,\n",
       "   16548600,\n",
       "   5025300,\n",
       "   9642000,\n",
       "   23787900,\n",
       "   22641900,\n",
       "   21170100,\n",
       "   19276200,\n",
       "   23799900,\n",
       "   10562100,\n",
       "   10595700,\n",
       "   22674000,\n",
       "   14618100,\n",
       "   24726600,\n",
       "   21721800,\n",
       "   21429300,\n",
       "   18830700,\n",
       "   20777100,\n",
       "   18309300,\n",
       "   22853700,\n",
       "   25850400,\n",
       "   22198500,\n",
       "   12409500,\n",
       "   25104900,\n",
       "   25654800,\n",
       "   24021600,\n",
       "   21522300,\n",
       "   26623800,\n",
       "   27378000,\n",
       "   27258300,\n",
       "   26766600,\n",
       "   11901000,\n",
       "   24991800,\n",
       "   23052000,\n",
       "   23409000,\n",
       "   17607900,\n",
       "   18339300,\n",
       "   26211000,\n",
       "   27756900,\n",
       "   4753800,\n",
       "   10194900,\n",
       "   27622200,\n",
       "   21342600,\n",
       "   22178400,\n",
       "   23879100,\n",
       "   11683200,\n",
       "   10107900,\n",
       "   4683600,\n",
       "   13442100,\n",
       "   17223600,\n",
       "   24314400,\n",
       "   25288500,\n",
       "   29382300,\n",
       "   28519500,\n",
       "   28016400,\n",
       "   21775200,\n",
       "   27607500,\n",
       "   27711900,\n",
       "   28748400,\n",
       "   28966500,\n",
       "   28785000,\n",
       "   28369800,\n",
       "   26683800,\n",
       "   27287700,\n",
       "   28120800,\n",
       "   27894300,\n",
       "   27756900,\n",
       "   25275000,\n",
       "   26827800,\n",
       "   24197400,\n",
       "   27721200,\n",
       "   25876800,\n",
       "   14297700,\n",
       "   21905400,\n",
       "   29499300,\n",
       "   29211600,\n",
       "   27363900,\n",
       "   26386200,\n",
       "   27834000,\n",
       "   28438800,\n",
       "   29639100,\n",
       "   28782000,\n",
       "   27997200,\n",
       "   28281900,\n",
       "   15512100,\n",
       "   27552600,\n",
       "   29576400,\n",
       "   29376000,\n",
       "   26856900,\n",
       "   9561000,\n",
       "   28809300,\n",
       "   28492200,\n",
       "   29228100,\n",
       "   24418500,\n",
       "   9593400,\n",
       "   24051000,\n",
       "   26674200,\n",
       "   27418500,\n",
       "   27996300,\n",
       "   26843400,\n",
       "   29630100,\n",
       "   26277300,\n",
       "   28711800,\n",
       "   28153200,\n",
       "   27720300,\n",
       "   21610800,\n",
       "   16884900,\n",
       "   24473400,\n",
       "   21722100,\n",
       "   27455100,\n",
       "   25038600,\n",
       "   26630700,\n",
       "   21466800,\n",
       "   20194500,\n",
       "   17749200,\n",
       "   25061100,\n",
       "   18042600,\n",
       "   27549600,\n",
       "   28202700,\n",
       "   27887400,\n",
       "   28307400,\n",
       "   27046200,\n",
       "   27049500,\n",
       "   24581700,\n",
       "   27210600,\n",
       "   28447200,\n",
       "   27582600,\n",
       "   26307600,\n",
       "   27106800,\n",
       "   22431300,\n",
       "   24995100,\n",
       "   25425000,\n",
       "   27566100,\n",
       "   24822900,\n",
       "   27515400,\n",
       "   25156200,\n",
       "   22476600,\n",
       "   25640100,\n",
       "   26698200,\n",
       "   25467900,\n",
       "   23038200,\n",
       "   24062400,\n",
       "   25153500,\n",
       "   26205000,\n",
       "   16696800,\n",
       "   22713000,\n",
       "   24162600,\n",
       "   21382800,\n",
       "   19601700,\n",
       "   19132500,\n",
       "   23094000,\n",
       "   26624400,\n",
       "   24564000,\n",
       "   24150000,\n",
       "   21825600,\n",
       "   8597700,\n",
       "   23881500,\n",
       "   25645500,\n",
       "   25407300,\n",
       "   23691300,\n",
       "   9206100,\n",
       "   13252800,\n",
       "   23594700,\n",
       "   24240600,\n",
       "   24568200,\n",
       "   21426300,\n",
       "   5718600,\n",
       "   12217200,\n",
       "   9767400,\n",
       "   24757500,\n",
       "   24924000,\n",
       "   23269200,\n",
       "   19087200,\n",
       "   21433200,\n",
       "   13264200,\n",
       "   20099700,\n",
       "   23046900,\n",
       "   23784300,\n",
       "   20214900,\n",
       "   21864300,\n",
       "   19905900,\n",
       "   2488500,\n",
       "   23481600,\n",
       "   23028000,\n",
       "   22245900,\n",
       "   5541600,\n",
       "   22499100,\n",
       "   22316700,\n",
       "   21735000,\n",
       "   22378200,\n",
       "   21857700,\n",
       "   21636000,\n",
       "   21397200,\n",
       "   21211800,\n",
       "   20420100,\n",
       "   17730000,\n",
       "   20090400,\n",
       "   20118000,\n",
       "   17304300,\n",
       "   18946200,\n",
       "   19169400,\n",
       "   17444400,\n",
       "   19750500,\n",
       "   4497900,\n",
       "   7159500,\n",
       "   19379400,\n",
       "   7951500,\n",
       "   18979500,\n",
       "   16578000,\n",
       "   2387700,\n",
       "   7866000,\n",
       "   18607800,\n",
       "   8252400,\n",
       "   15888900,\n",
       "   17957400,\n",
       "   13127700,\n",
       "   16103100,\n",
       "   17731500,\n",
       "   13401900,\n",
       "   6371400,\n",
       "   15388200,\n",
       "   12622200,\n",
       "   16852800,\n",
       "   17004900,\n",
       "   9921900,\n",
       "   9683100,\n",
       "   15408000,\n",
       "   15811800,\n",
       "   10533000,\n",
       "   11590500,\n",
       "   3276600,\n",
       "   7461300,\n",
       "   14222100,\n",
       "   15001800,\n",
       "   14462700,\n",
       "   10932900,\n",
       "   15290700,\n",
       "   12027600,\n",
       "   13481700,\n",
       "   10948800,\n",
       "   5500800,\n",
       "   14371500,\n",
       "   14166900,\n",
       "   13881900,\n",
       "   13974300,\n",
       "   14172900,\n",
       "   13446600,\n",
       "   13544700,\n",
       "   13427400,\n",
       "   11554800,\n",
       "   10320300,\n",
       "   7045800,\n",
       "   5149200,\n",
       "   10584300,\n",
       "   1093200,\n",
       "   2410200,\n",
       "   13180200,\n",
       "   12717300,\n",
       "   13010400,\n",
       "   12999900,\n",
       "   12810900,\n",
       "   12364500,\n",
       "   12475800,\n",
       "   12586200,\n",
       "   9589800,\n",
       "   10530600,\n",
       "   11386200,\n",
       "   12036600,\n",
       "   11645400,\n",
       "   10968900,\n",
       "   11333400,\n",
       "   11743500,\n",
       "   11288700,\n",
       "   3126600,\n",
       "   906000,\n",
       "   6975900,\n",
       "   10076700,\n",
       "   12107400,\n",
       "   11287500,\n",
       "   4458600,\n",
       "   12486900,\n",
       "   11932500,\n",
       "   10188600,\n",
       "   3411900,\n",
       "   1299600,\n",
       "   9032400,\n",
       "   6495600,\n",
       "   12930000,\n",
       "   11347500,\n",
       "   8400900,\n",
       "   3675300,\n",
       "   6149400,\n",
       "   6622800,\n",
       "   12842700,\n",
       "   12534000,\n",
       "   12640200,\n",
       "   12029700,\n",
       "   8218200,\n",
       "   897600,\n",
       "   1736700,\n",
       "   1829400,\n",
       "   14730600,\n",
       "   4659300,\n",
       "   5227500,\n",
       "   7802400,\n",
       "   4716300,\n",
       "   1436400,\n",
       "   7198800,\n",
       "   10884600,\n",
       "   14005200,\n",
       "   10362000,\n",
       "   14250300,\n",
       "   14436900,\n",
       "   4709100,\n",
       "   14906100,\n",
       "   7393500,\n",
       "   14862000,\n",
       "   1902600,\n",
       "   2938800,\n",
       "   15353400,\n",
       "   15255000,\n",
       "   15771600,\n",
       "   15444900,\n",
       "   15477000,\n",
       "   12000,\n",
       "   1313700,\n",
       "   3243900,\n",
       "   4035900,\n",
       "   3287400,\n",
       "   2799300,\n",
       "   4298700,\n",
       "   7221000,\n",
       "   15655500,\n",
       "   17345700,\n",
       "   17731500,\n",
       "   17870100,\n",
       "   17862000,\n",
       "   18236700,\n",
       "   18657600,\n",
       "   18039900,\n",
       "   7938600,\n",
       "   14849700,\n",
       "   19800000,\n",
       "   19771500,\n",
       "   19243500,\n",
       "   16930200,\n",
       "   20005800,\n",
       "   16619100,\n",
       "   20647200,\n",
       "   21316200,\n",
       "   21204900,\n",
       "   18894900,\n",
       "   20136600,\n",
       "   19968000,\n",
       "   19851300,\n",
       "   20361300,\n",
       "   3537600,\n",
       "   15045900,\n",
       "   16830900,\n",
       "   16044600,\n",
       "   19200900,\n",
       "   22450500,\n",
       "   22054500,\n",
       "   20571600,\n",
       "   11404200,\n",
       "   10730400,\n",
       "   18736800,\n",
       "   12054900,\n",
       "   11178000,\n",
       "   13825200,\n",
       "   9008400,\n",
       "   6233100,\n",
       "   11845800,\n",
       "   15081600,\n",
       "   2786700,\n",
       "   1788300,\n",
       "   24624900,\n",
       "   25132500,\n",
       "   24104100,\n",
       "   24154800,\n",
       "   24428400,\n",
       "   7841400,\n",
       "   5943900,\n",
       "   20200800,\n",
       "   10136700,\n",
       "   7181400,\n",
       "   11094000,\n",
       "   27009300,\n",
       "   19342200,\n",
       "   1811700,\n",
       "   10136400,\n",
       "   27688800,\n",
       "   25998000,\n",
       "   4230900,\n",
       "   19225500,\n",
       "   22419300,\n",
       "   26192100,\n",
       "   26698800,\n",
       "   26062200,\n",
       "   10311900,\n",
       "   10245900,\n",
       "   11333100,\n",
       "   26386500,\n",
       "   17673300,\n",
       "   28121400,\n",
       "   26566200,\n",
       "   14961900,\n",
       "   16417500,\n",
       "   19146000,\n",
       "   13559100,\n",
       "   22229700,\n",
       "   13672500,\n",
       "   14584800,\n",
       "   8337600,\n",
       "   20109300,\n",
       "   17841300,\n",
       "   18640800,\n",
       "   24267900,\n",
       "   27397200,\n",
       "   26605500,\n",
       "   28194900,\n",
       "   10163400,\n",
       "   29221500,\n",
       "   28743300,\n",
       "   21492900,\n",
       "   9711900,\n",
       "   23759700,\n",
       "   8852400,\n",
       "   22581000,\n",
       "   17891700,\n",
       "   3911400,\n",
       "   12607800,\n",
       "   11083800,\n",
       "   19437000,\n",
       "   12629100,\n",
       "   18104100,\n",
       "   9981000,\n",
       "   18996000,\n",
       "   13550400,\n",
       "   25976400,\n",
       "   24817500,\n",
       "   24752400,\n",
       "   29532900,\n",
       "   27228600,\n",
       "   21281400,\n",
       "   28296000,\n",
       "   23708400,\n",
       "   17811900,\n",
       "   28219500,\n",
       "   23471400,\n",
       "   27177600,\n",
       "   20811600,\n",
       "   10274100,\n",
       "   13078800,\n",
       "   13148400,\n",
       "   15253500,\n",
       "   20461800,\n",
       "   11436900,\n",
       "   17902500,\n",
       "   22656900,\n",
       "   27939300,\n",
       "   26853600,\n",
       "   12886800,\n",
       "   7151700,\n",
       "   17602500,\n",
       "   8013900,\n",
       "   9632700,\n",
       "   16325700,\n",
       "   25364100,\n",
       "   14165700,\n",
       "   19059300,\n",
       "   25462500,\n",
       "   21273600,\n",
       "   24463200,\n",
       "   28192500,\n",
       "   29036100,\n",
       "   22339500,\n",
       "   26402400,\n",
       "   17883600,\n",
       "   15735300,\n",
       "   14091000,\n",
       "   28623000,\n",
       "   28872900,\n",
       "   27261600,\n",
       "   28549800,\n",
       "   28535700,\n",
       "   28054500,\n",
       "   26860500,\n",
       "   27216900,\n",
       "   26431500,\n",
       "   16317300,\n",
       "   24177000,\n",
       "   27974400,\n",
       "   28281000,\n",
       "   21758100,\n",
       "   23130900,\n",
       "   26864100,\n",
       "   18547800,\n",
       "   18106200,\n",
       "   15814200,\n",
       "   23751900,\n",
       "   24789600,\n",
       "   26923800,\n",
       "   26769300,\n",
       "   26904900,\n",
       "   27070500,\n",
       "   27408600,\n",
       "   27492600,\n",
       "   27625500,\n",
       "   27195600,\n",
       "   26607300,\n",
       "   26807700,\n",
       "   26628300,\n",
       "   25440300,\n",
       "   23733600,\n",
       "   9499200,\n",
       "   6394800,\n",
       "   17863500,\n",
       "   22615500,\n",
       "   25275900,\n",
       "   23827500,\n",
       "   23124600,\n",
       "   21593700,\n",
       "   20481000,\n",
       "   24926700,\n",
       "   24511800,\n",
       "   24652500,\n",
       "   25120800,\n",
       "   20384400,\n",
       "   23632800,\n",
       "   25548300,\n",
       "   23546700,\n",
       "   20415600,\n",
       "   10705500,\n",
       "   13799400,\n",
       "   18705300,\n",
       "   14013000,\n",
       "   16587300,\n",
       "   6273000,\n",
       "   14166900,\n",
       "   23337000,\n",
       "   23778900,\n",
       "   20597100,\n",
       "   19752900,\n",
       "   21148200,\n",
       "   21214500,\n",
       "   20509800,\n",
       "   14761500,\n",
       "   17644800,\n",
       "   19924500,\n",
       "   20318400,\n",
       "   20572500,\n",
       "   19059600,\n",
       "   20634000,\n",
       "   10575300,\n",
       "   16389000,\n",
       "   16456200,\n",
       "   12445500,\n",
       "   19931400,\n",
       "   15985500,\n",
       "   20361000,\n",
       "   18146400,\n",
       "   11484300,\n",
       "   19402500,\n",
       "   19308300,\n",
       "   11016300,\n",
       "   15738900,\n",
       "   13631700,\n",
       "   20458200,\n",
       "   19636500,\n",
       "   14518800,\n",
       "   15958200,\n",
       "   18261000,\n",
       "   7105200,\n",
       "   15973200,\n",
       "   17762400,\n",
       "   10536900,\n",
       "   18781200,\n",
       "   18704100,\n",
       "   17667600,\n",
       "   15186300,\n",
       "   5460300,\n",
       "   18089100,\n",
       "   17856600,\n",
       "   18075300,\n",
       "   17715000,\n",
       "   16880700,\n",
       "   16292100,\n",
       "   16017600,\n",
       "   15581400,\n",
       "   13204200,\n",
       "   16207800,\n",
       "   13685100,\n",
       "   14337300,\n",
       "   14938200,\n",
       "   14835600,\n",
       "   15307200,\n",
       "   14962800,\n",
       "   14866500,\n",
       "   10683000,\n",
       "   11815800,\n",
       "   10580100,\n",
       "   11314200,\n",
       "   13910400,\n",
       "   13110000,\n",
       "   15180600,\n",
       "   13290600,\n",
       "   14392200,\n",
       "   11895600,\n",
       "   7975500,\n",
       "   10401000,\n",
       "   6557400,\n",
       "   8102100,\n",
       "   8457900,\n",
       "   2641500,\n",
       "   3871500,\n",
       "   13197900,\n",
       "   13407900,\n",
       "   12295500,\n",
       "   13100100,\n",
       "   2985900,\n",
       "   1444800,\n",
       "   12177000,\n",
       "   13453800,\n",
       "   12525900,\n",
       "   10543500,\n",
       "   6214500,\n",
       "   8417400,\n",
       "   1683900,\n",
       "   1512300,\n",
       "   1729500,\n",
       "   1512600,\n",
       "   2126100,\n",
       "   3476400,\n",
       "   1262700,\n",
       "   3074400,\n",
       "   12523800,\n",
       "   12089700,\n",
       "   12300600,\n",
       "   12206100,\n",
       "   12275700,\n",
       "   9737700,\n",
       "   1958400,\n",
       "   12325200,\n",
       "   12225900,\n",
       "   12256500,\n",
       "   1851300,\n",
       "   1408500,\n",
       "   10060800,\n",
       "   11388000,\n",
       "   12441000,\n",
       "   12450300]}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-8aa6d00d1e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(ts=test_set.get('target')[120], quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-79bf82a3e960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2655300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "predictor.predict(ts=2655300, quantiles=[0.10, 0.5, 0.90])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_timeseries = test_set[0].get('target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7437900,\n",
       " 8199600,\n",
       " 10694400,\n",
       " 8704800,\n",
       " 12803100,\n",
       " 12027300,\n",
       " 12519000,\n",
       " 11570100,\n",
       " 9417300,\n",
       " 9941700,\n",
       " 11968200,\n",
       " 12077400,\n",
       " 4561500,\n",
       " 13197300,\n",
       " 12252600,\n",
       " 11796900,\n",
       " 11721300,\n",
       " 7685400,\n",
       " 5451600,\n",
       " 9810900]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_timeseries[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b61695c07c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mour_timeseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "predictor.predict(ts=our_timeseries[:20], quantiles=[0.10, 0.5, 0.90])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-f80f688661f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "predictor.predict(ts=test_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19940106</td>\n",
       "      <td>6639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19940107</td>\n",
       "      <td>13244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19940108</td>\n",
       "      <td>12927900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19940109</td>\n",
       "      <td>12600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19940110</td>\n",
       "      <td>6406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19940111</td>\n",
       "      <td>12743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19940112</td>\n",
       "      <td>10453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19940113</td>\n",
       "      <td>12985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19940114</td>\n",
       "      <td>13080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19940115</td>\n",
       "      <td>11826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19940116</td>\n",
       "      <td>1974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19940117</td>\n",
       "      <td>13541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19940118</td>\n",
       "      <td>13673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19940119</td>\n",
       "      <td>6796800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19940120</td>\n",
       "      <td>5658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19940121</td>\n",
       "      <td>7073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19940122</td>\n",
       "      <td>3354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19940123</td>\n",
       "      <td>2579700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19940124</td>\n",
       "      <td>2387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19940125</td>\n",
       "      <td>8390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19940126</td>\n",
       "      <td>7326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19940127</td>\n",
       "      <td>10743900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19940128</td>\n",
       "      <td>12812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19940129</td>\n",
       "      <td>9065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19940130</td>\n",
       "      <td>3954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>20071202</td>\n",
       "      <td>12177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>20071203</td>\n",
       "      <td>13453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>20071204</td>\n",
       "      <td>12525900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>20071205</td>\n",
       "      <td>10543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>20071206</td>\n",
       "      <td>6214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>20071207</td>\n",
       "      <td>8417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>20071208</td>\n",
       "      <td>1683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>20071209</td>\n",
       "      <td>1512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>20071210</td>\n",
       "      <td>1729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>20071211</td>\n",
       "      <td>1512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>20071212</td>\n",
       "      <td>2126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>20071213</td>\n",
       "      <td>3476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>20071214</td>\n",
       "      <td>1262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>20071215</td>\n",
       "      <td>3074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>20071216</td>\n",
       "      <td>12523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>20071217</td>\n",
       "      <td>12089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>20071218</td>\n",
       "      <td>12300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>20071219</td>\n",
       "      <td>12206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>20071220</td>\n",
       "      <td>12275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>20071221</td>\n",
       "      <td>9737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>20071222</td>\n",
       "      <td>1958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>20071223</td>\n",
       "      <td>12325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>20071224</td>\n",
       "      <td>12225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>20071225</td>\n",
       "      <td>12256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5107</th>\n",
       "      <td>20071226</td>\n",
       "      <td>1851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>20071227</td>\n",
       "      <td>1408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5109</th>\n",
       "      <td>20071228</td>\n",
       "      <td>10060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>20071229</td>\n",
       "      <td>11388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>20071230</td>\n",
       "      <td>12441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>20071231</td>\n",
       "      <td>12450300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5113 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME\n",
       "0     19940101  12384900\n",
       "1     19940102  11908500\n",
       "2     19940103  12470700\n",
       "3     19940104  12725400\n",
       "4     19940105  10894800\n",
       "5     19940106   6639000\n",
       "6     19940107  13244700\n",
       "7     19940108  12927900\n",
       "8     19940109  12600300\n",
       "9     19940110   6406500\n",
       "10    19940111  12743400\n",
       "11    19940112  10453500\n",
       "12    19940113  12985200\n",
       "13    19940114  13080000\n",
       "14    19940115  11826300\n",
       "15    19940116   1974000\n",
       "16    19940117  13541700\n",
       "17    19940118  13673700\n",
       "18    19940119   6796800\n",
       "19    19940120   5658900\n",
       "20    19940121   7073400\n",
       "21    19940122   3354000\n",
       "22    19940123   2579700\n",
       "23    19940124   2387700\n",
       "24    19940125   8390700\n",
       "25    19940126   7326600\n",
       "26    19940127  10743900\n",
       "27    19940128  12812100\n",
       "28    19940129   9065100\n",
       "29    19940130   3954900\n",
       "...        ...       ...\n",
       "5083  20071202  12177000\n",
       "5084  20071203  13453800\n",
       "5085  20071204  12525900\n",
       "5086  20071205  10543500\n",
       "5087  20071206   6214500\n",
       "5088  20071207   8417400\n",
       "5089  20071208   1683900\n",
       "5090  20071209   1512300\n",
       "5091  20071210   1729500\n",
       "5092  20071211   1512600\n",
       "5093  20071212   2126100\n",
       "5094  20071213   3476400\n",
       "5095  20071214   1262700\n",
       "5096  20071215   3074400\n",
       "5097  20071216  12523800\n",
       "5098  20071217  12089700\n",
       "5099  20071218  12300600\n",
       "5100  20071219  12206100\n",
       "5101  20071220  12275700\n",
       "5102  20071221   9737700\n",
       "5103  20071222   1958400\n",
       "5104  20071223  12325200\n",
       "5105  20071224  12225900\n",
       "5106  20071225  12256500\n",
       "5107  20071226   1851300\n",
       "5108  20071227   1408500\n",
       "5109  20071228  10060800\n",
       "5110  20071229  11388000\n",
       "5111  20071230  12441000\n",
       "5112  20071231  12450300\n",
       "\n",
       "[5113 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Date','ACME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/ec2-user/dataset/test.csv' does not exist: b'/home/ec2-user/dataset/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-dc5db8225f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ec2-user/dataset/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ACME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/ec2-user/dataset/test.csv' does not exist: b'/home/ec2-user/dataset/test.csv'"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('/home/ec2-user/dataset/test.csv')[['Date','ACME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one = df[['Date','ACME']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "        lower_bound = round(df.shape[0] * .85)\n",
    "        upper_bound = df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = only_one[lower_bound:upper_bound]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20051125</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051126</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051127</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051128</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051129</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051130</th>\n",
       "      <td>12027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051201</th>\n",
       "      <td>12519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051202</th>\n",
       "      <td>11570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051203</th>\n",
       "      <td>9417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051204</th>\n",
       "      <td>9941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051205</th>\n",
       "      <td>11968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051206</th>\n",
       "      <td>12077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051207</th>\n",
       "      <td>4561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051208</th>\n",
       "      <td>13197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051209</th>\n",
       "      <td>12252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051210</th>\n",
       "      <td>11796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051211</th>\n",
       "      <td>11721300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051212</th>\n",
       "      <td>7685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051213</th>\n",
       "      <td>5451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051214</th>\n",
       "      <td>9810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051215</th>\n",
       "      <td>12423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051216</th>\n",
       "      <td>11590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051217</th>\n",
       "      <td>1609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051218</th>\n",
       "      <td>3166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051219</th>\n",
       "      <td>3104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051220</th>\n",
       "      <td>2655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051221</th>\n",
       "      <td>6268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051222</th>\n",
       "      <td>11357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051223</th>\n",
       "      <td>10005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051224</th>\n",
       "      <td>11276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071202</th>\n",
       "      <td>12177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071203</th>\n",
       "      <td>13453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071204</th>\n",
       "      <td>12525900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071205</th>\n",
       "      <td>10543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071206</th>\n",
       "      <td>6214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071207</th>\n",
       "      <td>8417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071208</th>\n",
       "      <td>1683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071209</th>\n",
       "      <td>1512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071210</th>\n",
       "      <td>1729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071211</th>\n",
       "      <td>1512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071212</th>\n",
       "      <td>2126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071213</th>\n",
       "      <td>3476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071214</th>\n",
       "      <td>1262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071215</th>\n",
       "      <td>3074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071216</th>\n",
       "      <td>12523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071217</th>\n",
       "      <td>12089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071218</th>\n",
       "      <td>12300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071219</th>\n",
       "      <td>12206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071220</th>\n",
       "      <td>12275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071221</th>\n",
       "      <td>9737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071222</th>\n",
       "      <td>1958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071223</th>\n",
       "      <td>12325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071224</th>\n",
       "      <td>12225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071225</th>\n",
       "      <td>12256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071226</th>\n",
       "      <td>1851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071227</th>\n",
       "      <td>1408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071228</th>\n",
       "      <td>10060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071229</th>\n",
       "      <td>11388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071230</th>\n",
       "      <td>12441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071231</th>\n",
       "      <td>12450300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ACME\n",
       "Date              \n",
       "20051125   7437900\n",
       "20051126   8199600\n",
       "20051127  10694400\n",
       "20051128   8704800\n",
       "20051129  12803100\n",
       "20051130  12027300\n",
       "20051201  12519000\n",
       "20051202  11570100\n",
       "20051203   9417300\n",
       "20051204   9941700\n",
       "20051205  11968200\n",
       "20051206  12077400\n",
       "20051207   4561500\n",
       "20051208  13197300\n",
       "20051209  12252600\n",
       "20051210  11796900\n",
       "20051211  11721300\n",
       "20051212   7685400\n",
       "20051213   5451600\n",
       "20051214   9810900\n",
       "20051215  12423600\n",
       "20051216  11590800\n",
       "20051217   1609800\n",
       "20051218   3166500\n",
       "20051219   3104700\n",
       "20051220   2655300\n",
       "20051221   6268200\n",
       "20051222  11357100\n",
       "20051223  10005900\n",
       "20051224  11276400\n",
       "...            ...\n",
       "20071202  12177000\n",
       "20071203  13453800\n",
       "20071204  12525900\n",
       "20071205  10543500\n",
       "20071206   6214500\n",
       "20071207   8417400\n",
       "20071208   1683900\n",
       "20071209   1512300\n",
       "20071210   1729500\n",
       "20071211   1512600\n",
       "20071212   2126100\n",
       "20071213   3476400\n",
       "20071214   1262700\n",
       "20071215   3074400\n",
       "20071216  12523800\n",
       "20071217  12089700\n",
       "20071218  12300600\n",
       "20071219  12206100\n",
       "20071220  12275700\n",
       "20071221   9737700\n",
       "20071222   1958400\n",
       "20071223  12325200\n",
       "20071224  12225900\n",
       "20071225  12256500\n",
       "20071226   1851300\n",
       "20071227   1408500\n",
       "20071228  10060800\n",
       "20071229  11388000\n",
       "20071230  12441000\n",
       "20071231  12450300\n",
       "\n",
       "[767 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1b4243bec224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__decode_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36m__encode_request\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdynamic_feat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         configuration = {\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mseries_to_dict\u001b[0;34m(ts, cat, dynamic_feat)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mencode_target\u001b[0;34m(ts)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(ts=test_df, quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-1b4243bec224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__decode_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36m__encode_request\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdynamic_feat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         configuration = {\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mseries_to_dict\u001b[0;34m(ts, cat, dynamic_feat)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mencode_target\u001b[0;34m(ts)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(ts=test_df, quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-711c74f5b217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>20051125</td>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>20051126</td>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>20051127</td>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>20051128</td>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>20051129</td>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME\n",
       "4346  20051125   7437900\n",
       "4347  20051126   8199600\n",
       "4348  20051127  10694400\n",
       "4349  20051128   8704800\n",
       "4350  20051129  12803100"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4346    20051125\n",
       "4347    20051126\n",
       "4348    20051127\n",
       "4349    20051128\n",
       "4350    20051129\n",
       "Name: Date, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Date'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_values=test_df['Date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4346   1970-01-01 00:00:00.020051125\n",
       "4347   1970-01-01 00:00:00.020051126\n",
       "4348   1970-01-01 00:00:00.020051127\n",
       "4349   1970-01-01 00:00:00.020051128\n",
       "4350   1970-01-01 00:00:00.020051129\n",
       "4351   1970-01-01 00:00:00.020051130\n",
       "4352   1970-01-01 00:00:00.020051201\n",
       "4353   1970-01-01 00:00:00.020051202\n",
       "4354   1970-01-01 00:00:00.020051203\n",
       "4355   1970-01-01 00:00:00.020051204\n",
       "4356   1970-01-01 00:00:00.020051205\n",
       "4357   1970-01-01 00:00:00.020051206\n",
       "4358   1970-01-01 00:00:00.020051207\n",
       "4359   1970-01-01 00:00:00.020051208\n",
       "4360   1970-01-01 00:00:00.020051209\n",
       "4361   1970-01-01 00:00:00.020051210\n",
       "4362   1970-01-01 00:00:00.020051211\n",
       "4363   1970-01-01 00:00:00.020051212\n",
       "4364   1970-01-01 00:00:00.020051213\n",
       "4365   1970-01-01 00:00:00.020051214\n",
       "4366   1970-01-01 00:00:00.020051215\n",
       "4367   1970-01-01 00:00:00.020051216\n",
       "4368   1970-01-01 00:00:00.020051217\n",
       "4369   1970-01-01 00:00:00.020051218\n",
       "4370   1970-01-01 00:00:00.020051219\n",
       "4371   1970-01-01 00:00:00.020051220\n",
       "4372   1970-01-01 00:00:00.020051221\n",
       "4373   1970-01-01 00:00:00.020051222\n",
       "4374   1970-01-01 00:00:00.020051223\n",
       "4375   1970-01-01 00:00:00.020051224\n",
       "                    ...             \n",
       "5083   1970-01-01 00:00:00.020071202\n",
       "5084   1970-01-01 00:00:00.020071203\n",
       "5085   1970-01-01 00:00:00.020071204\n",
       "5086   1970-01-01 00:00:00.020071205\n",
       "5087   1970-01-01 00:00:00.020071206\n",
       "5088   1970-01-01 00:00:00.020071207\n",
       "5089   1970-01-01 00:00:00.020071208\n",
       "5090   1970-01-01 00:00:00.020071209\n",
       "5091   1970-01-01 00:00:00.020071210\n",
       "5092   1970-01-01 00:00:00.020071211\n",
       "5093   1970-01-01 00:00:00.020071212\n",
       "5094   1970-01-01 00:00:00.020071213\n",
       "5095   1970-01-01 00:00:00.020071214\n",
       "5096   1970-01-01 00:00:00.020071215\n",
       "5097   1970-01-01 00:00:00.020071216\n",
       "5098   1970-01-01 00:00:00.020071217\n",
       "5099   1970-01-01 00:00:00.020071218\n",
       "5100   1970-01-01 00:00:00.020071219\n",
       "5101   1970-01-01 00:00:00.020071220\n",
       "5102   1970-01-01 00:00:00.020071221\n",
       "5103   1970-01-01 00:00:00.020071222\n",
       "5104   1970-01-01 00:00:00.020071223\n",
       "5105   1970-01-01 00:00:00.020071224\n",
       "5106   1970-01-01 00:00:00.020071225\n",
       "5107   1970-01-01 00:00:00.020071226\n",
       "5108   1970-01-01 00:00:00.020071227\n",
       "5109   1970-01-01 00:00:00.020071228\n",
       "5110   1970-01-01 00:00:00.020071229\n",
       "5111   1970-01-01 00:00:00.020071230\n",
       "5112   1970-01-01 00:00:00.020071231\n",
       "Name: Date, Length: 767, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(test_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4346   2005-11-25\n",
       "4347   2005-11-26\n",
       "4348   2005-11-27\n",
       "4349   2005-11-28\n",
       "4350   2005-11-29\n",
       "Name: Date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(test_df['Date'], format='%Y%m%d').head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_strings = pd.to_datetime(test_df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4346   2005-11-25\n",
       "4347   2005-11-26\n",
       "4348   2005-11-27\n",
       "4349   2005-11-28\n",
       "4350   2005-11-29\n",
       "Name: Date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_strings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>4346</th>\n",
       "      <th>4347</th>\n",
       "      <th>4348</th>\n",
       "      <th>4349</th>\n",
       "      <th>4350</th>\n",
       "      <th>4351</th>\n",
       "      <th>4352</th>\n",
       "      <th>4353</th>\n",
       "      <th>...</th>\n",
       "      <th>5103</th>\n",
       "      <th>5104</th>\n",
       "      <th>5105</th>\n",
       "      <th>5106</th>\n",
       "      <th>5107</th>\n",
       "      <th>5108</th>\n",
       "      <th>5109</th>\n",
       "      <th>5110</th>\n",
       "      <th>5111</th>\n",
       "      <th>5112</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>20051125</td>\n",
       "      <td>7437900</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>20051126</td>\n",
       "      <td>8199600</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>20051127</td>\n",
       "      <td>10694400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>20051128</td>\n",
       "      <td>8704800</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>20051129</td>\n",
       "      <td>12803100</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>20051130</td>\n",
       "      <td>12027300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>20051201</td>\n",
       "      <td>12519000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>20051202</td>\n",
       "      <td>11570100</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>20051203</td>\n",
       "      <td>9417300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4355</th>\n",
       "      <td>20051204</td>\n",
       "      <td>9941700</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>20051205</td>\n",
       "      <td>11968200</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>20051206</td>\n",
       "      <td>12077400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>20051207</td>\n",
       "      <td>4561500</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4359</th>\n",
       "      <td>20051208</td>\n",
       "      <td>13197300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360</th>\n",
       "      <td>20051209</td>\n",
       "      <td>12252600</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>20051210</td>\n",
       "      <td>11796900</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>20051211</td>\n",
       "      <td>11721300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>20051212</td>\n",
       "      <td>7685400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>20051213</td>\n",
       "      <td>5451600</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>20051214</td>\n",
       "      <td>9810900</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>20051215</td>\n",
       "      <td>12423600</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>20051216</td>\n",
       "      <td>11590800</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>20051217</td>\n",
       "      <td>1609800</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>20051218</td>\n",
       "      <td>3166500</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>20051219</td>\n",
       "      <td>3104700</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>20051220</td>\n",
       "      <td>2655300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>20051221</td>\n",
       "      <td>6268200</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>20051222</td>\n",
       "      <td>11357100</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>20051223</td>\n",
       "      <td>10005900</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>20051224</td>\n",
       "      <td>11276400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>20071203</td>\n",
       "      <td>13453800</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>20071204</td>\n",
       "      <td>12525900</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>20071205</td>\n",
       "      <td>10543500</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>20071206</td>\n",
       "      <td>6214500</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>20071207</td>\n",
       "      <td>8417400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>20071208</td>\n",
       "      <td>1683900</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>20071209</td>\n",
       "      <td>1512300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>20071210</td>\n",
       "      <td>1729500</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>20071211</td>\n",
       "      <td>1512600</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>20071212</td>\n",
       "      <td>2126100</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>20071213</td>\n",
       "      <td>3476400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>20071214</td>\n",
       "      <td>1262700</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>20071215</td>\n",
       "      <td>3074400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>20071216</td>\n",
       "      <td>12523800</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>20071217</td>\n",
       "      <td>12089700</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>20071218</td>\n",
       "      <td>12300600</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>20071219</td>\n",
       "      <td>12206100</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>20071220</td>\n",
       "      <td>12275700</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>20071221</td>\n",
       "      <td>9737700</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>20071222</td>\n",
       "      <td>1958400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>20071223</td>\n",
       "      <td>12325200</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>20071224</td>\n",
       "      <td>12225900</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>20071225</td>\n",
       "      <td>12256500</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5107</th>\n",
       "      <td>20071226</td>\n",
       "      <td>1851300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>20071227</td>\n",
       "      <td>1408500</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5109</th>\n",
       "      <td>20071228</td>\n",
       "      <td>10060800</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>20071229</td>\n",
       "      <td>11388000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>20071230</td>\n",
       "      <td>12441000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>20071231</td>\n",
       "      <td>12450300</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2005-11-25</td>\n",
       "      <td>2005-11-26</td>\n",
       "      <td>2005-11-27</td>\n",
       "      <td>2005-11-28</td>\n",
       "      <td>2005-11-29</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>2005-12-01</td>\n",
       "      <td>2005-12-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2007-12-22</td>\n",
       "      <td>2007-12-23</td>\n",
       "      <td>2007-12-24</td>\n",
       "      <td>2007-12-25</td>\n",
       "      <td>2007-12-26</td>\n",
       "      <td>2007-12-27</td>\n",
       "      <td>2007-12-28</td>\n",
       "      <td>2007-12-29</td>\n",
       "      <td>2007-12-30</td>\n",
       "      <td>2007-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME       4346       4347       4348       4349  \\\n",
       "4346  20051125   7437900        NaT        NaT        NaT        NaT   \n",
       "4347  20051126   8199600        NaT        NaT        NaT        NaT   \n",
       "4348  20051127  10694400        NaT        NaT        NaT        NaT   \n",
       "4349  20051128   8704800        NaT        NaT        NaT        NaT   \n",
       "4350  20051129  12803100        NaT        NaT        NaT        NaT   \n",
       "4351  20051130  12027300        NaT        NaT        NaT        NaT   \n",
       "4352  20051201  12519000        NaT        NaT        NaT        NaT   \n",
       "4353  20051202  11570100        NaT        NaT        NaT        NaT   \n",
       "4354  20051203   9417300        NaT        NaT        NaT        NaT   \n",
       "4355  20051204   9941700        NaT        NaT        NaT        NaT   \n",
       "4356  20051205  11968200        NaT        NaT        NaT        NaT   \n",
       "4357  20051206  12077400        NaT        NaT        NaT        NaT   \n",
       "4358  20051207   4561500        NaT        NaT        NaT        NaT   \n",
       "4359  20051208  13197300        NaT        NaT        NaT        NaT   \n",
       "4360  20051209  12252600        NaT        NaT        NaT        NaT   \n",
       "4361  20051210  11796900        NaT        NaT        NaT        NaT   \n",
       "4362  20051211  11721300        NaT        NaT        NaT        NaT   \n",
       "4363  20051212   7685400        NaT        NaT        NaT        NaT   \n",
       "4364  20051213   5451600        NaT        NaT        NaT        NaT   \n",
       "4365  20051214   9810900        NaT        NaT        NaT        NaT   \n",
       "4366  20051215  12423600        NaT        NaT        NaT        NaT   \n",
       "4367  20051216  11590800        NaT        NaT        NaT        NaT   \n",
       "4368  20051217   1609800        NaT        NaT        NaT        NaT   \n",
       "4369  20051218   3166500        NaT        NaT        NaT        NaT   \n",
       "4370  20051219   3104700        NaT        NaT        NaT        NaT   \n",
       "4371  20051220   2655300        NaT        NaT        NaT        NaT   \n",
       "4372  20051221   6268200        NaT        NaT        NaT        NaT   \n",
       "4373  20051222  11357100        NaT        NaT        NaT        NaT   \n",
       "4374  20051223  10005900        NaT        NaT        NaT        NaT   \n",
       "4375  20051224  11276400        NaT        NaT        NaT        NaT   \n",
       "...        ...       ...        ...        ...        ...        ...   \n",
       "5084  20071203  13453800        NaT        NaT        NaT        NaT   \n",
       "5085  20071204  12525900        NaT        NaT        NaT        NaT   \n",
       "5086  20071205  10543500        NaT        NaT        NaT        NaT   \n",
       "5087  20071206   6214500        NaT        NaT        NaT        NaT   \n",
       "5088  20071207   8417400        NaT        NaT        NaT        NaT   \n",
       "5089  20071208   1683900        NaT        NaT        NaT        NaT   \n",
       "5090  20071209   1512300        NaT        NaT        NaT        NaT   \n",
       "5091  20071210   1729500        NaT        NaT        NaT        NaT   \n",
       "5092  20071211   1512600        NaT        NaT        NaT        NaT   \n",
       "5093  20071212   2126100        NaT        NaT        NaT        NaT   \n",
       "5094  20071213   3476400        NaT        NaT        NaT        NaT   \n",
       "5095  20071214   1262700        NaT        NaT        NaT        NaT   \n",
       "5096  20071215   3074400        NaT        NaT        NaT        NaT   \n",
       "5097  20071216  12523800        NaT        NaT        NaT        NaT   \n",
       "5098  20071217  12089700        NaT        NaT        NaT        NaT   \n",
       "5099  20071218  12300600        NaT        NaT        NaT        NaT   \n",
       "5100  20071219  12206100        NaT        NaT        NaT        NaT   \n",
       "5101  20071220  12275700        NaT        NaT        NaT        NaT   \n",
       "5102  20071221   9737700        NaT        NaT        NaT        NaT   \n",
       "5103  20071222   1958400        NaT        NaT        NaT        NaT   \n",
       "5104  20071223  12325200        NaT        NaT        NaT        NaT   \n",
       "5105  20071224  12225900        NaT        NaT        NaT        NaT   \n",
       "5106  20071225  12256500        NaT        NaT        NaT        NaT   \n",
       "5107  20071226   1851300        NaT        NaT        NaT        NaT   \n",
       "5108  20071227   1408500        NaT        NaT        NaT        NaT   \n",
       "5109  20071228  10060800        NaT        NaT        NaT        NaT   \n",
       "5110  20071229  11388000        NaT        NaT        NaT        NaT   \n",
       "5111  20071230  12441000        NaT        NaT        NaT        NaT   \n",
       "5112  20071231  12450300        NaT        NaT        NaT        NaT   \n",
       "Date       NaT       NaT 2005-11-25 2005-11-26 2005-11-27 2005-11-28   \n",
       "\n",
       "           4350       4351       4352       4353  ...       5103       5104  \\\n",
       "4346        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4347        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4348        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4349        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4350        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4351        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4352        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4353        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4354        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4355        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4356        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4357        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4358        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4359        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4360        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4361        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4362        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4363        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4364        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4365        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4366        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4367        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4368        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4369        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4370        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4371        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4372        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4373        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4374        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "4375        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "5084        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5085        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5086        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5087        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5088        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5089        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5090        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5091        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5092        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5093        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5094        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5095        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5096        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5097        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5098        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5099        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5100        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5101        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5102        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5103        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5104        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5105        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5106        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5107        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5108        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5109        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5110        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5111        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "5112        NaT        NaT        NaT        NaT  ...        NaT        NaT   \n",
       "Date 2005-11-29 2005-11-30 2005-12-01 2005-12-02  ... 2007-12-22 2007-12-23   \n",
       "\n",
       "           5105       5106       5107       5108       5109       5110  \\\n",
       "4346        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4347        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4348        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4349        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4350        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4351        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4352        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4353        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4354        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4355        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4356        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4357        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4358        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4359        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4360        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4361        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4362        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4363        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4364        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4365        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4366        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4367        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4368        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4369        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4370        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4371        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4372        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4373        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4374        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "4375        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "5084        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5085        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5086        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5087        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5088        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5089        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5090        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5091        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5092        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5093        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5094        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5095        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5096        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5097        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5098        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5099        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5100        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5101        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5102        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5103        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5104        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5105        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5106        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5107        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5108        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5109        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5110        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5111        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "5112        NaT        NaT        NaT        NaT        NaT        NaT   \n",
       "Date 2007-12-24 2007-12-25 2007-12-26 2007-12-27 2007-12-28 2007-12-29   \n",
       "\n",
       "           5111       5112  \n",
       "4346        NaT        NaT  \n",
       "4347        NaT        NaT  \n",
       "4348        NaT        NaT  \n",
       "4349        NaT        NaT  \n",
       "4350        NaT        NaT  \n",
       "4351        NaT        NaT  \n",
       "4352        NaT        NaT  \n",
       "4353        NaT        NaT  \n",
       "4354        NaT        NaT  \n",
       "4355        NaT        NaT  \n",
       "4356        NaT        NaT  \n",
       "4357        NaT        NaT  \n",
       "4358        NaT        NaT  \n",
       "4359        NaT        NaT  \n",
       "4360        NaT        NaT  \n",
       "4361        NaT        NaT  \n",
       "4362        NaT        NaT  \n",
       "4363        NaT        NaT  \n",
       "4364        NaT        NaT  \n",
       "4365        NaT        NaT  \n",
       "4366        NaT        NaT  \n",
       "4367        NaT        NaT  \n",
       "4368        NaT        NaT  \n",
       "4369        NaT        NaT  \n",
       "4370        NaT        NaT  \n",
       "4371        NaT        NaT  \n",
       "4372        NaT        NaT  \n",
       "4373        NaT        NaT  \n",
       "4374        NaT        NaT  \n",
       "4375        NaT        NaT  \n",
       "...         ...        ...  \n",
       "5084        NaT        NaT  \n",
       "5085        NaT        NaT  \n",
       "5086        NaT        NaT  \n",
       "5087        NaT        NaT  \n",
       "5088        NaT        NaT  \n",
       "5089        NaT        NaT  \n",
       "5090        NaT        NaT  \n",
       "5091        NaT        NaT  \n",
       "5092        NaT        NaT  \n",
       "5093        NaT        NaT  \n",
       "5094        NaT        NaT  \n",
       "5095        NaT        NaT  \n",
       "5096        NaT        NaT  \n",
       "5097        NaT        NaT  \n",
       "5098        NaT        NaT  \n",
       "5099        NaT        NaT  \n",
       "5100        NaT        NaT  \n",
       "5101        NaT        NaT  \n",
       "5102        NaT        NaT  \n",
       "5103        NaT        NaT  \n",
       "5104        NaT        NaT  \n",
       "5105        NaT        NaT  \n",
       "5106        NaT        NaT  \n",
       "5107        NaT        NaT  \n",
       "5108        NaT        NaT  \n",
       "5109        NaT        NaT  \n",
       "5110        NaT        NaT  \n",
       "5111        NaT        NaT  \n",
       "5112        NaT        NaT  \n",
       "Date 2007-12-30 2007-12-31  \n",
       "\n",
       "[768 rows x 769 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.append(as_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>20051125</td>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>20051126</td>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>20051127</td>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>20051128</td>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>20051129</td>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME\n",
       "4346  20051125   7437900\n",
       "4347  20051126   8199600\n",
       "4348  20051127  10694400\n",
       "4349  20051128   8704800\n",
       "4350  20051129  12803100"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test_df['Timestamp'] = as_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>20051125</td>\n",
       "      <td>7437900</td>\n",
       "      <td>2005-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>20051126</td>\n",
       "      <td>8199600</td>\n",
       "      <td>2005-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>20051127</td>\n",
       "      <td>10694400</td>\n",
       "      <td>2005-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>20051128</td>\n",
       "      <td>8704800</td>\n",
       "      <td>2005-11-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>20051129</td>\n",
       "      <td>12803100</td>\n",
       "      <td>2005-11-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME  Timestamp\n",
       "4346  20051125   7437900 2005-11-25\n",
       "4347  20051126   8199600 2005-11-26\n",
       "4348  20051127  10694400 2005-11-27\n",
       "4349  20051128   8704800 2005-11-28\n",
       "4350  20051129  12803100 2005-11-29"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-839535406fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "test_df.drop('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>7437900</td>\n",
       "      <td>2005-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>8199600</td>\n",
       "      <td>2005-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>10694400</td>\n",
       "      <td>2005-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>8704800</td>\n",
       "      <td>2005-11-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>12803100</td>\n",
       "      <td>2005-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>12027300</td>\n",
       "      <td>2005-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>12519000</td>\n",
       "      <td>2005-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>11570100</td>\n",
       "      <td>2005-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>9417300</td>\n",
       "      <td>2005-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4355</th>\n",
       "      <td>9941700</td>\n",
       "      <td>2005-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>11968200</td>\n",
       "      <td>2005-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>12077400</td>\n",
       "      <td>2005-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>4561500</td>\n",
       "      <td>2005-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4359</th>\n",
       "      <td>13197300</td>\n",
       "      <td>2005-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360</th>\n",
       "      <td>12252600</td>\n",
       "      <td>2005-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>11796900</td>\n",
       "      <td>2005-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>11721300</td>\n",
       "      <td>2005-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>7685400</td>\n",
       "      <td>2005-12-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>5451600</td>\n",
       "      <td>2005-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>9810900</td>\n",
       "      <td>2005-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>12423600</td>\n",
       "      <td>2005-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>11590800</td>\n",
       "      <td>2005-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>1609800</td>\n",
       "      <td>2005-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>3166500</td>\n",
       "      <td>2005-12-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>3104700</td>\n",
       "      <td>2005-12-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>2655300</td>\n",
       "      <td>2005-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>6268200</td>\n",
       "      <td>2005-12-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>11357100</td>\n",
       "      <td>2005-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>10005900</td>\n",
       "      <td>2005-12-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>11276400</td>\n",
       "      <td>2005-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>12177000</td>\n",
       "      <td>2007-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>13453800</td>\n",
       "      <td>2007-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>12525900</td>\n",
       "      <td>2007-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>10543500</td>\n",
       "      <td>2007-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>6214500</td>\n",
       "      <td>2007-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>8417400</td>\n",
       "      <td>2007-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>1683900</td>\n",
       "      <td>2007-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>1512300</td>\n",
       "      <td>2007-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>1729500</td>\n",
       "      <td>2007-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>1512600</td>\n",
       "      <td>2007-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>2126100</td>\n",
       "      <td>2007-12-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>3476400</td>\n",
       "      <td>2007-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>1262700</td>\n",
       "      <td>2007-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>3074400</td>\n",
       "      <td>2007-12-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>12523800</td>\n",
       "      <td>2007-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>12089700</td>\n",
       "      <td>2007-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>12300600</td>\n",
       "      <td>2007-12-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>12206100</td>\n",
       "      <td>2007-12-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>12275700</td>\n",
       "      <td>2007-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>9737700</td>\n",
       "      <td>2007-12-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>1958400</td>\n",
       "      <td>2007-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>12325200</td>\n",
       "      <td>2007-12-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>12225900</td>\n",
       "      <td>2007-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>12256500</td>\n",
       "      <td>2007-12-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5107</th>\n",
       "      <td>1851300</td>\n",
       "      <td>2007-12-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>1408500</td>\n",
       "      <td>2007-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5109</th>\n",
       "      <td>10060800</td>\n",
       "      <td>2007-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>11388000</td>\n",
       "      <td>2007-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>12441000</td>\n",
       "      <td>2007-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>12450300</td>\n",
       "      <td>2007-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ACME  Timestamp\n",
       "4346   7437900 2005-11-25\n",
       "4347   8199600 2005-11-26\n",
       "4348  10694400 2005-11-27\n",
       "4349   8704800 2005-11-28\n",
       "4350  12803100 2005-11-29\n",
       "4351  12027300 2005-11-30\n",
       "4352  12519000 2005-12-01\n",
       "4353  11570100 2005-12-02\n",
       "4354   9417300 2005-12-03\n",
       "4355   9941700 2005-12-04\n",
       "4356  11968200 2005-12-05\n",
       "4357  12077400 2005-12-06\n",
       "4358   4561500 2005-12-07\n",
       "4359  13197300 2005-12-08\n",
       "4360  12252600 2005-12-09\n",
       "4361  11796900 2005-12-10\n",
       "4362  11721300 2005-12-11\n",
       "4363   7685400 2005-12-12\n",
       "4364   5451600 2005-12-13\n",
       "4365   9810900 2005-12-14\n",
       "4366  12423600 2005-12-15\n",
       "4367  11590800 2005-12-16\n",
       "4368   1609800 2005-12-17\n",
       "4369   3166500 2005-12-18\n",
       "4370   3104700 2005-12-19\n",
       "4371   2655300 2005-12-20\n",
       "4372   6268200 2005-12-21\n",
       "4373  11357100 2005-12-22\n",
       "4374  10005900 2005-12-23\n",
       "4375  11276400 2005-12-24\n",
       "...        ...        ...\n",
       "5083  12177000 2007-12-02\n",
       "5084  13453800 2007-12-03\n",
       "5085  12525900 2007-12-04\n",
       "5086  10543500 2007-12-05\n",
       "5087   6214500 2007-12-06\n",
       "5088   8417400 2007-12-07\n",
       "5089   1683900 2007-12-08\n",
       "5090   1512300 2007-12-09\n",
       "5091   1729500 2007-12-10\n",
       "5092   1512600 2007-12-11\n",
       "5093   2126100 2007-12-12\n",
       "5094   3476400 2007-12-13\n",
       "5095   1262700 2007-12-14\n",
       "5096   3074400 2007-12-15\n",
       "5097  12523800 2007-12-16\n",
       "5098  12089700 2007-12-17\n",
       "5099  12300600 2007-12-18\n",
       "5100  12206100 2007-12-19\n",
       "5101  12275700 2007-12-20\n",
       "5102   9737700 2007-12-21\n",
       "5103   1958400 2007-12-22\n",
       "5104  12325200 2007-12-23\n",
       "5105  12225900 2007-12-24\n",
       "5106  12256500 2007-12-25\n",
       "5107   1851300 2007-12-26\n",
       "5108   1408500 2007-12-27\n",
       "5109  10060800 2007-12-28\n",
       "5110  11388000 2007-12-29\n",
       "5111  12441000 2007-12-30\n",
       "5112  12450300 2007-12-31\n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_columns = test_df.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-11-25</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-26</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-27</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-28</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-29</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-30</th>\n",
       "      <td>12027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-01</th>\n",
       "      <td>12519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-02</th>\n",
       "      <td>11570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-03</th>\n",
       "      <td>9417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-04</th>\n",
       "      <td>9941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-05</th>\n",
       "      <td>11968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-06</th>\n",
       "      <td>12077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-07</th>\n",
       "      <td>4561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-08</th>\n",
       "      <td>13197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-09</th>\n",
       "      <td>12252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-10</th>\n",
       "      <td>11796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-11</th>\n",
       "      <td>11721300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-12</th>\n",
       "      <td>7685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-13</th>\n",
       "      <td>5451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-14</th>\n",
       "      <td>9810900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-15</th>\n",
       "      <td>12423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-16</th>\n",
       "      <td>11590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-17</th>\n",
       "      <td>1609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-18</th>\n",
       "      <td>3166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-19</th>\n",
       "      <td>3104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-20</th>\n",
       "      <td>2655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-21</th>\n",
       "      <td>6268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-22</th>\n",
       "      <td>11357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-23</th>\n",
       "      <td>10005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-24</th>\n",
       "      <td>11276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-02</th>\n",
       "      <td>12177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-03</th>\n",
       "      <td>13453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-04</th>\n",
       "      <td>12525900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-05</th>\n",
       "      <td>10543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-06</th>\n",
       "      <td>6214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-07</th>\n",
       "      <td>8417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-08</th>\n",
       "      <td>1683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-09</th>\n",
       "      <td>1512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-10</th>\n",
       "      <td>1729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-11</th>\n",
       "      <td>1512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-12</th>\n",
       "      <td>2126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-13</th>\n",
       "      <td>3476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-14</th>\n",
       "      <td>1262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-15</th>\n",
       "      <td>3074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-16</th>\n",
       "      <td>12523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-17</th>\n",
       "      <td>12089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-18</th>\n",
       "      <td>12300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-19</th>\n",
       "      <td>12206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-20</th>\n",
       "      <td>12275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-21</th>\n",
       "      <td>9737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-22</th>\n",
       "      <td>1958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-23</th>\n",
       "      <td>12325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-24</th>\n",
       "      <td>12225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-25</th>\n",
       "      <td>12256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-26</th>\n",
       "      <td>1851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-27</th>\n",
       "      <td>1408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-28</th>\n",
       "      <td>10060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-29</th>\n",
       "      <td>11388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-30</th>\n",
       "      <td>12441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-31</th>\n",
       "      <td>12450300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2005-11-25   7437900\n",
       "2005-11-26   8199600\n",
       "2005-11-27  10694400\n",
       "2005-11-28   8704800\n",
       "2005-11-29  12803100\n",
       "2005-11-30  12027300\n",
       "2005-12-01  12519000\n",
       "2005-12-02  11570100\n",
       "2005-12-03   9417300\n",
       "2005-12-04   9941700\n",
       "2005-12-05  11968200\n",
       "2005-12-06  12077400\n",
       "2005-12-07   4561500\n",
       "2005-12-08  13197300\n",
       "2005-12-09  12252600\n",
       "2005-12-10  11796900\n",
       "2005-12-11  11721300\n",
       "2005-12-12   7685400\n",
       "2005-12-13   5451600\n",
       "2005-12-14   9810900\n",
       "2005-12-15  12423600\n",
       "2005-12-16  11590800\n",
       "2005-12-17   1609800\n",
       "2005-12-18   3166500\n",
       "2005-12-19   3104700\n",
       "2005-12-20   2655300\n",
       "2005-12-21   6268200\n",
       "2005-12-22  11357100\n",
       "2005-12-23  10005900\n",
       "2005-12-24  11276400\n",
       "...              ...\n",
       "2007-12-02  12177000\n",
       "2007-12-03  13453800\n",
       "2007-12-04  12525900\n",
       "2007-12-05  10543500\n",
       "2007-12-06   6214500\n",
       "2007-12-07   8417400\n",
       "2007-12-08   1683900\n",
       "2007-12-09   1512300\n",
       "2007-12-10   1729500\n",
       "2007-12-11   1512600\n",
       "2007-12-12   2126100\n",
       "2007-12-13   3476400\n",
       "2007-12-14   1262700\n",
       "2007-12-15   3074400\n",
       "2007-12-16  12523800\n",
       "2007-12-17  12089700\n",
       "2007-12-18  12300600\n",
       "2007-12-19  12206100\n",
       "2007-12-20  12275700\n",
       "2007-12-21   9737700\n",
       "2007-12-22   1958400\n",
       "2007-12-23  12325200\n",
       "2007-12-24  12225900\n",
       "2007-12-25  12256500\n",
       "2007-12-26   1851300\n",
       "2007-12-27   1408500\n",
       "2007-12-28  10060800\n",
       "2007-12-29  11388000\n",
       "2007-12-30  12441000\n",
       "2007-12-31  12450300\n",
       "\n",
       "[767 rows x 1 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_columns.set_index('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>7437900</td>\n",
       "      <td>2005-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>8199600</td>\n",
       "      <td>2005-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4348</th>\n",
       "      <td>10694400</td>\n",
       "      <td>2005-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>8704800</td>\n",
       "      <td>2005-11-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>12803100</td>\n",
       "      <td>2005-11-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ACME  Timestamp\n",
       "4346   7437900 2005-11-25\n",
       "4347   8199600 2005-11-26\n",
       "4348  10694400 2005-11-27\n",
       "4349   8704800 2005-11-28\n",
       "4350  12803100 2005-11-29"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_columns.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamped = two_columns.set_index('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-11-25</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-26</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-27</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-28</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-29</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2005-11-25   7437900\n",
       "2005-11-26   8199600\n",
       "2005-11-27  10694400\n",
       "2005-11-28   8704800\n",
       "2005-11-29  12803100"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot add integral value to Timestamp without freq.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-93daa46cc516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestamped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/timestamps.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__add__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot add integral value to Timestamp without freq."
     ]
    }
   ],
   "source": [
    "result = predictor.predict(ts=timestamped, quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-84db0e919dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestamped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(ts=timestamped[0], quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timestamped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-11-25</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-26</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-27</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-28</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-29</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2005-11-25   7437900\n",
       "2005-11-26   8199600\n",
       "2005-11-27  10694400\n",
       "2005-11-28   8704800\n",
       "2005-11-29  12803100"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Timestamp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-444bee666f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtimestamped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Timestamp'"
     ]
    }
   ],
   "source": [
    "timestamped['Timestamp'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-11-25', '2005-11-26', '2005-11-27', '2005-11-28',\n",
       "               '2005-11-29', '2005-11-30', '2005-12-01', '2005-12-02',\n",
       "               '2005-12-03', '2005-12-04',\n",
       "               ...\n",
       "               '2007-12-22', '2007-12-23', '2007-12-24', '2007-12-25',\n",
       "               '2007-12-26', '2007-12-27', '2007-12-28', '2007-12-29',\n",
       "               '2007-12-30', '2007-12-31'],\n",
       "              dtype='datetime64[ns]', name='Timestamp', length=767, freq=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamped = timestamped.asfreq('d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-11-25', '2005-11-26', '2005-11-27', '2005-11-28',\n",
       "               '2005-11-29', '2005-11-30', '2005-12-01', '2005-12-02',\n",
       "               '2005-12-03', '2005-12-04',\n",
       "               ...\n",
       "               '2007-12-22', '2007-12-23', '2007-12-24', '2007-12-25',\n",
       "               '2007-12-26', '2007-12-27', '2007-12-28', '2007-12-29',\n",
       "               '2007-12-30', '2007-12-31'],\n",
       "              dtype='datetime64[ns]', name='Timestamp', length=767, freq='D')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-11-25</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-26</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-27</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-28</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-29</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2005-11-25   7437900\n",
       "2005-11-26   8199600\n",
       "2005-11-27  10694400\n",
       "2005-11-28   8704800\n",
       "2005-11-29  12803100"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:19: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-93daa46cc516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestamped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__decode_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36m__encode_request\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdynamic_feat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         configuration = {\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mseries_to_dict\u001b[0;34m(ts, cat, dynamic_feat)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mencode_target\u001b[0;34m(ts)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseries_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(ts=timestamped, quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2005-11-25 00:00:00', freq='D')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-1ac336799d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "np.isfinite(timestamped.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:19: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:46: FutureWarning: Creating a DatetimeIndex by passing range endpoints is deprecated.  Use `pandas.date_range` instead.\n"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(ts=timestamped['ACME'], quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACME    7437900\n",
       "Name: 2005-11-25 00:00:00, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4346",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4346",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-e1bcb0c68c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwo_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4346\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4346"
     ]
    }
   ],
   "source": [
    "two_columns[4346]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACME                     7437900\n",
       "Timestamp    2005-11-25 00:00:00\n",
       "Name: 4346, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_columns.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-11-25</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-26</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-27</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-28</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-29</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2005-11-25   7437900\n",
       "2005-11-26   8199600\n",
       "2005-11-27  10694400\n",
       "2005-11-28   8704800\n",
       "2005-11-29  12803100"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-11-25', '2005-11-26', '2005-11-27', '2005-11-28',\n",
       "               '2005-11-29', '2005-11-30', '2005-12-01', '2005-12-02',\n",
       "               '2005-12-03', '2005-12-04',\n",
       "               ...\n",
       "               '2007-12-22', '2007-12-23', '2007-12-24', '2007-12-25',\n",
       "               '2007-12-26', '2007-12-27', '2007-12-28', '2007-12-29',\n",
       "               '2007-12-30', '2007-12-31'],\n",
       "              dtype='datetime64[ns]', name='Timestamp', length=767, freq='D')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = []\n",
    "for i in range(767):\n",
    "    timeseries.append(timestamped.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ACME    7437900\n",
       " Name: 2005-11-25 00:00:00, dtype: int64, ACME    8199600\n",
       " Name: 2005-11-26 00:00:00, dtype: int64, ACME    10694400\n",
       " Name: 2005-11-27 00:00:00, dtype: int64, ACME    8704800\n",
       " Name: 2005-11-28 00:00:00, dtype: int64, ACME    12803100\n",
       " Name: 2005-11-29 00:00:00, dtype: int64, ACME    12027300\n",
       " Name: 2005-11-30 00:00:00, dtype: int64, ACME    12519000\n",
       " Name: 2005-12-01 00:00:00, dtype: int64, ACME    11570100\n",
       " Name: 2005-12-02 00:00:00, dtype: int64, ACME    9417300\n",
       " Name: 2005-12-03 00:00:00, dtype: int64, ACME    9941700\n",
       " Name: 2005-12-04 00:00:00, dtype: int64, ACME    11968200\n",
       " Name: 2005-12-05 00:00:00, dtype: int64, ACME    12077400\n",
       " Name: 2005-12-06 00:00:00, dtype: int64, ACME    4561500\n",
       " Name: 2005-12-07 00:00:00, dtype: int64, ACME    13197300\n",
       " Name: 2005-12-08 00:00:00, dtype: int64, ACME    12252600\n",
       " Name: 2005-12-09 00:00:00, dtype: int64, ACME    11796900\n",
       " Name: 2005-12-10 00:00:00, dtype: int64, ACME    11721300\n",
       " Name: 2005-12-11 00:00:00, dtype: int64, ACME    7685400\n",
       " Name: 2005-12-12 00:00:00, dtype: int64, ACME    5451600\n",
       " Name: 2005-12-13 00:00:00, dtype: int64, ACME    9810900\n",
       " Name: 2005-12-14 00:00:00, dtype: int64, ACME    12423600\n",
       " Name: 2005-12-15 00:00:00, dtype: int64, ACME    11590800\n",
       " Name: 2005-12-16 00:00:00, dtype: int64, ACME    1609800\n",
       " Name: 2005-12-17 00:00:00, dtype: int64, ACME    3166500\n",
       " Name: 2005-12-18 00:00:00, dtype: int64, ACME    3104700\n",
       " Name: 2005-12-19 00:00:00, dtype: int64, ACME    2655300\n",
       " Name: 2005-12-20 00:00:00, dtype: int64, ACME    6268200\n",
       " Name: 2005-12-21 00:00:00, dtype: int64, ACME    11357100\n",
       " Name: 2005-12-22 00:00:00, dtype: int64, ACME    10005900\n",
       " Name: 2005-12-23 00:00:00, dtype: int64, ACME    11276400\n",
       " Name: 2005-12-24 00:00:00, dtype: int64, ACME    11739600\n",
       " Name: 2005-12-25 00:00:00, dtype: int64, ACME    11870100\n",
       " Name: 2005-12-26 00:00:00, dtype: int64, ACME    11089500\n",
       " Name: 2005-12-27 00:00:00, dtype: int64, ACME    8790000\n",
       " Name: 2005-12-28 00:00:00, dtype: int64, ACME    11895600\n",
       " Name: 2005-12-29 00:00:00, dtype: int64, ACME    12165300\n",
       " Name: 2005-12-30 00:00:00, dtype: int64, ACME    7313100\n",
       " Name: 2005-12-31 00:00:00, dtype: int64, ACME    8252700\n",
       " Name: 2006-01-01 00:00:00, dtype: int64, ACME    11886600\n",
       " Name: 2006-01-02 00:00:00, dtype: int64, ACME    12072600\n",
       " Name: 2006-01-03 00:00:00, dtype: int64, ACME    10689000\n",
       " Name: 2006-01-04 00:00:00, dtype: int64, ACME    12416400\n",
       " Name: 2006-01-05 00:00:00, dtype: int64, ACME    12456000\n",
       " Name: 2006-01-06 00:00:00, dtype: int64, ACME    12309900\n",
       " Name: 2006-01-07 00:00:00, dtype: int64, ACME    9021300\n",
       " Name: 2006-01-08 00:00:00, dtype: int64, ACME    9142200\n",
       " Name: 2006-01-09 00:00:00, dtype: int64, ACME    12788700\n",
       " Name: 2006-01-10 00:00:00, dtype: int64, ACME    13036200\n",
       " Name: 2006-01-11 00:00:00, dtype: int64, ACME    11170200\n",
       " Name: 2006-01-12 00:00:00, dtype: int64, ACME    13284000\n",
       " Name: 2006-01-13 00:00:00, dtype: int64, ACME    11163600\n",
       " Name: 2006-01-14 00:00:00, dtype: int64, ACME    8958300\n",
       " Name: 2006-01-15 00:00:00, dtype: int64, ACME    7813500\n",
       " Name: 2006-01-16 00:00:00, dtype: int64, ACME    13773600\n",
       " Name: 2006-01-17 00:00:00, dtype: int64, ACME    11419800\n",
       " Name: 2006-01-18 00:00:00, dtype: int64, ACME    11549700\n",
       " Name: 2006-01-19 00:00:00, dtype: int64, ACME    11442600\n",
       " Name: 2006-01-20 00:00:00, dtype: int64, ACME    12351300\n",
       " Name: 2006-01-21 00:00:00, dtype: int64, ACME    10773000\n",
       " Name: 2006-01-22 00:00:00, dtype: int64, ACME    13398300\n",
       " Name: 2006-01-23 00:00:00, dtype: int64, ACME    14266200\n",
       " Name: 2006-01-24 00:00:00, dtype: int64, ACME    13605900\n",
       " Name: 2006-01-25 00:00:00, dtype: int64, ACME    7520400\n",
       " Name: 2006-01-26 00:00:00, dtype: int64, ACME    3917700\n",
       " Name: 2006-01-27 00:00:00, dtype: int64, ACME    8274000\n",
       " Name: 2006-01-28 00:00:00, dtype: int64, ACME    13047900\n",
       " Name: 2006-01-29 00:00:00, dtype: int64, ACME    15183000\n",
       " Name: 2006-01-30 00:00:00, dtype: int64, ACME    14596800\n",
       " Name: 2006-01-31 00:00:00, dtype: int64, ACME    14742600\n",
       " Name: 2006-02-01 00:00:00, dtype: int64, ACME    14959500\n",
       " Name: 2006-02-02 00:00:00, dtype: int64, ACME    14130000\n",
       " Name: 2006-02-03 00:00:00, dtype: int64, ACME    14401800\n",
       " Name: 2006-02-04 00:00:00, dtype: int64, ACME    14026500\n",
       " Name: 2006-02-05 00:00:00, dtype: int64, ACME    15988200\n",
       " Name: 2006-02-06 00:00:00, dtype: int64, ACME    16366200\n",
       " Name: 2006-02-07 00:00:00, dtype: int64, ACME    16184100\n",
       " Name: 2006-02-08 00:00:00, dtype: int64, ACME    14604600\n",
       " Name: 2006-02-09 00:00:00, dtype: int64, ACME    13388400\n",
       " Name: 2006-02-10 00:00:00, dtype: int64, ACME    17387400\n",
       " Name: 2006-02-11 00:00:00, dtype: int64, ACME    17657100\n",
       " Name: 2006-02-12 00:00:00, dtype: int64, ACME    17504400\n",
       " Name: 2006-02-13 00:00:00, dtype: int64, ACME    17134200\n",
       " Name: 2006-02-14 00:00:00, dtype: int64, ACME    16662600\n",
       " Name: 2006-02-15 00:00:00, dtype: int64, ACME    14343600\n",
       " Name: 2006-02-16 00:00:00, dtype: int64, ACME    8371200\n",
       " Name: 2006-02-17 00:00:00, dtype: int64, ACME    6264900\n",
       " Name: 2006-02-18 00:00:00, dtype: int64, ACME    4595700\n",
       " Name: 2006-02-19 00:00:00, dtype: int64, ACME    2858700\n",
       " Name: 2006-02-20 00:00:00, dtype: int64, ACME    3267300\n",
       " Name: 2006-02-21 00:00:00, dtype: int64, ACME    5874300\n",
       " Name: 2006-02-22 00:00:00, dtype: int64, ACME    11259300\n",
       " Name: 2006-02-23 00:00:00, dtype: int64, ACME    9503700\n",
       " Name: 2006-02-24 00:00:00, dtype: int64, ACME    10217100\n",
       " Name: 2006-02-25 00:00:00, dtype: int64, ACME    18846900\n",
       " Name: 2006-02-26 00:00:00, dtype: int64, ACME    18447300\n",
       " Name: 2006-02-27 00:00:00, dtype: int64, ACME    18719100\n",
       " Name: 2006-02-28 00:00:00, dtype: int64, ACME    16971600\n",
       " Name: 2006-03-01 00:00:00, dtype: int64, ACME    10020000\n",
       " Name: 2006-03-02 00:00:00, dtype: int64, ACME    18915300\n",
       " Name: 2006-03-03 00:00:00, dtype: int64, ACME    14541900\n",
       " Name: 2006-03-04 00:00:00, dtype: int64, ACME    19118400\n",
       " Name: 2006-03-05 00:00:00, dtype: int64, ACME    19968900\n",
       " Name: 2006-03-06 00:00:00, dtype: int64, ACME    15298500\n",
       " Name: 2006-03-07 00:00:00, dtype: int64, ACME    10589100\n",
       " Name: 2006-03-08 00:00:00, dtype: int64, ACME    12572100\n",
       " Name: 2006-03-09 00:00:00, dtype: int64, ACME    18650700\n",
       " Name: 2006-03-10 00:00:00, dtype: int64, ACME    19277400\n",
       " Name: 2006-03-11 00:00:00, dtype: int64, ACME    14770800\n",
       " Name: 2006-03-12 00:00:00, dtype: int64, ACME    22081800\n",
       " Name: 2006-03-13 00:00:00, dtype: int64, ACME    22152000\n",
       " Name: 2006-03-14 00:00:00, dtype: int64, ACME    19551000\n",
       " Name: 2006-03-15 00:00:00, dtype: int64, ACME    22544400\n",
       " Name: 2006-03-16 00:00:00, dtype: int64, ACME    5637600\n",
       " Name: 2006-03-17 00:00:00, dtype: int64, ACME    1752900\n",
       " Name: 2006-03-18 00:00:00, dtype: int64, ACME    1836300\n",
       " Name: 2006-03-19 00:00:00, dtype: int64, ACME    13347000\n",
       " Name: 2006-03-20 00:00:00, dtype: int64, ACME    16548600\n",
       " Name: 2006-03-21 00:00:00, dtype: int64, ACME    5025300\n",
       " Name: 2006-03-22 00:00:00, dtype: int64, ACME    9642000\n",
       " Name: 2006-03-23 00:00:00, dtype: int64, ACME    23787900\n",
       " Name: 2006-03-24 00:00:00, dtype: int64, ACME    22641900\n",
       " Name: 2006-03-25 00:00:00, dtype: int64, ACME    21170100\n",
       " Name: 2006-03-26 00:00:00, dtype: int64, ACME    19276200\n",
       " Name: 2006-03-27 00:00:00, dtype: int64, ACME    23799900\n",
       " Name: 2006-03-28 00:00:00, dtype: int64, ACME    10562100\n",
       " Name: 2006-03-29 00:00:00, dtype: int64, ACME    10595700\n",
       " Name: 2006-03-30 00:00:00, dtype: int64, ACME    22674000\n",
       " Name: 2006-03-31 00:00:00, dtype: int64, ACME    14618100\n",
       " Name: 2006-04-01 00:00:00, dtype: int64, ACME    24726600\n",
       " Name: 2006-04-02 00:00:00, dtype: int64, ACME    21721800\n",
       " Name: 2006-04-03 00:00:00, dtype: int64, ACME    21429300\n",
       " Name: 2006-04-04 00:00:00, dtype: int64, ACME    18830700\n",
       " Name: 2006-04-05 00:00:00, dtype: int64, ACME    20777100\n",
       " Name: 2006-04-06 00:00:00, dtype: int64, ACME    18309300\n",
       " Name: 2006-04-07 00:00:00, dtype: int64, ACME    22853700\n",
       " Name: 2006-04-08 00:00:00, dtype: int64, ACME    25850400\n",
       " Name: 2006-04-09 00:00:00, dtype: int64, ACME    22198500\n",
       " Name: 2006-04-10 00:00:00, dtype: int64, ACME    12409500\n",
       " Name: 2006-04-11 00:00:00, dtype: int64, ACME    25104900\n",
       " Name: 2006-04-12 00:00:00, dtype: int64, ACME    25654800\n",
       " Name: 2006-04-13 00:00:00, dtype: int64, ACME    24021600\n",
       " Name: 2006-04-14 00:00:00, dtype: int64, ACME    21522300\n",
       " Name: 2006-04-15 00:00:00, dtype: int64, ACME    26623800\n",
       " Name: 2006-04-16 00:00:00, dtype: int64, ACME    27378000\n",
       " Name: 2006-04-17 00:00:00, dtype: int64, ACME    27258300\n",
       " Name: 2006-04-18 00:00:00, dtype: int64, ACME    26766600\n",
       " Name: 2006-04-19 00:00:00, dtype: int64, ACME    11901000\n",
       " Name: 2006-04-20 00:00:00, dtype: int64, ACME    24991800\n",
       " Name: 2006-04-21 00:00:00, dtype: int64, ACME    23052000\n",
       " Name: 2006-04-22 00:00:00, dtype: int64, ACME    23409000\n",
       " Name: 2006-04-23 00:00:00, dtype: int64, ACME    17607900\n",
       " Name: 2006-04-24 00:00:00, dtype: int64, ACME    18339300\n",
       " Name: 2006-04-25 00:00:00, dtype: int64, ACME    26211000\n",
       " Name: 2006-04-26 00:00:00, dtype: int64, ACME    27756900\n",
       " Name: 2006-04-27 00:00:00, dtype: int64, ACME    4753800\n",
       " Name: 2006-04-28 00:00:00, dtype: int64, ACME    10194900\n",
       " Name: 2006-04-29 00:00:00, dtype: int64, ACME    27622200\n",
       " Name: 2006-04-30 00:00:00, dtype: int64, ACME    21342600\n",
       " Name: 2006-05-01 00:00:00, dtype: int64, ACME    22178400\n",
       " Name: 2006-05-02 00:00:00, dtype: int64, ACME    23879100\n",
       " Name: 2006-05-03 00:00:00, dtype: int64, ACME    11683200\n",
       " Name: 2006-05-04 00:00:00, dtype: int64, ACME    10107900\n",
       " Name: 2006-05-05 00:00:00, dtype: int64, ACME    4683600\n",
       " Name: 2006-05-06 00:00:00, dtype: int64, ACME    13442100\n",
       " Name: 2006-05-07 00:00:00, dtype: int64, ACME    17223600\n",
       " Name: 2006-05-08 00:00:00, dtype: int64, ACME    24314400\n",
       " Name: 2006-05-09 00:00:00, dtype: int64, ACME    25288500\n",
       " Name: 2006-05-10 00:00:00, dtype: int64, ACME    29382300\n",
       " Name: 2006-05-11 00:00:00, dtype: int64, ACME    28519500\n",
       " Name: 2006-05-12 00:00:00, dtype: int64, ACME    28016400\n",
       " Name: 2006-05-13 00:00:00, dtype: int64, ACME    21775200\n",
       " Name: 2006-05-14 00:00:00, dtype: int64, ACME    27607500\n",
       " Name: 2006-05-15 00:00:00, dtype: int64, ACME    27711900\n",
       " Name: 2006-05-16 00:00:00, dtype: int64, ACME    28748400\n",
       " Name: 2006-05-17 00:00:00, dtype: int64, ACME    28966500\n",
       " Name: 2006-05-18 00:00:00, dtype: int64, ACME    28785000\n",
       " Name: 2006-05-19 00:00:00, dtype: int64, ACME    28369800\n",
       " Name: 2006-05-20 00:00:00, dtype: int64, ACME    26683800\n",
       " Name: 2006-05-21 00:00:00, dtype: int64, ACME    27287700\n",
       " Name: 2006-05-22 00:00:00, dtype: int64, ACME    28120800\n",
       " Name: 2006-05-23 00:00:00, dtype: int64, ACME    27894300\n",
       " Name: 2006-05-24 00:00:00, dtype: int64, ACME    27756900\n",
       " Name: 2006-05-25 00:00:00, dtype: int64, ACME    25275000\n",
       " Name: 2006-05-26 00:00:00, dtype: int64, ACME    26827800\n",
       " Name: 2006-05-27 00:00:00, dtype: int64, ACME    24197400\n",
       " Name: 2006-05-28 00:00:00, dtype: int64, ACME    27721200\n",
       " Name: 2006-05-29 00:00:00, dtype: int64, ACME    25876800\n",
       " Name: 2006-05-30 00:00:00, dtype: int64, ACME    14297700\n",
       " Name: 2006-05-31 00:00:00, dtype: int64, ACME    21905400\n",
       " Name: 2006-06-01 00:00:00, dtype: int64, ACME    29499300\n",
       " Name: 2006-06-02 00:00:00, dtype: int64, ACME    29211600\n",
       " Name: 2006-06-03 00:00:00, dtype: int64, ACME    27363900\n",
       " Name: 2006-06-04 00:00:00, dtype: int64, ACME    26386200\n",
       " Name: 2006-06-05 00:00:00, dtype: int64, ACME    27834000\n",
       " Name: 2006-06-06 00:00:00, dtype: int64, ACME    28438800\n",
       " Name: 2006-06-07 00:00:00, dtype: int64, ACME    29639100\n",
       " Name: 2006-06-08 00:00:00, dtype: int64, ACME    28782000\n",
       " Name: 2006-06-09 00:00:00, dtype: int64, ACME    27997200\n",
       " Name: 2006-06-10 00:00:00, dtype: int64, ACME    28281900\n",
       " Name: 2006-06-11 00:00:00, dtype: int64, ACME    15512100\n",
       " Name: 2006-06-12 00:00:00, dtype: int64, ACME    27552600\n",
       " Name: 2006-06-13 00:00:00, dtype: int64, ACME    29576400\n",
       " Name: 2006-06-14 00:00:00, dtype: int64, ACME    29376000\n",
       " Name: 2006-06-15 00:00:00, dtype: int64, ACME    26856900\n",
       " Name: 2006-06-16 00:00:00, dtype: int64, ACME    9561000\n",
       " Name: 2006-06-17 00:00:00, dtype: int64, ACME    28809300\n",
       " Name: 2006-06-18 00:00:00, dtype: int64, ACME    28492200\n",
       " Name: 2006-06-19 00:00:00, dtype: int64, ACME    29228100\n",
       " Name: 2006-06-20 00:00:00, dtype: int64, ACME    24418500\n",
       " Name: 2006-06-21 00:00:00, dtype: int64, ACME    9593400\n",
       " Name: 2006-06-22 00:00:00, dtype: int64, ACME    24051000\n",
       " Name: 2006-06-23 00:00:00, dtype: int64, ACME    26674200\n",
       " Name: 2006-06-24 00:00:00, dtype: int64, ACME    27418500\n",
       " Name: 2006-06-25 00:00:00, dtype: int64, ACME    27996300\n",
       " Name: 2006-06-26 00:00:00, dtype: int64, ACME    26843400\n",
       " Name: 2006-06-27 00:00:00, dtype: int64, ACME    29630100\n",
       " Name: 2006-06-28 00:00:00, dtype: int64, ACME    26277300\n",
       " Name: 2006-06-29 00:00:00, dtype: int64, ACME    28711800\n",
       " Name: 2006-06-30 00:00:00, dtype: int64, ACME    28153200\n",
       " Name: 2006-07-01 00:00:00, dtype: int64, ACME    27720300\n",
       " Name: 2006-07-02 00:00:00, dtype: int64, ACME    21610800\n",
       " Name: 2006-07-03 00:00:00, dtype: int64, ACME    16884900\n",
       " Name: 2006-07-04 00:00:00, dtype: int64, ACME    24473400\n",
       " Name: 2006-07-05 00:00:00, dtype: int64, ACME    21722100\n",
       " Name: 2006-07-06 00:00:00, dtype: int64, ACME    27455100\n",
       " Name: 2006-07-07 00:00:00, dtype: int64, ACME    25038600\n",
       " Name: 2006-07-08 00:00:00, dtype: int64, ACME    26630700\n",
       " Name: 2006-07-09 00:00:00, dtype: int64, ACME    21466800\n",
       " Name: 2006-07-10 00:00:00, dtype: int64, ACME    20194500\n",
       " Name: 2006-07-11 00:00:00, dtype: int64, ACME    17749200\n",
       " Name: 2006-07-12 00:00:00, dtype: int64, ACME    25061100\n",
       " Name: 2006-07-13 00:00:00, dtype: int64, ACME    18042600\n",
       " Name: 2006-07-14 00:00:00, dtype: int64, ACME    27549600\n",
       " Name: 2006-07-15 00:00:00, dtype: int64, ACME    28202700\n",
       " Name: 2006-07-16 00:00:00, dtype: int64, ACME    27887400\n",
       " Name: 2006-07-17 00:00:00, dtype: int64, ACME    28307400\n",
       " Name: 2006-07-18 00:00:00, dtype: int64, ACME    27046200\n",
       " Name: 2006-07-19 00:00:00, dtype: int64, ACME    27049500\n",
       " Name: 2006-07-20 00:00:00, dtype: int64, ACME    24581700\n",
       " Name: 2006-07-21 00:00:00, dtype: int64, ACME    27210600\n",
       " Name: 2006-07-22 00:00:00, dtype: int64, ACME    28447200\n",
       " Name: 2006-07-23 00:00:00, dtype: int64, ACME    27582600\n",
       " Name: 2006-07-24 00:00:00, dtype: int64, ACME    26307600\n",
       " Name: 2006-07-25 00:00:00, dtype: int64, ACME    27106800\n",
       " Name: 2006-07-26 00:00:00, dtype: int64, ACME    22431300\n",
       " Name: 2006-07-27 00:00:00, dtype: int64, ACME    24995100\n",
       " Name: 2006-07-28 00:00:00, dtype: int64, ACME    25425000\n",
       " Name: 2006-07-29 00:00:00, dtype: int64, ACME    27566100\n",
       " Name: 2006-07-30 00:00:00, dtype: int64, ACME    24822900\n",
       " Name: 2006-07-31 00:00:00, dtype: int64, ACME    27515400\n",
       " Name: 2006-08-01 00:00:00, dtype: int64, ACME    25156200\n",
       " Name: 2006-08-02 00:00:00, dtype: int64, ACME    22476600\n",
       " Name: 2006-08-03 00:00:00, dtype: int64, ACME    25640100\n",
       " Name: 2006-08-04 00:00:00, dtype: int64, ACME    26698200\n",
       " Name: 2006-08-05 00:00:00, dtype: int64, ACME    25467900\n",
       " Name: 2006-08-06 00:00:00, dtype: int64, ACME    23038200\n",
       " Name: 2006-08-07 00:00:00, dtype: int64, ACME    24062400\n",
       " Name: 2006-08-08 00:00:00, dtype: int64, ACME    25153500\n",
       " Name: 2006-08-09 00:00:00, dtype: int64, ACME    26205000\n",
       " Name: 2006-08-10 00:00:00, dtype: int64, ACME    16696800\n",
       " Name: 2006-08-11 00:00:00, dtype: int64, ACME    22713000\n",
       " Name: 2006-08-12 00:00:00, dtype: int64, ACME    24162600\n",
       " Name: 2006-08-13 00:00:00, dtype: int64, ACME    21382800\n",
       " Name: 2006-08-14 00:00:00, dtype: int64, ACME    19601700\n",
       " Name: 2006-08-15 00:00:00, dtype: int64, ACME    19132500\n",
       " Name: 2006-08-16 00:00:00, dtype: int64, ACME    23094000\n",
       " Name: 2006-08-17 00:00:00, dtype: int64, ACME    26624400\n",
       " Name: 2006-08-18 00:00:00, dtype: int64, ACME    24564000\n",
       " Name: 2006-08-19 00:00:00, dtype: int64, ACME    24150000\n",
       " Name: 2006-08-20 00:00:00, dtype: int64, ACME    21825600\n",
       " Name: 2006-08-21 00:00:00, dtype: int64, ACME    8597700\n",
       " Name: 2006-08-22 00:00:00, dtype: int64, ACME    23881500\n",
       " Name: 2006-08-23 00:00:00, dtype: int64, ACME    25645500\n",
       " Name: 2006-08-24 00:00:00, dtype: int64, ACME    25407300\n",
       " Name: 2006-08-25 00:00:00, dtype: int64, ACME    23691300\n",
       " Name: 2006-08-26 00:00:00, dtype: int64, ACME    9206100\n",
       " Name: 2006-08-27 00:00:00, dtype: int64, ACME    13252800\n",
       " Name: 2006-08-28 00:00:00, dtype: int64, ACME    23594700\n",
       " Name: 2006-08-29 00:00:00, dtype: int64, ACME    24240600\n",
       " Name: 2006-08-30 00:00:00, dtype: int64, ACME    24568200\n",
       " Name: 2006-08-31 00:00:00, dtype: int64, ACME    21426300\n",
       " Name: 2006-09-01 00:00:00, dtype: int64, ACME    5718600\n",
       " Name: 2006-09-02 00:00:00, dtype: int64, ACME    12217200\n",
       " Name: 2006-09-03 00:00:00, dtype: int64, ACME    9767400\n",
       " Name: 2006-09-04 00:00:00, dtype: int64, ACME    24757500\n",
       " Name: 2006-09-05 00:00:00, dtype: int64, ACME    24924000\n",
       " Name: 2006-09-06 00:00:00, dtype: int64, ACME    23269200\n",
       " Name: 2006-09-07 00:00:00, dtype: int64, ACME    19087200\n",
       " Name: 2006-09-08 00:00:00, dtype: int64, ACME    21433200\n",
       " Name: 2006-09-09 00:00:00, dtype: int64, ACME    13264200\n",
       " Name: 2006-09-10 00:00:00, dtype: int64, ACME    20099700\n",
       " Name: 2006-09-11 00:00:00, dtype: int64, ACME    23046900\n",
       " Name: 2006-09-12 00:00:00, dtype: int64, ACME    23784300\n",
       " Name: 2006-09-13 00:00:00, dtype: int64, ACME    20214900\n",
       " Name: 2006-09-14 00:00:00, dtype: int64, ACME    21864300\n",
       " Name: 2006-09-15 00:00:00, dtype: int64, ACME    19905900\n",
       " Name: 2006-09-16 00:00:00, dtype: int64, ACME    2488500\n",
       " Name: 2006-09-17 00:00:00, dtype: int64, ACME    23481600\n",
       " Name: 2006-09-18 00:00:00, dtype: int64, ACME    23028000\n",
       " Name: 2006-09-19 00:00:00, dtype: int64, ACME    22245900\n",
       " Name: 2006-09-20 00:00:00, dtype: int64, ACME    5541600\n",
       " Name: 2006-09-21 00:00:00, dtype: int64, ACME    22499100\n",
       " Name: 2006-09-22 00:00:00, dtype: int64, ACME    22316700\n",
       " Name: 2006-09-23 00:00:00, dtype: int64, ACME    21735000\n",
       " Name: 2006-09-24 00:00:00, dtype: int64, ACME    22378200\n",
       " Name: 2006-09-25 00:00:00, dtype: int64, ACME    21857700\n",
       " Name: 2006-09-26 00:00:00, dtype: int64, ACME    21636000\n",
       " Name: 2006-09-27 00:00:00, dtype: int64, ACME    21397200\n",
       " Name: 2006-09-28 00:00:00, dtype: int64, ACME    21211800\n",
       " Name: 2006-09-29 00:00:00, dtype: int64, ACME    20420100\n",
       " Name: 2006-09-30 00:00:00, dtype: int64, ACME    17730000\n",
       " Name: 2006-10-01 00:00:00, dtype: int64, ACME    20090400\n",
       " Name: 2006-10-02 00:00:00, dtype: int64, ACME    20118000\n",
       " Name: 2006-10-03 00:00:00, dtype: int64, ACME    17304300\n",
       " Name: 2006-10-04 00:00:00, dtype: int64, ACME    18946200\n",
       " Name: 2006-10-05 00:00:00, dtype: int64, ACME    19169400\n",
       " Name: 2006-10-06 00:00:00, dtype: int64, ACME    17444400\n",
       " Name: 2006-10-07 00:00:00, dtype: int64, ACME    19750500\n",
       " Name: 2006-10-08 00:00:00, dtype: int64, ACME    4497900\n",
       " Name: 2006-10-09 00:00:00, dtype: int64, ACME    7159500\n",
       " Name: 2006-10-10 00:00:00, dtype: int64, ACME    19379400\n",
       " Name: 2006-10-11 00:00:00, dtype: int64, ACME    7951500\n",
       " Name: 2006-10-12 00:00:00, dtype: int64, ACME    18979500\n",
       " Name: 2006-10-13 00:00:00, dtype: int64, ACME    16578000\n",
       " Name: 2006-10-14 00:00:00, dtype: int64, ACME    2387700\n",
       " Name: 2006-10-15 00:00:00, dtype: int64, ACME    7866000\n",
       " Name: 2006-10-16 00:00:00, dtype: int64, ACME    18607800\n",
       " Name: 2006-10-17 00:00:00, dtype: int64, ACME    8252400\n",
       " Name: 2006-10-18 00:00:00, dtype: int64, ACME    15888900\n",
       " Name: 2006-10-19 00:00:00, dtype: int64, ACME    17957400\n",
       " Name: 2006-10-20 00:00:00, dtype: int64, ACME    13127700\n",
       " Name: 2006-10-21 00:00:00, dtype: int64, ACME    16103100\n",
       " Name: 2006-10-22 00:00:00, dtype: int64, ACME    17731500\n",
       " Name: 2006-10-23 00:00:00, dtype: int64, ACME    13401900\n",
       " Name: 2006-10-24 00:00:00, dtype: int64, ACME    6371400\n",
       " Name: 2006-10-25 00:00:00, dtype: int64, ACME    15388200\n",
       " Name: 2006-10-26 00:00:00, dtype: int64, ACME    12622200\n",
       " Name: 2006-10-27 00:00:00, dtype: int64, ACME    16852800\n",
       " Name: 2006-10-28 00:00:00, dtype: int64, ACME    17004900\n",
       " Name: 2006-10-29 00:00:00, dtype: int64, ACME    9921900\n",
       " Name: 2006-10-30 00:00:00, dtype: int64, ACME    9683100\n",
       " Name: 2006-10-31 00:00:00, dtype: int64, ACME    15408000\n",
       " Name: 2006-11-01 00:00:00, dtype: int64, ACME    15811800\n",
       " Name: 2006-11-02 00:00:00, dtype: int64, ACME    10533000\n",
       " Name: 2006-11-03 00:00:00, dtype: int64, ACME    11590500\n",
       " Name: 2006-11-04 00:00:00, dtype: int64, ACME    3276600\n",
       " Name: 2006-11-05 00:00:00, dtype: int64, ACME    7461300\n",
       " Name: 2006-11-06 00:00:00, dtype: int64, ACME    14222100\n",
       " Name: 2006-11-07 00:00:00, dtype: int64, ACME    15001800\n",
       " Name: 2006-11-08 00:00:00, dtype: int64, ACME    14462700\n",
       " Name: 2006-11-09 00:00:00, dtype: int64, ACME    10932900\n",
       " Name: 2006-11-10 00:00:00, dtype: int64, ACME    15290700\n",
       " Name: 2006-11-11 00:00:00, dtype: int64, ACME    12027600\n",
       " Name: 2006-11-12 00:00:00, dtype: int64, ACME    13481700\n",
       " Name: 2006-11-13 00:00:00, dtype: int64, ACME    10948800\n",
       " Name: 2006-11-14 00:00:00, dtype: int64, ACME    5500800\n",
       " Name: 2006-11-15 00:00:00, dtype: int64, ACME    14371500\n",
       " Name: 2006-11-16 00:00:00, dtype: int64, ACME    14166900\n",
       " Name: 2006-11-17 00:00:00, dtype: int64, ACME    13881900\n",
       " Name: 2006-11-18 00:00:00, dtype: int64, ACME    13974300\n",
       " Name: 2006-11-19 00:00:00, dtype: int64, ACME    14172900\n",
       " Name: 2006-11-20 00:00:00, dtype: int64, ACME    13446600\n",
       " Name: 2006-11-21 00:00:00, dtype: int64, ACME    13544700\n",
       " Name: 2006-11-22 00:00:00, dtype: int64, ACME    13427400\n",
       " Name: 2006-11-23 00:00:00, dtype: int64, ACME    11554800\n",
       " Name: 2006-11-24 00:00:00, dtype: int64, ACME    10320300\n",
       " Name: 2006-11-25 00:00:00, dtype: int64, ACME    7045800\n",
       " Name: 2006-11-26 00:00:00, dtype: int64, ACME    5149200\n",
       " Name: 2006-11-27 00:00:00, dtype: int64, ACME    10584300\n",
       " Name: 2006-11-28 00:00:00, dtype: int64, ACME    1093200\n",
       " Name: 2006-11-29 00:00:00, dtype: int64, ACME    2410200\n",
       " Name: 2006-11-30 00:00:00, dtype: int64, ACME    13180200\n",
       " Name: 2006-12-01 00:00:00, dtype: int64, ACME    12717300\n",
       " Name: 2006-12-02 00:00:00, dtype: int64, ACME    13010400\n",
       " Name: 2006-12-03 00:00:00, dtype: int64, ACME    12999900\n",
       " Name: 2006-12-04 00:00:00, dtype: int64, ACME    12810900\n",
       " Name: 2006-12-05 00:00:00, dtype: int64, ACME    12364500\n",
       " Name: 2006-12-06 00:00:00, dtype: int64, ACME    12475800\n",
       " Name: 2006-12-07 00:00:00, dtype: int64, ACME    12586200\n",
       " Name: 2006-12-08 00:00:00, dtype: int64, ACME    9589800\n",
       " Name: 2006-12-09 00:00:00, dtype: int64, ACME    10530600\n",
       " Name: 2006-12-10 00:00:00, dtype: int64, ACME    11386200\n",
       " Name: 2006-12-11 00:00:00, dtype: int64, ACME    12036600\n",
       " Name: 2006-12-12 00:00:00, dtype: int64, ACME    11645400\n",
       " Name: 2006-12-13 00:00:00, dtype: int64, ACME    10968900\n",
       " Name: 2006-12-14 00:00:00, dtype: int64, ACME    11333400\n",
       " Name: 2006-12-15 00:00:00, dtype: int64, ACME    11743500\n",
       " Name: 2006-12-16 00:00:00, dtype: int64, ACME    11288700\n",
       " Name: 2006-12-17 00:00:00, dtype: int64, ACME    3126600\n",
       " Name: 2006-12-18 00:00:00, dtype: int64, ACME    906000\n",
       " Name: 2006-12-19 00:00:00, dtype: int64, ACME    6975900\n",
       " Name: 2006-12-20 00:00:00, dtype: int64, ACME    10076700\n",
       " Name: 2006-12-21 00:00:00, dtype: int64, ACME    12107400\n",
       " Name: 2006-12-22 00:00:00, dtype: int64, ACME    11287500\n",
       " Name: 2006-12-23 00:00:00, dtype: int64, ACME    4458600\n",
       " Name: 2006-12-24 00:00:00, dtype: int64, ACME    12486900\n",
       " Name: 2006-12-25 00:00:00, dtype: int64, ACME    11932500\n",
       " Name: 2006-12-26 00:00:00, dtype: int64, ACME    10188600\n",
       " Name: 2006-12-27 00:00:00, dtype: int64, ACME    3411900\n",
       " Name: 2006-12-28 00:00:00, dtype: int64, ACME    1299600\n",
       " Name: 2006-12-29 00:00:00, dtype: int64, ACME    9032400\n",
       " Name: 2006-12-30 00:00:00, dtype: int64, ACME    6495600\n",
       " Name: 2006-12-31 00:00:00, dtype: int64, ACME    12930000\n",
       " Name: 2007-01-01 00:00:00, dtype: int64, ACME    11347500\n",
       " Name: 2007-01-02 00:00:00, dtype: int64, ACME    8400900\n",
       " Name: 2007-01-03 00:00:00, dtype: int64, ACME    3675300\n",
       " Name: 2007-01-04 00:00:00, dtype: int64, ACME    6149400\n",
       " Name: 2007-01-05 00:00:00, dtype: int64, ACME    6622800\n",
       " Name: 2007-01-06 00:00:00, dtype: int64, ACME    12842700\n",
       " Name: 2007-01-07 00:00:00, dtype: int64, ACME    12534000\n",
       " Name: 2007-01-08 00:00:00, dtype: int64, ACME    12640200\n",
       " Name: 2007-01-09 00:00:00, dtype: int64, ACME    12029700\n",
       " Name: 2007-01-10 00:00:00, dtype: int64, ACME    8218200\n",
       " Name: 2007-01-11 00:00:00, dtype: int64, ACME    897600\n",
       " Name: 2007-01-12 00:00:00, dtype: int64, ACME    1736700\n",
       " Name: 2007-01-13 00:00:00, dtype: int64, ACME    1829400\n",
       " Name: 2007-01-14 00:00:00, dtype: int64, ACME    14730600\n",
       " Name: 2007-01-15 00:00:00, dtype: int64, ACME    4659300\n",
       " Name: 2007-01-16 00:00:00, dtype: int64, ACME    5227500\n",
       " Name: 2007-01-17 00:00:00, dtype: int64, ACME    7802400\n",
       " Name: 2007-01-18 00:00:00, dtype: int64, ACME    4716300\n",
       " Name: 2007-01-19 00:00:00, dtype: int64, ACME    1436400\n",
       " Name: 2007-01-20 00:00:00, dtype: int64, ACME    7198800\n",
       " Name: 2007-01-21 00:00:00, dtype: int64, ACME    10884600\n",
       " Name: 2007-01-22 00:00:00, dtype: int64, ACME    14005200\n",
       " Name: 2007-01-23 00:00:00, dtype: int64, ACME    10362000\n",
       " Name: 2007-01-24 00:00:00, dtype: int64, ACME    14250300\n",
       " Name: 2007-01-25 00:00:00, dtype: int64, ACME    14436900\n",
       " Name: 2007-01-26 00:00:00, dtype: int64, ACME    4709100\n",
       " Name: 2007-01-27 00:00:00, dtype: int64, ACME    14906100\n",
       " Name: 2007-01-28 00:00:00, dtype: int64, ACME    7393500\n",
       " Name: 2007-01-29 00:00:00, dtype: int64, ACME    14862000\n",
       " Name: 2007-01-30 00:00:00, dtype: int64, ACME    1902600\n",
       " Name: 2007-01-31 00:00:00, dtype: int64, ACME    2938800\n",
       " Name: 2007-02-01 00:00:00, dtype: int64, ACME    15353400\n",
       " Name: 2007-02-02 00:00:00, dtype: int64, ACME    15255000\n",
       " Name: 2007-02-03 00:00:00, dtype: int64, ACME    15771600\n",
       " Name: 2007-02-04 00:00:00, dtype: int64, ACME    15444900\n",
       " Name: 2007-02-05 00:00:00, dtype: int64, ACME    15477000\n",
       " Name: 2007-02-06 00:00:00, dtype: int64, ACME    12000\n",
       " Name: 2007-02-07 00:00:00, dtype: int64, ACME    1313700\n",
       " Name: 2007-02-08 00:00:00, dtype: int64, ACME    3243900\n",
       " Name: 2007-02-09 00:00:00, dtype: int64, ACME    4035900\n",
       " Name: 2007-02-10 00:00:00, dtype: int64, ACME    3287400\n",
       " Name: 2007-02-11 00:00:00, dtype: int64, ACME    2799300\n",
       " Name: 2007-02-12 00:00:00, dtype: int64, ACME    4298700\n",
       " Name: 2007-02-13 00:00:00, dtype: int64, ACME    7221000\n",
       " Name: 2007-02-14 00:00:00, dtype: int64, ACME    15655500\n",
       " Name: 2007-02-15 00:00:00, dtype: int64, ACME    17345700\n",
       " Name: 2007-02-16 00:00:00, dtype: int64, ACME    17731500\n",
       " Name: 2007-02-17 00:00:00, dtype: int64, ACME    17870100\n",
       " Name: 2007-02-18 00:00:00, dtype: int64, ACME    17862000\n",
       " Name: 2007-02-19 00:00:00, dtype: int64, ACME    18236700\n",
       " Name: 2007-02-20 00:00:00, dtype: int64, ACME    18657600\n",
       " Name: 2007-02-21 00:00:00, dtype: int64, ACME    18039900\n",
       " Name: 2007-02-22 00:00:00, dtype: int64, ACME    7938600\n",
       " Name: 2007-02-23 00:00:00, dtype: int64, ACME    14849700\n",
       " Name: 2007-02-24 00:00:00, dtype: int64, ACME    19800000\n",
       " Name: 2007-02-25 00:00:00, dtype: int64, ACME    19771500\n",
       " Name: 2007-02-26 00:00:00, dtype: int64, ACME    19243500\n",
       " Name: 2007-02-27 00:00:00, dtype: int64, ACME    16930200\n",
       " Name: 2007-02-28 00:00:00, dtype: int64, ACME    20005800\n",
       " Name: 2007-03-01 00:00:00, dtype: int64, ACME    16619100\n",
       " Name: 2007-03-02 00:00:00, dtype: int64, ACME    20647200\n",
       " Name: 2007-03-03 00:00:00, dtype: int64, ACME    21316200\n",
       " Name: 2007-03-04 00:00:00, dtype: int64, ACME    21204900\n",
       " Name: 2007-03-05 00:00:00, dtype: int64, ACME    18894900\n",
       " Name: 2007-03-06 00:00:00, dtype: int64, ACME    20136600\n",
       " Name: 2007-03-07 00:00:00, dtype: int64, ACME    19968000\n",
       " Name: 2007-03-08 00:00:00, dtype: int64, ACME    19851300\n",
       " Name: 2007-03-09 00:00:00, dtype: int64, ACME    20361300\n",
       " Name: 2007-03-10 00:00:00, dtype: int64, ACME    3537600\n",
       " Name: 2007-03-11 00:00:00, dtype: int64, ACME    15045900\n",
       " Name: 2007-03-12 00:00:00, dtype: int64, ACME    16830900\n",
       " Name: 2007-03-13 00:00:00, dtype: int64, ACME    16044600\n",
       " Name: 2007-03-14 00:00:00, dtype: int64, ACME    19200900\n",
       " Name: 2007-03-15 00:00:00, dtype: int64, ACME    22450500\n",
       " Name: 2007-03-16 00:00:00, dtype: int64, ACME    22054500\n",
       " Name: 2007-03-17 00:00:00, dtype: int64, ACME    20571600\n",
       " Name: 2007-03-18 00:00:00, dtype: int64, ACME    11404200\n",
       " Name: 2007-03-19 00:00:00, dtype: int64, ACME    10730400\n",
       " Name: 2007-03-20 00:00:00, dtype: int64, ACME    18736800\n",
       " Name: 2007-03-21 00:00:00, dtype: int64, ACME    12054900\n",
       " Name: 2007-03-22 00:00:00, dtype: int64, ACME    11178000\n",
       " Name: 2007-03-23 00:00:00, dtype: int64, ACME    13825200\n",
       " Name: 2007-03-24 00:00:00, dtype: int64, ACME    9008400\n",
       " Name: 2007-03-25 00:00:00, dtype: int64, ACME    6233100\n",
       " Name: 2007-03-26 00:00:00, dtype: int64, ACME    11845800\n",
       " Name: 2007-03-27 00:00:00, dtype: int64, ACME    15081600\n",
       " Name: 2007-03-28 00:00:00, dtype: int64, ACME    2786700\n",
       " Name: 2007-03-29 00:00:00, dtype: int64, ACME    1788300\n",
       " Name: 2007-03-30 00:00:00, dtype: int64, ACME    24624900\n",
       " Name: 2007-03-31 00:00:00, dtype: int64, ACME    25132500\n",
       " Name: 2007-04-01 00:00:00, dtype: int64, ACME    24104100\n",
       " Name: 2007-04-02 00:00:00, dtype: int64, ACME    24154800\n",
       " Name: 2007-04-03 00:00:00, dtype: int64, ACME    24428400\n",
       " Name: 2007-04-04 00:00:00, dtype: int64, ACME    7841400\n",
       " Name: 2007-04-05 00:00:00, dtype: int64, ACME    5943900\n",
       " Name: 2007-04-06 00:00:00, dtype: int64, ACME    20200800\n",
       " Name: 2007-04-07 00:00:00, dtype: int64, ACME    10136700\n",
       " Name: 2007-04-08 00:00:00, dtype: int64, ACME    7181400\n",
       " Name: 2007-04-09 00:00:00, dtype: int64, ACME    11094000\n",
       " Name: 2007-04-10 00:00:00, dtype: int64, ACME    27009300\n",
       " Name: 2007-04-11 00:00:00, dtype: int64, ACME    19342200\n",
       " Name: 2007-04-12 00:00:00, dtype: int64, ACME    1811700\n",
       " Name: 2007-04-13 00:00:00, dtype: int64, ACME    10136400\n",
       " Name: 2007-04-14 00:00:00, dtype: int64, ACME    27688800\n",
       " Name: 2007-04-15 00:00:00, dtype: int64, ACME    25998000\n",
       " Name: 2007-04-16 00:00:00, dtype: int64, ACME    4230900\n",
       " Name: 2007-04-17 00:00:00, dtype: int64, ACME    19225500\n",
       " Name: 2007-04-18 00:00:00, dtype: int64, ACME    22419300\n",
       " Name: 2007-04-19 00:00:00, dtype: int64, ACME    26192100\n",
       " Name: 2007-04-20 00:00:00, dtype: int64, ACME    26698800\n",
       " Name: 2007-04-21 00:00:00, dtype: int64, ACME    26062200\n",
       " Name: 2007-04-22 00:00:00, dtype: int64, ACME    10311900\n",
       " Name: 2007-04-23 00:00:00, dtype: int64, ACME    10245900\n",
       " Name: 2007-04-24 00:00:00, dtype: int64, ACME    11333100\n",
       " Name: 2007-04-25 00:00:00, dtype: int64, ACME    26386500\n",
       " Name: 2007-04-26 00:00:00, dtype: int64, ACME    17673300\n",
       " Name: 2007-04-27 00:00:00, dtype: int64, ACME    28121400\n",
       " Name: 2007-04-28 00:00:00, dtype: int64, ACME    26566200\n",
       " Name: 2007-04-29 00:00:00, dtype: int64, ACME    14961900\n",
       " Name: 2007-04-30 00:00:00, dtype: int64, ACME    16417500\n",
       " Name: 2007-05-01 00:00:00, dtype: int64, ACME    19146000\n",
       " Name: 2007-05-02 00:00:00, dtype: int64, ACME    13559100\n",
       " Name: 2007-05-03 00:00:00, dtype: int64, ACME    22229700\n",
       " Name: 2007-05-04 00:00:00, dtype: int64, ACME    13672500\n",
       " Name: 2007-05-05 00:00:00, dtype: int64, ACME    14584800\n",
       " Name: 2007-05-06 00:00:00, dtype: int64, ACME    8337600\n",
       " Name: 2007-05-07 00:00:00, dtype: int64, ACME    20109300\n",
       " Name: 2007-05-08 00:00:00, dtype: int64, ACME    17841300\n",
       " Name: 2007-05-09 00:00:00, dtype: int64, ACME    18640800\n",
       " Name: 2007-05-10 00:00:00, dtype: int64, ACME    24267900\n",
       " Name: 2007-05-11 00:00:00, dtype: int64, ACME    27397200\n",
       " Name: 2007-05-12 00:00:00, dtype: int64, ACME    26605500\n",
       " Name: 2007-05-13 00:00:00, dtype: int64, ACME    28194900\n",
       " Name: 2007-05-14 00:00:00, dtype: int64, ACME    10163400\n",
       " Name: 2007-05-15 00:00:00, dtype: int64, ACME    29221500\n",
       " Name: 2007-05-16 00:00:00, dtype: int64, ACME    28743300\n",
       " Name: 2007-05-17 00:00:00, dtype: int64, ACME    21492900\n",
       " Name: 2007-05-18 00:00:00, dtype: int64, ACME    9711900\n",
       " Name: 2007-05-19 00:00:00, dtype: int64, ACME    23759700\n",
       " Name: 2007-05-20 00:00:00, dtype: int64, ACME    8852400\n",
       " Name: 2007-05-21 00:00:00, dtype: int64, ACME    22581000\n",
       " Name: 2007-05-22 00:00:00, dtype: int64, ACME    17891700\n",
       " Name: 2007-05-23 00:00:00, dtype: int64, ACME    3911400\n",
       " Name: 2007-05-24 00:00:00, dtype: int64, ACME    12607800\n",
       " Name: 2007-05-25 00:00:00, dtype: int64, ACME    11083800\n",
       " Name: 2007-05-26 00:00:00, dtype: int64, ACME    19437000\n",
       " Name: 2007-05-27 00:00:00, dtype: int64, ACME    12629100\n",
       " Name: 2007-05-28 00:00:00, dtype: int64, ACME    18104100\n",
       " Name: 2007-05-29 00:00:00, dtype: int64, ACME    9981000\n",
       " Name: 2007-05-30 00:00:00, dtype: int64, ACME    18996000\n",
       " Name: 2007-05-31 00:00:00, dtype: int64, ACME    13550400\n",
       " Name: 2007-06-01 00:00:00, dtype: int64, ACME    25976400\n",
       " Name: 2007-06-02 00:00:00, dtype: int64, ACME    24817500\n",
       " Name: 2007-06-03 00:00:00, dtype: int64, ACME    24752400\n",
       " Name: 2007-06-04 00:00:00, dtype: int64, ACME    29532900\n",
       " Name: 2007-06-05 00:00:00, dtype: int64, ACME    27228600\n",
       " Name: 2007-06-06 00:00:00, dtype: int64, ACME    21281400\n",
       " Name: 2007-06-07 00:00:00, dtype: int64, ACME    28296000\n",
       " Name: 2007-06-08 00:00:00, dtype: int64, ACME    23708400\n",
       " Name: 2007-06-09 00:00:00, dtype: int64, ACME    17811900\n",
       " Name: 2007-06-10 00:00:00, dtype: int64, ACME    28219500\n",
       " Name: 2007-06-11 00:00:00, dtype: int64, ACME    23471400\n",
       " Name: 2007-06-12 00:00:00, dtype: int64, ACME    27177600\n",
       " Name: 2007-06-13 00:00:00, dtype: int64, ACME    20811600\n",
       " Name: 2007-06-14 00:00:00, dtype: int64, ACME    10274100\n",
       " Name: 2007-06-15 00:00:00, dtype: int64, ACME    13078800\n",
       " Name: 2007-06-16 00:00:00, dtype: int64, ACME    13148400\n",
       " Name: 2007-06-17 00:00:00, dtype: int64, ACME    15253500\n",
       " Name: 2007-06-18 00:00:00, dtype: int64, ACME    20461800\n",
       " Name: 2007-06-19 00:00:00, dtype: int64, ACME    11436900\n",
       " Name: 2007-06-20 00:00:00, dtype: int64, ACME    17902500\n",
       " Name: 2007-06-21 00:00:00, dtype: int64, ACME    22656900\n",
       " Name: 2007-06-22 00:00:00, dtype: int64, ACME    27939300\n",
       " Name: 2007-06-23 00:00:00, dtype: int64, ACME    26853600\n",
       " Name: 2007-06-24 00:00:00, dtype: int64, ACME    12886800\n",
       " Name: 2007-06-25 00:00:00, dtype: int64, ACME    7151700\n",
       " Name: 2007-06-26 00:00:00, dtype: int64, ACME    17602500\n",
       " Name: 2007-06-27 00:00:00, dtype: int64, ACME    8013900\n",
       " Name: 2007-06-28 00:00:00, dtype: int64, ACME    9632700\n",
       " Name: 2007-06-29 00:00:00, dtype: int64, ACME    16325700\n",
       " Name: 2007-06-30 00:00:00, dtype: int64, ACME    25364100\n",
       " Name: 2007-07-01 00:00:00, dtype: int64, ACME    14165700\n",
       " Name: 2007-07-02 00:00:00, dtype: int64, ACME    19059300\n",
       " Name: 2007-07-03 00:00:00, dtype: int64, ACME    25462500\n",
       " Name: 2007-07-04 00:00:00, dtype: int64, ACME    21273600\n",
       " Name: 2007-07-05 00:00:00, dtype: int64, ACME    24463200\n",
       " Name: 2007-07-06 00:00:00, dtype: int64, ACME    28192500\n",
       " Name: 2007-07-07 00:00:00, dtype: int64, ACME    29036100\n",
       " Name: 2007-07-08 00:00:00, dtype: int64, ACME    22339500\n",
       " Name: 2007-07-09 00:00:00, dtype: int64, ACME    26402400\n",
       " Name: 2007-07-10 00:00:00, dtype: int64, ACME    17883600\n",
       " Name: 2007-07-11 00:00:00, dtype: int64, ACME    15735300\n",
       " Name: 2007-07-12 00:00:00, dtype: int64, ACME    14091000\n",
       " Name: 2007-07-13 00:00:00, dtype: int64, ACME    28623000\n",
       " Name: 2007-07-14 00:00:00, dtype: int64, ACME    28872900\n",
       " Name: 2007-07-15 00:00:00, dtype: int64, ACME    27261600\n",
       " Name: 2007-07-16 00:00:00, dtype: int64, ACME    28549800\n",
       " Name: 2007-07-17 00:00:00, dtype: int64, ACME    28535700\n",
       " Name: 2007-07-18 00:00:00, dtype: int64, ACME    28054500\n",
       " Name: 2007-07-19 00:00:00, dtype: int64, ACME    26860500\n",
       " Name: 2007-07-20 00:00:00, dtype: int64, ACME    27216900\n",
       " Name: 2007-07-21 00:00:00, dtype: int64, ACME    26431500\n",
       " Name: 2007-07-22 00:00:00, dtype: int64, ACME    16317300\n",
       " Name: 2007-07-23 00:00:00, dtype: int64, ACME    24177000\n",
       " Name: 2007-07-24 00:00:00, dtype: int64, ACME    27974400\n",
       " Name: 2007-07-25 00:00:00, dtype: int64, ACME    28281000\n",
       " Name: 2007-07-26 00:00:00, dtype: int64, ACME    21758100\n",
       " Name: 2007-07-27 00:00:00, dtype: int64, ACME    23130900\n",
       " Name: 2007-07-28 00:00:00, dtype: int64, ACME    26864100\n",
       " Name: 2007-07-29 00:00:00, dtype: int64, ACME    18547800\n",
       " Name: 2007-07-30 00:00:00, dtype: int64, ACME    18106200\n",
       " Name: 2007-07-31 00:00:00, dtype: int64, ACME    15814200\n",
       " Name: 2007-08-01 00:00:00, dtype: int64, ACME    23751900\n",
       " Name: 2007-08-02 00:00:00, dtype: int64, ACME    24789600\n",
       " Name: 2007-08-03 00:00:00, dtype: int64, ACME    26923800\n",
       " Name: 2007-08-04 00:00:00, dtype: int64, ACME    26769300\n",
       " Name: 2007-08-05 00:00:00, dtype: int64, ACME    26904900\n",
       " Name: 2007-08-06 00:00:00, dtype: int64, ACME    27070500\n",
       " Name: 2007-08-07 00:00:00, dtype: int64, ACME    27408600\n",
       " Name: 2007-08-08 00:00:00, dtype: int64, ACME    27492600\n",
       " Name: 2007-08-09 00:00:00, dtype: int64, ACME    27625500\n",
       " Name: 2007-08-10 00:00:00, dtype: int64, ACME    27195600\n",
       " Name: 2007-08-11 00:00:00, dtype: int64, ACME    26607300\n",
       " Name: 2007-08-12 00:00:00, dtype: int64, ACME    26807700\n",
       " Name: 2007-08-13 00:00:00, dtype: int64, ACME    26628300\n",
       " Name: 2007-08-14 00:00:00, dtype: int64, ACME    25440300\n",
       " Name: 2007-08-15 00:00:00, dtype: int64, ACME    23733600\n",
       " Name: 2007-08-16 00:00:00, dtype: int64, ACME    9499200\n",
       " Name: 2007-08-17 00:00:00, dtype: int64, ACME    6394800\n",
       " Name: 2007-08-18 00:00:00, dtype: int64, ACME    17863500\n",
       " Name: 2007-08-19 00:00:00, dtype: int64, ACME    22615500\n",
       " Name: 2007-08-20 00:00:00, dtype: int64, ACME    25275900\n",
       " Name: 2007-08-21 00:00:00, dtype: int64, ACME    23827500\n",
       " Name: 2007-08-22 00:00:00, dtype: int64, ACME    23124600\n",
       " Name: 2007-08-23 00:00:00, dtype: int64, ACME    21593700\n",
       " Name: 2007-08-24 00:00:00, dtype: int64, ACME    20481000\n",
       " Name: 2007-08-25 00:00:00, dtype: int64, ACME    24926700\n",
       " Name: 2007-08-26 00:00:00, dtype: int64, ACME    24511800\n",
       " Name: 2007-08-27 00:00:00, dtype: int64, ACME    24652500\n",
       " Name: 2007-08-28 00:00:00, dtype: int64, ACME    25120800\n",
       " Name: 2007-08-29 00:00:00, dtype: int64, ACME    20384400\n",
       " Name: 2007-08-30 00:00:00, dtype: int64, ACME    23632800\n",
       " Name: 2007-08-31 00:00:00, dtype: int64, ACME    25548300\n",
       " Name: 2007-09-01 00:00:00, dtype: int64, ACME    23546700\n",
       " Name: 2007-09-02 00:00:00, dtype: int64, ACME    20415600\n",
       " Name: 2007-09-03 00:00:00, dtype: int64, ACME    10705500\n",
       " Name: 2007-09-04 00:00:00, dtype: int64, ACME    13799400\n",
       " Name: 2007-09-05 00:00:00, dtype: int64, ACME    18705300\n",
       " Name: 2007-09-06 00:00:00, dtype: int64, ACME    14013000\n",
       " Name: 2007-09-07 00:00:00, dtype: int64, ACME    16587300\n",
       " Name: 2007-09-08 00:00:00, dtype: int64, ACME    6273000\n",
       " Name: 2007-09-09 00:00:00, dtype: int64, ACME    14166900\n",
       " Name: 2007-09-10 00:00:00, dtype: int64, ACME    23337000\n",
       " Name: 2007-09-11 00:00:00, dtype: int64, ACME    23778900\n",
       " Name: 2007-09-12 00:00:00, dtype: int64, ACME    20597100\n",
       " Name: 2007-09-13 00:00:00, dtype: int64, ACME    19752900\n",
       " Name: 2007-09-14 00:00:00, dtype: int64, ACME    21148200\n",
       " Name: 2007-09-15 00:00:00, dtype: int64, ACME    21214500\n",
       " Name: 2007-09-16 00:00:00, dtype: int64, ACME    20509800\n",
       " Name: 2007-09-17 00:00:00, dtype: int64, ACME    14761500\n",
       " Name: 2007-09-18 00:00:00, dtype: int64, ACME    17644800\n",
       " Name: 2007-09-19 00:00:00, dtype: int64, ACME    19924500\n",
       " Name: 2007-09-20 00:00:00, dtype: int64, ACME    20318400\n",
       " Name: 2007-09-21 00:00:00, dtype: int64, ACME    20572500\n",
       " Name: 2007-09-22 00:00:00, dtype: int64, ACME    19059600\n",
       " Name: 2007-09-23 00:00:00, dtype: int64, ACME    20634000\n",
       " Name: 2007-09-24 00:00:00, dtype: int64, ACME    10575300\n",
       " Name: 2007-09-25 00:00:00, dtype: int64, ACME    16389000\n",
       " Name: 2007-09-26 00:00:00, dtype: int64, ACME    16456200\n",
       " Name: 2007-09-27 00:00:00, dtype: int64, ACME    12445500\n",
       " Name: 2007-09-28 00:00:00, dtype: int64, ACME    19931400\n",
       " Name: 2007-09-29 00:00:00, dtype: int64, ACME    15985500\n",
       " Name: 2007-09-30 00:00:00, dtype: int64, ACME    20361000\n",
       " Name: 2007-10-01 00:00:00, dtype: int64, ACME    18146400\n",
       " Name: 2007-10-02 00:00:00, dtype: int64, ACME    11484300\n",
       " Name: 2007-10-03 00:00:00, dtype: int64, ACME    19402500\n",
       " Name: 2007-10-04 00:00:00, dtype: int64, ACME    19308300\n",
       " Name: 2007-10-05 00:00:00, dtype: int64, ACME    11016300\n",
       " Name: 2007-10-06 00:00:00, dtype: int64, ACME    15738900\n",
       " Name: 2007-10-07 00:00:00, dtype: int64, ACME    13631700\n",
       " Name: 2007-10-08 00:00:00, dtype: int64, ACME    20458200\n",
       " Name: 2007-10-09 00:00:00, dtype: int64, ACME    19636500\n",
       " Name: 2007-10-10 00:00:00, dtype: int64, ACME    14518800\n",
       " Name: 2007-10-11 00:00:00, dtype: int64, ACME    15958200\n",
       " Name: 2007-10-12 00:00:00, dtype: int64, ACME    18261000\n",
       " Name: 2007-10-13 00:00:00, dtype: int64, ACME    7105200\n",
       " Name: 2007-10-14 00:00:00, dtype: int64, ACME    15973200\n",
       " Name: 2007-10-15 00:00:00, dtype: int64, ACME    17762400\n",
       " Name: 2007-10-16 00:00:00, dtype: int64, ACME    10536900\n",
       " Name: 2007-10-17 00:00:00, dtype: int64, ACME    18781200\n",
       " Name: 2007-10-18 00:00:00, dtype: int64, ACME    18704100\n",
       " Name: 2007-10-19 00:00:00, dtype: int64, ACME    17667600\n",
       " Name: 2007-10-20 00:00:00, dtype: int64, ACME    15186300\n",
       " Name: 2007-10-21 00:00:00, dtype: int64, ACME    5460300\n",
       " Name: 2007-10-22 00:00:00, dtype: int64, ACME    18089100\n",
       " Name: 2007-10-23 00:00:00, dtype: int64, ACME    17856600\n",
       " Name: 2007-10-24 00:00:00, dtype: int64, ACME    18075300\n",
       " Name: 2007-10-25 00:00:00, dtype: int64, ACME    17715000\n",
       " Name: 2007-10-26 00:00:00, dtype: int64, ACME    16880700\n",
       " Name: 2007-10-27 00:00:00, dtype: int64, ACME    16292100\n",
       " Name: 2007-10-28 00:00:00, dtype: int64, ACME    16017600\n",
       " Name: 2007-10-29 00:00:00, dtype: int64, ACME    15581400\n",
       " Name: 2007-10-30 00:00:00, dtype: int64, ACME    13204200\n",
       " Name: 2007-10-31 00:00:00, dtype: int64, ACME    16207800\n",
       " Name: 2007-11-01 00:00:00, dtype: int64, ACME    13685100\n",
       " Name: 2007-11-02 00:00:00, dtype: int64, ACME    14337300\n",
       " Name: 2007-11-03 00:00:00, dtype: int64, ACME    14938200\n",
       " Name: 2007-11-04 00:00:00, dtype: int64, ACME    14835600\n",
       " Name: 2007-11-05 00:00:00, dtype: int64, ACME    15307200\n",
       " Name: 2007-11-06 00:00:00, dtype: int64, ACME    14962800\n",
       " Name: 2007-11-07 00:00:00, dtype: int64, ACME    14866500\n",
       " Name: 2007-11-08 00:00:00, dtype: int64, ACME    10683000\n",
       " Name: 2007-11-09 00:00:00, dtype: int64, ACME    11815800\n",
       " Name: 2007-11-10 00:00:00, dtype: int64, ACME    10580100\n",
       " Name: 2007-11-11 00:00:00, dtype: int64, ACME    11314200\n",
       " Name: 2007-11-12 00:00:00, dtype: int64, ACME    13910400\n",
       " Name: 2007-11-13 00:00:00, dtype: int64, ACME    13110000\n",
       " Name: 2007-11-14 00:00:00, dtype: int64, ACME    15180600\n",
       " Name: 2007-11-15 00:00:00, dtype: int64, ACME    13290600\n",
       " Name: 2007-11-16 00:00:00, dtype: int64, ACME    14392200\n",
       " Name: 2007-11-17 00:00:00, dtype: int64, ACME    11895600\n",
       " Name: 2007-11-18 00:00:00, dtype: int64, ACME    7975500\n",
       " Name: 2007-11-19 00:00:00, dtype: int64, ACME    10401000\n",
       " Name: 2007-11-20 00:00:00, dtype: int64, ACME    6557400\n",
       " Name: 2007-11-21 00:00:00, dtype: int64, ACME    8102100\n",
       " Name: 2007-11-22 00:00:00, dtype: int64, ACME    8457900\n",
       " Name: 2007-11-23 00:00:00, dtype: int64, ACME    2641500\n",
       " Name: 2007-11-24 00:00:00, dtype: int64, ACME    3871500\n",
       " Name: 2007-11-25 00:00:00, dtype: int64, ACME    13197900\n",
       " Name: 2007-11-26 00:00:00, dtype: int64, ACME    13407900\n",
       " Name: 2007-11-27 00:00:00, dtype: int64, ACME    12295500\n",
       " Name: 2007-11-28 00:00:00, dtype: int64, ACME    13100100\n",
       " Name: 2007-11-29 00:00:00, dtype: int64, ACME    2985900\n",
       " Name: 2007-11-30 00:00:00, dtype: int64, ACME    1444800\n",
       " Name: 2007-12-01 00:00:00, dtype: int64, ACME    12177000\n",
       " Name: 2007-12-02 00:00:00, dtype: int64, ACME    13453800\n",
       " Name: 2007-12-03 00:00:00, dtype: int64, ACME    12525900\n",
       " Name: 2007-12-04 00:00:00, dtype: int64, ACME    10543500\n",
       " Name: 2007-12-05 00:00:00, dtype: int64, ACME    6214500\n",
       " Name: 2007-12-06 00:00:00, dtype: int64, ACME    8417400\n",
       " Name: 2007-12-07 00:00:00, dtype: int64, ACME    1683900\n",
       " Name: 2007-12-08 00:00:00, dtype: int64, ACME    1512300\n",
       " Name: 2007-12-09 00:00:00, dtype: int64, ACME    1729500\n",
       " Name: 2007-12-10 00:00:00, dtype: int64, ACME    1512600\n",
       " Name: 2007-12-11 00:00:00, dtype: int64, ACME    2126100\n",
       " Name: 2007-12-12 00:00:00, dtype: int64, ACME    3476400\n",
       " Name: 2007-12-13 00:00:00, dtype: int64, ACME    1262700\n",
       " Name: 2007-12-14 00:00:00, dtype: int64, ACME    3074400\n",
       " Name: 2007-12-15 00:00:00, dtype: int64, ACME    12523800\n",
       " Name: 2007-12-16 00:00:00, dtype: int64, ACME    12089700\n",
       " Name: 2007-12-17 00:00:00, dtype: int64, ACME    12300600\n",
       " Name: 2007-12-18 00:00:00, dtype: int64, ACME    12206100\n",
       " Name: 2007-12-19 00:00:00, dtype: int64, ACME    12275700\n",
       " Name: 2007-12-20 00:00:00, dtype: int64, ACME    9737700\n",
       " Name: 2007-12-21 00:00:00, dtype: int64, ACME    1958400\n",
       " Name: 2007-12-22 00:00:00, dtype: int64, ACME    12325200\n",
       " Name: 2007-12-23 00:00:00, dtype: int64, ACME    12225900\n",
       " Name: 2007-12-24 00:00:00, dtype: int64, ACME    12256500\n",
       " Name: 2007-12-25 00:00:00, dtype: int64, ACME    1851300\n",
       " Name: 2007-12-26 00:00:00, dtype: int64, ACME    1408500\n",
       " Name: 2007-12-27 00:00:00, dtype: int64, ACME    10060800\n",
       " Name: 2007-12-28 00:00:00, dtype: int64, ACME    11388000\n",
       " Name: 2007-12-29 00:00:00, dtype: int64, ACME    12441000\n",
       " Name: 2007-12-30 00:00:00, dtype: int64, ACME    12450300\n",
       " Name: 2007-12-31 00:00:00, dtype: int64]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp\n",
       "2005-11-25     7437900\n",
       "2005-11-26     8199600\n",
       "2005-11-27    10694400\n",
       "2005-11-28     8704800\n",
       "2005-11-29    12803100\n",
       "2005-11-30    12027300\n",
       "2005-12-01    12519000\n",
       "2005-12-02    11570100\n",
       "2005-12-03     9417300\n",
       "2005-12-04     9941700\n",
       "2005-12-05    11968200\n",
       "2005-12-06    12077400\n",
       "2005-12-07     4561500\n",
       "2005-12-08    13197300\n",
       "2005-12-09    12252600\n",
       "2005-12-10    11796900\n",
       "2005-12-11    11721300\n",
       "2005-12-12     7685400\n",
       "2005-12-13     5451600\n",
       "2005-12-14     9810900\n",
       "2005-12-15    12423600\n",
       "2005-12-16    11590800\n",
       "2005-12-17     1609800\n",
       "2005-12-18     3166500\n",
       "2005-12-19     3104700\n",
       "2005-12-20     2655300\n",
       "2005-12-21     6268200\n",
       "2005-12-22    11357100\n",
       "2005-12-23    10005900\n",
       "2005-12-24    11276400\n",
       "                ...   \n",
       "2007-12-02    12177000\n",
       "2007-12-03    13453800\n",
       "2007-12-04    12525900\n",
       "2007-12-05    10543500\n",
       "2007-12-06     6214500\n",
       "2007-12-07     8417400\n",
       "2007-12-08     1683900\n",
       "2007-12-09     1512300\n",
       "2007-12-10     1729500\n",
       "2007-12-11     1512600\n",
       "2007-12-12     2126100\n",
       "2007-12-13     3476400\n",
       "2007-12-14     1262700\n",
       "2007-12-15     3074400\n",
       "2007-12-16    12523800\n",
       "2007-12-17    12089700\n",
       "2007-12-18    12300600\n",
       "2007-12-19    12206100\n",
       "2007-12-20    12275700\n",
       "2007-12-21     9737700\n",
       "2007-12-22     1958400\n",
       "2007-12-23    12325200\n",
       "2007-12-24    12225900\n",
       "2007-12-25    12256500\n",
       "2007-12-26     1851300\n",
       "2007-12-27     1408500\n",
       "2007-12-28    10060800\n",
       "2007-12-29    11388000\n",
       "2007-12-30    12441000\n",
       "2007-12-31    12450300\n",
       "Freq: D, Name: ACME, Length: 767, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped['ACME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-01</th>\n",
       "      <td>7992482.00</td>\n",
       "      <td>12758486.0</td>\n",
       "      <td>10616136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-02</th>\n",
       "      <td>7710491.50</td>\n",
       "      <td>13182622.0</td>\n",
       "      <td>10567171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-03</th>\n",
       "      <td>10429879.00</td>\n",
       "      <td>14325090.0</td>\n",
       "      <td>11902439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-04</th>\n",
       "      <td>11223589.00</td>\n",
       "      <td>14775977.0</td>\n",
       "      <td>12748810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-05</th>\n",
       "      <td>10658310.00</td>\n",
       "      <td>14067459.0</td>\n",
       "      <td>12614409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-06</th>\n",
       "      <td>10009965.00</td>\n",
       "      <td>14516078.0</td>\n",
       "      <td>12380910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-07</th>\n",
       "      <td>10131425.00</td>\n",
       "      <td>14929840.0</td>\n",
       "      <td>12766399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-08</th>\n",
       "      <td>10399952.00</td>\n",
       "      <td>15407408.0</td>\n",
       "      <td>12761416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-09</th>\n",
       "      <td>9797899.00</td>\n",
       "      <td>16281284.0</td>\n",
       "      <td>12741510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-10</th>\n",
       "      <td>7658372.50</td>\n",
       "      <td>15727274.0</td>\n",
       "      <td>11613947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-11</th>\n",
       "      <td>4169193.00</td>\n",
       "      <td>15462921.0</td>\n",
       "      <td>10432786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-12</th>\n",
       "      <td>4027824.25</td>\n",
       "      <td>14276258.0</td>\n",
       "      <td>10680259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-13</th>\n",
       "      <td>6238357.50</td>\n",
       "      <td>14154149.0</td>\n",
       "      <td>10723148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-14</th>\n",
       "      <td>8202210.00</td>\n",
       "      <td>14535557.0</td>\n",
       "      <td>11724866.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-15</th>\n",
       "      <td>8405796.00</td>\n",
       "      <td>14664908.0</td>\n",
       "      <td>12139352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-16</th>\n",
       "      <td>9201276.00</td>\n",
       "      <td>14872377.0</td>\n",
       "      <td>12237092.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-17</th>\n",
       "      <td>10060676.00</td>\n",
       "      <td>14336385.0</td>\n",
       "      <td>12697624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-18</th>\n",
       "      <td>10789315.00</td>\n",
       "      <td>14815479.0</td>\n",
       "      <td>13099910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-19</th>\n",
       "      <td>12443511.00</td>\n",
       "      <td>15606749.0</td>\n",
       "      <td>13760604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-20</th>\n",
       "      <td>12222970.00</td>\n",
       "      <td>15158571.0</td>\n",
       "      <td>13968173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-21</th>\n",
       "      <td>12237291.00</td>\n",
       "      <td>15437742.0</td>\n",
       "      <td>13789151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-22</th>\n",
       "      <td>11182225.00</td>\n",
       "      <td>14818157.0</td>\n",
       "      <td>13163281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-23</th>\n",
       "      <td>11665162.00</td>\n",
       "      <td>16328840.0</td>\n",
       "      <td>13741965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-24</th>\n",
       "      <td>11974311.00</td>\n",
       "      <td>16799736.0</td>\n",
       "      <td>14213721.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-25</th>\n",
       "      <td>11870294.00</td>\n",
       "      <td>17060644.0</td>\n",
       "      <td>14653199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-26</th>\n",
       "      <td>10531753.00</td>\n",
       "      <td>15227269.0</td>\n",
       "      <td>12621994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-27</th>\n",
       "      <td>8112679.00</td>\n",
       "      <td>14539131.0</td>\n",
       "      <td>11365428.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-28</th>\n",
       "      <td>6643041.00</td>\n",
       "      <td>14888831.0</td>\n",
       "      <td>11244104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-29</th>\n",
       "      <td>6938157.50</td>\n",
       "      <td>15124530.0</td>\n",
       "      <td>11698381.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-30</th>\n",
       "      <td>8908484.00</td>\n",
       "      <td>15333002.0</td>\n",
       "      <td>11736959.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0.1         0.9         0.5\n",
       "2008-01-01   7992482.00  12758486.0  10616136.0\n",
       "2008-01-02   7710491.50  13182622.0  10567171.0\n",
       "2008-01-03  10429879.00  14325090.0  11902439.0\n",
       "2008-01-04  11223589.00  14775977.0  12748810.0\n",
       "2008-01-05  10658310.00  14067459.0  12614409.0\n",
       "2008-01-06  10009965.00  14516078.0  12380910.0\n",
       "2008-01-07  10131425.00  14929840.0  12766399.0\n",
       "2008-01-08  10399952.00  15407408.0  12761416.0\n",
       "2008-01-09   9797899.00  16281284.0  12741510.0\n",
       "2008-01-10   7658372.50  15727274.0  11613947.0\n",
       "2008-01-11   4169193.00  15462921.0  10432786.0\n",
       "2008-01-12   4027824.25  14276258.0  10680259.0\n",
       "2008-01-13   6238357.50  14154149.0  10723148.0\n",
       "2008-01-14   8202210.00  14535557.0  11724866.0\n",
       "2008-01-15   8405796.00  14664908.0  12139352.0\n",
       "2008-01-16   9201276.00  14872377.0  12237092.0\n",
       "2008-01-17  10060676.00  14336385.0  12697624.0\n",
       "2008-01-18  10789315.00  14815479.0  13099910.0\n",
       "2008-01-19  12443511.00  15606749.0  13760604.0\n",
       "2008-01-20  12222970.00  15158571.0  13968173.0\n",
       "2008-01-21  12237291.00  15437742.0  13789151.0\n",
       "2008-01-22  11182225.00  14818157.0  13163281.0\n",
       "2008-01-23  11665162.00  16328840.0  13741965.0\n",
       "2008-01-24  11974311.00  16799736.0  14213721.0\n",
       "2008-01-25  11870294.00  17060644.0  14653199.0\n",
       "2008-01-26  10531753.00  15227269.0  12621994.0\n",
       "2008-01-27   8112679.00  14539131.0  11365428.0\n",
       "2008-01-28   6643041.00  14888831.0  11244104.0\n",
       "2008-01-29   6938157.50  15124530.0  11698381.0\n",
       "2008-01-30   8908484.00  15333002.0  11736959.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-11-25</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-26</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-27</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-28</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-29</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2005-11-25   7437900\n",
       "2005-11-26   8199600\n",
       "2005-11-27  10694400\n",
       "2005-11-28   8704800\n",
       "2005-11-29  12803100"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp\n",
       "2005-11-25     7437900\n",
       "2005-11-26     8199600\n",
       "2005-11-27    10694400\n",
       "2005-11-28     8704800\n",
       "2005-11-29    12803100\n",
       "Freq: D, Name: ACME, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped['ACME'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-01</th>\n",
       "      <td>7992482.0</td>\n",
       "      <td>12758486.0</td>\n",
       "      <td>10616136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-02</th>\n",
       "      <td>7710491.5</td>\n",
       "      <td>13182622.0</td>\n",
       "      <td>10567171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-03</th>\n",
       "      <td>10429879.0</td>\n",
       "      <td>14325090.0</td>\n",
       "      <td>11902439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-04</th>\n",
       "      <td>11223589.0</td>\n",
       "      <td>14775977.0</td>\n",
       "      <td>12748810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-05</th>\n",
       "      <td>10658310.0</td>\n",
       "      <td>14067459.0</td>\n",
       "      <td>12614409.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0.1         0.9         0.5\n",
       "2008-01-01   7992482.0  12758486.0  10616136.0\n",
       "2008-01-02   7710491.5  13182622.0  10567171.0\n",
       "2008-01-03  10429879.0  14325090.0  11902439.0\n",
       "2008-01-04  11223589.0  14775977.0  12748810.0\n",
       "2008-01-05  10658310.0  14067459.0  12614409.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_training = pd.Timestamp(\"2014-09-01 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-12-27</th>\n",
       "      <td>1408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-28</th>\n",
       "      <td>10060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-29</th>\n",
       "      <td>11388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-30</th>\n",
       "      <td>12441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-31</th>\n",
       "      <td>12450300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2007-12-27   1408500\n",
       "2007-12-28  10060800\n",
       "2007-12-29  11388000\n",
       "2007-12-30  12441000\n",
       "2007-12-31  12450300"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-11-25</th>\n",
       "      <td>7437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-26</th>\n",
       "      <td>8199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-27</th>\n",
       "      <td>10694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-28</th>\n",
       "      <td>8704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-11-29</th>\n",
       "      <td>12803100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACME\n",
       "Timestamp           \n",
       "2005-11-25   7437900\n",
       "2005-11-26   8199600\n",
       "2005-11-27  10694400\n",
       "2005-11-28   8704800\n",
       "2005-11-29  12803100"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_training = pd.Timestamp(\"2005-11-24 00:00:00\", freq='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling served model to generate predictions starting from 2008-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:19: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:46: FutureWarning: Creating a DatetimeIndex by passing range endpoints is deprecated.  Use `pandas.date_range` instead.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:48: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(predictor, target_ts=timestamped['ACME'], forecast_date=pd.Timestamp('2008-01-01', freq='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-01</th>\n",
       "      <td>7992482.0</td>\n",
       "      <td>12758486.0</td>\n",
       "      <td>10616136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-02</th>\n",
       "      <td>7710491.5</td>\n",
       "      <td>13182622.0</td>\n",
       "      <td>10567171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-03</th>\n",
       "      <td>10429879.0</td>\n",
       "      <td>14325090.0</td>\n",
       "      <td>11902439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-04</th>\n",
       "      <td>11223589.0</td>\n",
       "      <td>14775977.0</td>\n",
       "      <td>12748810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-05</th>\n",
       "      <td>10658310.0</td>\n",
       "      <td>14067459.0</td>\n",
       "      <td>12614409.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0.1         0.9         0.5\n",
       "2008-01-01   7992482.0  12758486.0  10616136.0\n",
       "2008-01-02   7710491.5  13182622.0  10567171.0\n",
       "2008-01-03  10429879.0  14325090.0  11902439.0\n",
       "2008-01-04  11223589.0  14775977.0  12748810.0\n",
       "2008-01-05  10658310.0  14067459.0  12614409.0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = get_split(df, 'd', split_type = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '1994-01-01 00:00:00',\n",
       "  'target': [17412600,\n",
       "   17187300,\n",
       "   16946400,\n",
       "   16883100,\n",
       "   16567500,\n",
       "   5341500,\n",
       "   16074600,\n",
       "   15863400,\n",
       "   11075700,\n",
       "   15723600,\n",
       "   13943700,\n",
       "   6248100,\n",
       "   4716600,\n",
       "   5364900,\n",
       "   8388300,\n",
       "   7826100,\n",
       "   2207100,\n",
       "   5768100,\n",
       "   6405600,\n",
       "   2123100,\n",
       "   3197700,\n",
       "   4359000,\n",
       "   7564500,\n",
       "   6014400,\n",
       "   1881300,\n",
       "   2334900,\n",
       "   11250900,\n",
       "   13417800,\n",
       "   2506200,\n",
       "   8445300,\n",
       "   13469700,\n",
       "   13712400,\n",
       "   13110300,\n",
       "   8187900,\n",
       "   13126200,\n",
       "   13004700,\n",
       "   9901200,\n",
       "   10989600,\n",
       "   8441400,\n",
       "   12669600,\n",
       "   12648300,\n",
       "   12414900,\n",
       "   11738100,\n",
       "   2874900,\n",
       "   11244900,\n",
       "   11052600,\n",
       "   12349200,\n",
       "   11243700,\n",
       "   11311800,\n",
       "   11031600,\n",
       "   1225200,\n",
       "   11949900,\n",
       "   11509200,\n",
       "   978000,\n",
       "   4112400,\n",
       "   10108500,\n",
       "   8343300,\n",
       "   11750400,\n",
       "   11390100,\n",
       "   11635500,\n",
       "   11653200,\n",
       "   11037600,\n",
       "   11112000,\n",
       "   4201200,\n",
       "   11535600,\n",
       "   10921500,\n",
       "   10965600,\n",
       "   4985700,\n",
       "   2643900,\n",
       "   10571100,\n",
       "   11852400,\n",
       "   11770500,\n",
       "   11623200,\n",
       "   5019600,\n",
       "   11444700,\n",
       "   9840300,\n",
       "   10545000,\n",
       "   11651700,\n",
       "   12263700,\n",
       "   7391100,\n",
       "   11423700,\n",
       "   11885700,\n",
       "   12102000,\n",
       "   11874600,\n",
       "   6425100,\n",
       "   6237000,\n",
       "   9430200,\n",
       "   6875100,\n",
       "   1824600,\n",
       "   1956300,\n",
       "   7280100,\n",
       "   13231800,\n",
       "   11670900,\n",
       "   9030600,\n",
       "   13126200,\n",
       "   12843900,\n",
       "   6564600,\n",
       "   13510800,\n",
       "   3535800,\n",
       "   14232900,\n",
       "   13588800,\n",
       "   7439100,\n",
       "   14300100,\n",
       "   3674700,\n",
       "   1204800,\n",
       "   14812200,\n",
       "   11416500,\n",
       "   1043400,\n",
       "   1949100,\n",
       "   12849300,\n",
       "   15569100,\n",
       "   10707600,\n",
       "   3027300,\n",
       "   11967000,\n",
       "   2736300,\n",
       "   15044100,\n",
       "   6067500,\n",
       "   6213900,\n",
       "   12419400,\n",
       "   14732100,\n",
       "   16504200,\n",
       "   13870800,\n",
       "   14771100,\n",
       "   14781600,\n",
       "   17162100,\n",
       "   12979200,\n",
       "   3542700,\n",
       "   2047200,\n",
       "   13183800,\n",
       "   17646900,\n",
       "   18418800,\n",
       "   5924400,\n",
       "   12360300,\n",
       "   18781200,\n",
       "   8481300,\n",
       "   1146600,\n",
       "   3128400,\n",
       "   14432400,\n",
       "   19879200,\n",
       "   20264100,\n",
       "   18618600,\n",
       "   21096900,\n",
       "   20181300,\n",
       "   14267100,\n",
       "   9411300,\n",
       "   3017700,\n",
       "   17495100,\n",
       "   7210500,\n",
       "   16476900,\n",
       "   20935500,\n",
       "   21237000,\n",
       "   16640400,\n",
       "   17032500,\n",
       "   15575400,\n",
       "   10649700,\n",
       "   20110200,\n",
       "   7078800,\n",
       "   14295600,\n",
       "   10215900,\n",
       "   7789200,\n",
       "   20140500,\n",
       "   23717100,\n",
       "   24411900,\n",
       "   23704800,\n",
       "   23746500,\n",
       "   21577200,\n",
       "   23265300,\n",
       "   21837900,\n",
       "   13801200,\n",
       "   10614900,\n",
       "   8971500,\n",
       "   22868400,\n",
       "   16109100,\n",
       "   2610900,\n",
       "   20630400,\n",
       "   17452500,\n",
       "   22936800,\n",
       "   26136300,\n",
       "   25672500,\n",
       "   25449300,\n",
       "   23372700,\n",
       "   13765500,\n",
       "   6854100,\n",
       "   24428100,\n",
       "   22300200,\n",
       "   25803000,\n",
       "   4048800,\n",
       "   14927100,\n",
       "   23465400,\n",
       "   26349600,\n",
       "   26766300,\n",
       "   19307100,\n",
       "   21285600,\n",
       "   7319100,\n",
       "   12789000,\n",
       "   27361500,\n",
       "   27419700,\n",
       "   27917100,\n",
       "   27680400,\n",
       "   21023100,\n",
       "   20709600,\n",
       "   24956100,\n",
       "   24422700,\n",
       "   19665600,\n",
       "   17163000,\n",
       "   20949000,\n",
       "   5761800,\n",
       "   20375700,\n",
       "   28247700,\n",
       "   20102400,\n",
       "   26534400,\n",
       "   20755500,\n",
       "   27054600,\n",
       "   27054600,\n",
       "   21406500,\n",
       "   21733200,\n",
       "   27244800,\n",
       "   24132000,\n",
       "   8185200,\n",
       "   15823500,\n",
       "   7815900,\n",
       "   28124100,\n",
       "   19725300,\n",
       "   27951900,\n",
       "   29661900,\n",
       "   29438100,\n",
       "   22469700,\n",
       "   16734600,\n",
       "   19259700,\n",
       "   21702300,\n",
       "   24736200,\n",
       "   16493100,\n",
       "   10609800,\n",
       "   6051300,\n",
       "   17298300,\n",
       "   24472500,\n",
       "   25282500,\n",
       "   25663800,\n",
       "   27652200,\n",
       "   18786900,\n",
       "   26203200,\n",
       "   21834900,\n",
       "   11501100,\n",
       "   13066800,\n",
       "   20665500,\n",
       "   20773200,\n",
       "   16137000,\n",
       "   29311200,\n",
       "   29473500,\n",
       "   28037400,\n",
       "   20485800,\n",
       "   21624000,\n",
       "   17232000,\n",
       "   11376900,\n",
       "   13869900,\n",
       "   20408100,\n",
       "   20417100,\n",
       "   27554700,\n",
       "   25294800,\n",
       "   23511900,\n",
       "   19968300,\n",
       "   24568500,\n",
       "   27474300,\n",
       "   22161300,\n",
       "   25421100,\n",
       "   27494700,\n",
       "   27892200,\n",
       "   27742200,\n",
       "   27900300,\n",
       "   27991200,\n",
       "   24756300,\n",
       "   21708600,\n",
       "   24743400,\n",
       "   27495900,\n",
       "   27421800,\n",
       "   27198000,\n",
       "   24853200,\n",
       "   19295700,\n",
       "   11206200,\n",
       "   7614300,\n",
       "   26469900,\n",
       "   16269000,\n",
       "   3567300,\n",
       "   5983200,\n",
       "   24680700,\n",
       "   25690200,\n",
       "   26577300,\n",
       "   26222100,\n",
       "   26160600,\n",
       "   17002800,\n",
       "   25483200,\n",
       "   15557400,\n",
       "   18937500,\n",
       "   10055700,\n",
       "   24428100,\n",
       "   17598600,\n",
       "   22085700,\n",
       "   23219700,\n",
       "   24703200,\n",
       "   8449800,\n",
       "   8145300,\n",
       "   24609000,\n",
       "   25087500,\n",
       "   23599200,\n",
       "   12705000,\n",
       "   12489900,\n",
       "   24102600,\n",
       "   20471100,\n",
       "   23104800,\n",
       "   22468800,\n",
       "   20201700,\n",
       "   24102600,\n",
       "   23380800,\n",
       "   19415700,\n",
       "   24704400,\n",
       "   18089700,\n",
       "   20316900,\n",
       "   16792800,\n",
       "   22481100,\n",
       "   22581600,\n",
       "   19866000,\n",
       "   20027400,\n",
       "   23729100,\n",
       "   24081600,\n",
       "   24396900,\n",
       "   24033300,\n",
       "   23324400,\n",
       "   21961800,\n",
       "   20596500,\n",
       "   21479400,\n",
       "   17715300,\n",
       "   16968300,\n",
       "   17342700,\n",
       "   20734500,\n",
       "   17908800,\n",
       "   17911200,\n",
       "   21125400,\n",
       "   20407500,\n",
       "   14832300,\n",
       "   12935700,\n",
       "   15536400,\n",
       "   17864700,\n",
       "   14229300,\n",
       "   19460400,\n",
       "   14463300,\n",
       "   17599500,\n",
       "   7373400,\n",
       "   12957600,\n",
       "   20302200,\n",
       "   15040200,\n",
       "   12049800,\n",
       "   13208400,\n",
       "   6418800,\n",
       "   9271800,\n",
       "   17259600,\n",
       "   4692600,\n",
       "   3283800,\n",
       "   2683500,\n",
       "   17382300,\n",
       "   17661900,\n",
       "   15912900,\n",
       "   17513400,\n",
       "   17460900,\n",
       "   8637600,\n",
       "   17264100,\n",
       "   16735500,\n",
       "   12564900,\n",
       "   14369100,\n",
       "   2716800,\n",
       "   16857300,\n",
       "   15631500,\n",
       "   6292800,\n",
       "   6046200,\n",
       "   2593800,\n",
       "   8279400,\n",
       "   12180600,\n",
       "   14397000,\n",
       "   2788800,\n",
       "   4912800,\n",
       "   1521900,\n",
       "   2619600,\n",
       "   15080700,\n",
       "   15059100,\n",
       "   15027900,\n",
       "   14304900,\n",
       "   13413000,\n",
       "   12486000,\n",
       "   7180500,\n",
       "   4422300,\n",
       "   5221500,\n",
       "   2713500,\n",
       "   3081000,\n",
       "   1929600,\n",
       "   2041800,\n",
       "   1948500,\n",
       "   3912300,\n",
       "   5261100,\n",
       "   5842500,\n",
       "   2691900,\n",
       "   1530900,\n",
       "   4283400,\n",
       "   11910300,\n",
       "   12303900,\n",
       "   12114900,\n",
       "   12329100,\n",
       "   6826500,\n",
       "   1210200,\n",
       "   10651800,\n",
       "   12588300,\n",
       "   12330300,\n",
       "   10459500,\n",
       "   10190700,\n",
       "   2570400,\n",
       "   3653100,\n",
       "   11653800,\n",
       "   8260500,\n",
       "   11336700,\n",
       "   11789400,\n",
       "   11463900,\n",
       "   11497200,\n",
       "   11600100,\n",
       "   11808600,\n",
       "   10635900,\n",
       "   9726000,\n",
       "   11531400,\n",
       "   11046600,\n",
       "   11411400,\n",
       "   11585400,\n",
       "   11196000,\n",
       "   4229700,\n",
       "   9209400,\n",
       "   11630400,\n",
       "   11118900,\n",
       "   11309100,\n",
       "   7236300,\n",
       "   9362700,\n",
       "   6981000,\n",
       "   7562100,\n",
       "   10464600,\n",
       "   4087200,\n",
       "   4105500,\n",
       "   2977500,\n",
       "   1215300,\n",
       "   1647000,\n",
       "   9465600,\n",
       "   9031500,\n",
       "   11736000,\n",
       "   10911900,\n",
       "   4901100,\n",
       "   3090600,\n",
       "   2867700,\n",
       "   12867300,\n",
       "   9649800,\n",
       "   9660300,\n",
       "   12709200,\n",
       "   12212700,\n",
       "   10286700,\n",
       "   10714800,\n",
       "   12967800,\n",
       "   10629300,\n",
       "   13485000,\n",
       "   13588200,\n",
       "   11471400,\n",
       "   13539300,\n",
       "   12701100,\n",
       "   3403500,\n",
       "   1806900,\n",
       "   3456900,\n",
       "   1300500,\n",
       "   2879100,\n",
       "   2625300,\n",
       "   5159100,\n",
       "   15113400,\n",
       "   13054200,\n",
       "   10463100,\n",
       "   3825000,\n",
       "   7112700,\n",
       "   3451200,\n",
       "   15468600,\n",
       "   13884600,\n",
       "   9039300,\n",
       "   2894400,\n",
       "   14977500,\n",
       "   15469200,\n",
       "   15858000,\n",
       "   13447500,\n",
       "   10492800,\n",
       "   5701200,\n",
       "   5082300,\n",
       "   17086200,\n",
       "   16060200,\n",
       "   14715600,\n",
       "   2129400,\n",
       "   17292600,\n",
       "   14264100,\n",
       "   7521000,\n",
       "   4185000,\n",
       "   17981100,\n",
       "   11767500,\n",
       "   17359200,\n",
       "   6294600,\n",
       "   19101000,\n",
       "   11216100,\n",
       "   10654500,\n",
       "   15195900,\n",
       "   18930900,\n",
       "   16344300,\n",
       "   20302500,\n",
       "   20845200,\n",
       "   21272400,\n",
       "   21039600,\n",
       "   12290700,\n",
       "   3846900,\n",
       "   10726200,\n",
       "   21773700,\n",
       "   19973700,\n",
       "   18624600,\n",
       "   19062900,\n",
       "   20363400,\n",
       "   5255700,\n",
       "   13942800,\n",
       "   15795300,\n",
       "   16552200,\n",
       "   3959400,\n",
       "   18909900,\n",
       "   23232000,\n",
       "   16378800,\n",
       "   22051500,\n",
       "   16965300,\n",
       "   23586300,\n",
       "   24955200,\n",
       "   22140900,\n",
       "   21147300,\n",
       "   22431000,\n",
       "   5716800,\n",
       "   24207300,\n",
       "   22047900,\n",
       "   23550000,\n",
       "   13514700,\n",
       "   24289500,\n",
       "   25603500,\n",
       "   25212600,\n",
       "   25040700,\n",
       "   23207700,\n",
       "   22107300,\n",
       "   23494800,\n",
       "   10463100,\n",
       "   13292400,\n",
       "   17672400,\n",
       "   19092300,\n",
       "   24865500,\n",
       "   27204600,\n",
       "   21347700,\n",
       "   22963800,\n",
       "   27200700,\n",
       "   25702800,\n",
       "   25909500,\n",
       "   8908800,\n",
       "   27189000,\n",
       "   11530800,\n",
       "   6896400,\n",
       "   5570100,\n",
       "   6658800,\n",
       "   10806300,\n",
       "   24625800,\n",
       "   9836400,\n",
       "   17992500,\n",
       "   23928300,\n",
       "   24162300,\n",
       "   24038400,\n",
       "   10172400,\n",
       "   18181800,\n",
       "   27988500,\n",
       "   18794700,\n",
       "   21057900,\n",
       "   26783100,\n",
       "   22372200,\n",
       "   23151600,\n",
       "   28227300,\n",
       "   27994200,\n",
       "   27496500,\n",
       "   20253000,\n",
       "   25482300,\n",
       "   14771400,\n",
       "   17659800,\n",
       "   8206500,\n",
       "   13610700,\n",
       "   12516300,\n",
       "   14277000,\n",
       "   11245200,\n",
       "   25816200,\n",
       "   27147900,\n",
       "   20310900,\n",
       "   21003000,\n",
       "   10030500,\n",
       "   24468000,\n",
       "   26152200,\n",
       "   23285100,\n",
       "   21949200,\n",
       "   17139900,\n",
       "   22089300,\n",
       "   23553300,\n",
       "   20533500,\n",
       "   23254800,\n",
       "   18763500,\n",
       "   22404600,\n",
       "   24765300,\n",
       "   27452400,\n",
       "   27594600,\n",
       "   28172100,\n",
       "   28697400,\n",
       "   28043100,\n",
       "   26932200,\n",
       "   26037300,\n",
       "   26067900,\n",
       "   26480700,\n",
       "   27653700,\n",
       "   28026900,\n",
       "   28370100,\n",
       "   27792300,\n",
       "   18503100,\n",
       "   23150100,\n",
       "   28051200,\n",
       "   20945100,\n",
       "   20020800,\n",
       "   25678200,\n",
       "   18772800,\n",
       "   28136400,\n",
       "   27312000,\n",
       "   26578200,\n",
       "   26162700,\n",
       "   26925900,\n",
       "   25961700,\n",
       "   19216800,\n",
       "   24201300,\n",
       "   25859100,\n",
       "   26877000,\n",
       "   19012200,\n",
       "   23841600,\n",
       "   27464100,\n",
       "   27295500,\n",
       "   27378300,\n",
       "   26819400,\n",
       "   21832800,\n",
       "   23465700,\n",
       "   18300900,\n",
       "   22286700,\n",
       "   28458300,\n",
       "   28302900,\n",
       "   26890800,\n",
       "   26932200,\n",
       "   26653800,\n",
       "   26087700,\n",
       "   26110200,\n",
       "   21778200,\n",
       "   23165700,\n",
       "   16723200,\n",
       "   19678500,\n",
       "   14941200,\n",
       "   23424900,\n",
       "   23497200,\n",
       "   21048900,\n",
       "   23110800,\n",
       "   12648300,\n",
       "   5879700,\n",
       "   8742600,\n",
       "   6998100,\n",
       "   23789700,\n",
       "   25192500,\n",
       "   24472500,\n",
       "   24631500,\n",
       "   20253900,\n",
       "   22303500,\n",
       "   19127400,\n",
       "   24222300,\n",
       "   23527800,\n",
       "   24054900,\n",
       "   3354300,\n",
       "   20629200,\n",
       "   23314500,\n",
       "   22550100,\n",
       "   23978700,\n",
       "   23979000,\n",
       "   21546900,\n",
       "   23288400,\n",
       "   21977100,\n",
       "   23523000,\n",
       "   21744000,\n",
       "   21291900,\n",
       "   21665400,\n",
       "   21909300,\n",
       "   20935200,\n",
       "   5980200,\n",
       "   17374800,\n",
       "   21353100,\n",
       "   12294300,\n",
       "   16800300,\n",
       "   22908900,\n",
       "   19839600,\n",
       "   20916000,\n",
       "   21432300,\n",
       "   21034500,\n",
       "   21350700,\n",
       "   20617500,\n",
       "   19909800,\n",
       "   19736100,\n",
       "   20158500,\n",
       "   19358100,\n",
       "   18999600,\n",
       "   18578700,\n",
       "   19113600,\n",
       "   16828800,\n",
       "   14024100,\n",
       "   18148800,\n",
       "   15379200,\n",
       "   15630600,\n",
       "   15508500,\n",
       "   6083400,\n",
       "   14930100,\n",
       "   18298200,\n",
       "   17887800,\n",
       "   7477200,\n",
       "   13002000,\n",
       "   16363200,\n",
       "   15201300,\n",
       "   15946800,\n",
       "   15982500,\n",
       "   17003100,\n",
       "   16998000,\n",
       "   16887000,\n",
       "   16163100,\n",
       "   15624600,\n",
       "   16073100,\n",
       "   15581700,\n",
       "   6965700,\n",
       "   17231700,\n",
       "   16606200,\n",
       "   16045200,\n",
       "   15703200,\n",
       "   15628200,\n",
       "   11771400,\n",
       "   4958400,\n",
       "   8560500,\n",
       "   15712500,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   12254700,\n",
       "   14910300,\n",
       "   15035700,\n",
       "   13860900,\n",
       "   11051100,\n",
       "   10116300,\n",
       "   13116900,\n",
       "   13769700,\n",
       "   12937500,\n",
       "   13844100,\n",
       "   13421400,\n",
       "   13421400,\n",
       "   14074200,\n",
       "   13783500,\n",
       "   13519500,\n",
       "   9787500,\n",
       "   6970500,\n",
       "   13237200,\n",
       "   12667500,\n",
       "   13185300,\n",
       "   6247200]}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling served model to generate predictions starting from 1994-01-01 00:00:00\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-b464370ea974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestamped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ACME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1994-01-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-139-244675c1ba8e>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(predictor, target_ts, cat, dynamic_feat, forecast_date, show_samples, plot_history, confidence)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# call the end point to get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# plot the samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a417a3e1db56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprediction_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/arrays/datetimelike.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mgetitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(predictor, target_ts=timestamped['ACME'], forecast_date=pd.Timestamp('1994-01-01', freq='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = predictor.predict(ts=timestamped['ACME'], quantiles=[0.10, 0.5, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp\n",
       "2005-11-25     7437900\n",
       "2005-11-26     8199600\n",
       "2005-11-27    10694400\n",
       "2005-11-28     8704800\n",
       "2005-11-29    12803100\n",
       "2005-11-30    12027300\n",
       "2005-12-01    12519000\n",
       "2005-12-02    11570100\n",
       "2005-12-03     9417300\n",
       "2005-12-04     9941700\n",
       "2005-12-05    11968200\n",
       "2005-12-06    12077400\n",
       "2005-12-07     4561500\n",
       "2005-12-08    13197300\n",
       "2005-12-09    12252600\n",
       "2005-12-10    11796900\n",
       "2005-12-11    11721300\n",
       "2005-12-12     7685400\n",
       "2005-12-13     5451600\n",
       "2005-12-14     9810900\n",
       "2005-12-15    12423600\n",
       "2005-12-16    11590800\n",
       "2005-12-17     1609800\n",
       "2005-12-18     3166500\n",
       "2005-12-19     3104700\n",
       "2005-12-20     2655300\n",
       "2005-12-21     6268200\n",
       "2005-12-22    11357100\n",
       "2005-12-23    10005900\n",
       "2005-12-24    11276400\n",
       "                ...   \n",
       "2007-12-02    12177000\n",
       "2007-12-03    13453800\n",
       "2007-12-04    12525900\n",
       "2007-12-05    10543500\n",
       "2007-12-06     6214500\n",
       "2007-12-07     8417400\n",
       "2007-12-08     1683900\n",
       "2007-12-09     1512300\n",
       "2007-12-10     1729500\n",
       "2007-12-11     1512600\n",
       "2007-12-12     2126100\n",
       "2007-12-13     3476400\n",
       "2007-12-14     1262700\n",
       "2007-12-15     3074400\n",
       "2007-12-16    12523800\n",
       "2007-12-17    12089700\n",
       "2007-12-18    12300600\n",
       "2007-12-19    12206100\n",
       "2007-12-20    12275700\n",
       "2007-12-21     9737700\n",
       "2007-12-22     1958400\n",
       "2007-12-23    12325200\n",
       "2007-12-24    12225900\n",
       "2007-12-25    12256500\n",
       "2007-12-26     1851300\n",
       "2007-12-27     1408500\n",
       "2007-12-28    10060800\n",
       "2007-12-29    11388000\n",
       "2007-12-30    12441000\n",
       "2007-12-31    12450300\n",
       "Freq: D, Name: ACME, Length: 767, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped['ACME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = round(df.shape[0] * 0.5)\n",
    "lower_bound = round(df.shape[0] * 0.65)\n",
    "validation_df = only_one[lower_bound:upper_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, ACME]\n",
       "Index: []"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one = df[['Date','ACME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = only_one[lower_bound:upper_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, ACME]\n",
       "Index: []"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME\n",
       "0  19940101  12384900\n",
       "1  19940102  11908500\n",
       "2  19940103  12470700\n",
       "3  19940104  12725400\n",
       "4  19940105  10894800"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = only_one[upper_bound:lower_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>20001231</td>\n",
       "      <td>3333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>20010101</td>\n",
       "      <td>4652700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>20010102</td>\n",
       "      <td>4750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>20010103</td>\n",
       "      <td>12811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>20010104</td>\n",
       "      <td>12755700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME\n",
       "2556  20001231   3333600\n",
       "2557  20010101   4652700\n",
       "2558  20010102   4750200\n",
       "2559  20010103  12811200\n",
       "2560  20010104  12755700"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "validation_df['Timestamp'] = pd.to_datetime(validation_df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-8810b153b8e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_df_only_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "validation_df_only_two = validation_df.drop('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>20001231</td>\n",
       "      <td>3333600</td>\n",
       "      <td>2000-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>20010101</td>\n",
       "      <td>4652700</td>\n",
       "      <td>2001-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>20010102</td>\n",
       "      <td>4750200</td>\n",
       "      <td>2001-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>20010103</td>\n",
       "      <td>12811200</td>\n",
       "      <td>2001-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>20010104</td>\n",
       "      <td>12755700</td>\n",
       "      <td>2001-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>20010105</td>\n",
       "      <td>12808500</td>\n",
       "      <td>2001-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>20010106</td>\n",
       "      <td>11605200</td>\n",
       "      <td>2001-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>20010107</td>\n",
       "      <td>10151400</td>\n",
       "      <td>2001-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>20010108</td>\n",
       "      <td>13141800</td>\n",
       "      <td>2001-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>20010109</td>\n",
       "      <td>12949200</td>\n",
       "      <td>2001-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>20010110</td>\n",
       "      <td>2400900</td>\n",
       "      <td>2001-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>20010111</td>\n",
       "      <td>3647700</td>\n",
       "      <td>2001-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>20010112</td>\n",
       "      <td>5057400</td>\n",
       "      <td>2001-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>20010113</td>\n",
       "      <td>7526400</td>\n",
       "      <td>2001-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>20010114</td>\n",
       "      <td>13294800</td>\n",
       "      <td>2001-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>20010115</td>\n",
       "      <td>11721600</td>\n",
       "      <td>2001-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>20010116</td>\n",
       "      <td>1934700</td>\n",
       "      <td>2001-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>20010117</td>\n",
       "      <td>4771800</td>\n",
       "      <td>2001-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>20010118</td>\n",
       "      <td>10996200</td>\n",
       "      <td>2001-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>20010119</td>\n",
       "      <td>13291500</td>\n",
       "      <td>2001-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>20010120</td>\n",
       "      <td>11609100</td>\n",
       "      <td>2001-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>20010121</td>\n",
       "      <td>13519800</td>\n",
       "      <td>2001-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>20010122</td>\n",
       "      <td>13963800</td>\n",
       "      <td>2001-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>20010123</td>\n",
       "      <td>3246000</td>\n",
       "      <td>2001-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>20010124</td>\n",
       "      <td>9239400</td>\n",
       "      <td>2001-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>20010125</td>\n",
       "      <td>10505100</td>\n",
       "      <td>2001-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>20010126</td>\n",
       "      <td>14770200</td>\n",
       "      <td>2001-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>20010127</td>\n",
       "      <td>2879700</td>\n",
       "      <td>2001-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>20010128</td>\n",
       "      <td>1077900</td>\n",
       "      <td>2001-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>20010129</td>\n",
       "      <td>12549900</td>\n",
       "      <td>2001-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>20030107</td>\n",
       "      <td>12420000</td>\n",
       "      <td>2003-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>20030108</td>\n",
       "      <td>12713100</td>\n",
       "      <td>2003-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>20030109</td>\n",
       "      <td>11878200</td>\n",
       "      <td>2003-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>20030110</td>\n",
       "      <td>12957000</td>\n",
       "      <td>2003-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>20030111</td>\n",
       "      <td>8911200</td>\n",
       "      <td>2003-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>20030112</td>\n",
       "      <td>2350200</td>\n",
       "      <td>2003-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>20030113</td>\n",
       "      <td>3077100</td>\n",
       "      <td>2003-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>20030114</td>\n",
       "      <td>11821500</td>\n",
       "      <td>2003-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>20030115</td>\n",
       "      <td>2746200</td>\n",
       "      <td>2003-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3302</th>\n",
       "      <td>20030116</td>\n",
       "      <td>13502400</td>\n",
       "      <td>2003-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>20030117</td>\n",
       "      <td>8755800</td>\n",
       "      <td>2003-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3304</th>\n",
       "      <td>20030118</td>\n",
       "      <td>13803000</td>\n",
       "      <td>2003-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305</th>\n",
       "      <td>20030119</td>\n",
       "      <td>13764000</td>\n",
       "      <td>2003-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>20030120</td>\n",
       "      <td>14045400</td>\n",
       "      <td>2003-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>20030121</td>\n",
       "      <td>6780000</td>\n",
       "      <td>2003-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>20030122</td>\n",
       "      <td>3048300</td>\n",
       "      <td>2003-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>20030123</td>\n",
       "      <td>14184300</td>\n",
       "      <td>2003-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>20030124</td>\n",
       "      <td>7472100</td>\n",
       "      <td>2003-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>20030125</td>\n",
       "      <td>13102500</td>\n",
       "      <td>2003-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>20030126</td>\n",
       "      <td>12663300</td>\n",
       "      <td>2003-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>20030127</td>\n",
       "      <td>10636500</td>\n",
       "      <td>2003-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>20030128</td>\n",
       "      <td>10643700</td>\n",
       "      <td>2003-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>20030129</td>\n",
       "      <td>6982800</td>\n",
       "      <td>2003-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>20030130</td>\n",
       "      <td>14763600</td>\n",
       "      <td>2003-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>20030131</td>\n",
       "      <td>15281700</td>\n",
       "      <td>2003-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>20030201</td>\n",
       "      <td>15013200</td>\n",
       "      <td>2003-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>20030202</td>\n",
       "      <td>13665900</td>\n",
       "      <td>2003-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>20030203</td>\n",
       "      <td>14706300</td>\n",
       "      <td>2003-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>20030204</td>\n",
       "      <td>16366800</td>\n",
       "      <td>2003-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>20030205</td>\n",
       "      <td>4728000</td>\n",
       "      <td>2003-02-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME  Timestamp\n",
       "2556  20001231   3333600 2000-12-31\n",
       "2557  20010101   4652700 2001-01-01\n",
       "2558  20010102   4750200 2001-01-02\n",
       "2559  20010103  12811200 2001-01-03\n",
       "2560  20010104  12755700 2001-01-04\n",
       "2561  20010105  12808500 2001-01-05\n",
       "2562  20010106  11605200 2001-01-06\n",
       "2563  20010107  10151400 2001-01-07\n",
       "2564  20010108  13141800 2001-01-08\n",
       "2565  20010109  12949200 2001-01-09\n",
       "2566  20010110   2400900 2001-01-10\n",
       "2567  20010111   3647700 2001-01-11\n",
       "2568  20010112   5057400 2001-01-12\n",
       "2569  20010113   7526400 2001-01-13\n",
       "2570  20010114  13294800 2001-01-14\n",
       "2571  20010115  11721600 2001-01-15\n",
       "2572  20010116   1934700 2001-01-16\n",
       "2573  20010117   4771800 2001-01-17\n",
       "2574  20010118  10996200 2001-01-18\n",
       "2575  20010119  13291500 2001-01-19\n",
       "2576  20010120  11609100 2001-01-20\n",
       "2577  20010121  13519800 2001-01-21\n",
       "2578  20010122  13963800 2001-01-22\n",
       "2579  20010123   3246000 2001-01-23\n",
       "2580  20010124   9239400 2001-01-24\n",
       "2581  20010125  10505100 2001-01-25\n",
       "2582  20010126  14770200 2001-01-26\n",
       "2583  20010127   2879700 2001-01-27\n",
       "2584  20010128   1077900 2001-01-28\n",
       "2585  20010129  12549900 2001-01-29\n",
       "...        ...       ...        ...\n",
       "3293  20030107  12420000 2003-01-07\n",
       "3294  20030108  12713100 2003-01-08\n",
       "3295  20030109  11878200 2003-01-09\n",
       "3296  20030110  12957000 2003-01-10\n",
       "3297  20030111   8911200 2003-01-11\n",
       "3298  20030112   2350200 2003-01-12\n",
       "3299  20030113   3077100 2003-01-13\n",
       "3300  20030114  11821500 2003-01-14\n",
       "3301  20030115   2746200 2003-01-15\n",
       "3302  20030116  13502400 2003-01-16\n",
       "3303  20030117   8755800 2003-01-17\n",
       "3304  20030118  13803000 2003-01-18\n",
       "3305  20030119  13764000 2003-01-19\n",
       "3306  20030120  14045400 2003-01-20\n",
       "3307  20030121   6780000 2003-01-21\n",
       "3308  20030122   3048300 2003-01-22\n",
       "3309  20030123  14184300 2003-01-23\n",
       "3310  20030124   7472100 2003-01-24\n",
       "3311  20030125  13102500 2003-01-25\n",
       "3312  20030126  12663300 2003-01-26\n",
       "3313  20030127  10636500 2003-01-27\n",
       "3314  20030128  10643700 2003-01-28\n",
       "3315  20030129   6982800 2003-01-29\n",
       "3316  20030130  14763600 2003-01-30\n",
       "3317  20030131  15281700 2003-01-31\n",
       "3318  20030201  15013200 2003-02-01\n",
       "3319  20030202  13665900 2003-02-02\n",
       "3320  20030203  14706300 2003-02-03\n",
       "3321  20030204  16366800 2003-02-04\n",
       "3322  20030205   4728000 2003-02-05\n",
       "\n",
       "[767 rows x 3 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d0fa66181bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "validation_set = get_split(df, 'D', split_type = 'validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19940106</td>\n",
       "      <td>6639000</td>\n",
       "      <td>6817200</td>\n",
       "      <td>8157900</td>\n",
       "      <td>7673100</td>\n",
       "      <td>3500400</td>\n",
       "      <td>2245200</td>\n",
       "      <td>9719400</td>\n",
       "      <td>6137100</td>\n",
       "      <td>4328700</td>\n",
       "      <td>...</td>\n",
       "      <td>5257200</td>\n",
       "      <td>6687000</td>\n",
       "      <td>5631600</td>\n",
       "      <td>7990500</td>\n",
       "      <td>9402600</td>\n",
       "      <td>8463600</td>\n",
       "      <td>7323900</td>\n",
       "      <td>7113600</td>\n",
       "      <td>2124600</td>\n",
       "      <td>3324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19940107</td>\n",
       "      <td>13244700</td>\n",
       "      <td>12418800</td>\n",
       "      <td>12369900</td>\n",
       "      <td>12873000</td>\n",
       "      <td>12181800</td>\n",
       "      <td>9877800</td>\n",
       "      <td>12114300</td>\n",
       "      <td>12175200</td>\n",
       "      <td>11836500</td>\n",
       "      <td>...</td>\n",
       "      <td>11495100</td>\n",
       "      <td>12486300</td>\n",
       "      <td>12098700</td>\n",
       "      <td>13191300</td>\n",
       "      <td>11855100</td>\n",
       "      <td>11494800</td>\n",
       "      <td>12620400</td>\n",
       "      <td>12380700</td>\n",
       "      <td>10944900</td>\n",
       "      <td>11403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19940108</td>\n",
       "      <td>12927900</td>\n",
       "      <td>12375600</td>\n",
       "      <td>12634500</td>\n",
       "      <td>13066500</td>\n",
       "      <td>11608800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>12029400</td>\n",
       "      <td>12217500</td>\n",
       "      <td>11505300</td>\n",
       "      <td>...</td>\n",
       "      <td>11284800</td>\n",
       "      <td>12471300</td>\n",
       "      <td>12072300</td>\n",
       "      <td>10429500</td>\n",
       "      <td>11939100</td>\n",
       "      <td>11280300</td>\n",
       "      <td>12419700</td>\n",
       "      <td>12225900</td>\n",
       "      <td>11029200</td>\n",
       "      <td>11268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19940109</td>\n",
       "      <td>12600300</td>\n",
       "      <td>11601000</td>\n",
       "      <td>12156000</td>\n",
       "      <td>12464700</td>\n",
       "      <td>10866000</td>\n",
       "      <td>11295300</td>\n",
       "      <td>11937900</td>\n",
       "      <td>10443300</td>\n",
       "      <td>9218400</td>\n",
       "      <td>...</td>\n",
       "      <td>8755800</td>\n",
       "      <td>12391800</td>\n",
       "      <td>11369400</td>\n",
       "      <td>11324400</td>\n",
       "      <td>11498700</td>\n",
       "      <td>9737100</td>\n",
       "      <td>11985000</td>\n",
       "      <td>11454900</td>\n",
       "      <td>10518300</td>\n",
       "      <td>8577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19940110</td>\n",
       "      <td>6406500</td>\n",
       "      <td>3935700</td>\n",
       "      <td>12321900</td>\n",
       "      <td>8164800</td>\n",
       "      <td>11328600</td>\n",
       "      <td>10785000</td>\n",
       "      <td>12081600</td>\n",
       "      <td>1873800</td>\n",
       "      <td>9658800</td>\n",
       "      <td>...</td>\n",
       "      <td>3155100</td>\n",
       "      <td>3879900</td>\n",
       "      <td>11709300</td>\n",
       "      <td>3808500</td>\n",
       "      <td>11526900</td>\n",
       "      <td>2064300</td>\n",
       "      <td>7641000</td>\n",
       "      <td>2282400</td>\n",
       "      <td>10859700</td>\n",
       "      <td>2520600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19940111</td>\n",
       "      <td>12743400</td>\n",
       "      <td>7137000</td>\n",
       "      <td>12966300</td>\n",
       "      <td>12774600</td>\n",
       "      <td>12005100</td>\n",
       "      <td>11424900</td>\n",
       "      <td>12149400</td>\n",
       "      <td>2835600</td>\n",
       "      <td>2574000</td>\n",
       "      <td>...</td>\n",
       "      <td>3411600</td>\n",
       "      <td>12371400</td>\n",
       "      <td>11973000</td>\n",
       "      <td>12872400</td>\n",
       "      <td>11970900</td>\n",
       "      <td>1908000</td>\n",
       "      <td>1998300</td>\n",
       "      <td>1577100</td>\n",
       "      <td>11256600</td>\n",
       "      <td>2923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19940112</td>\n",
       "      <td>10453500</td>\n",
       "      <td>7371000</td>\n",
       "      <td>12855300</td>\n",
       "      <td>11448000</td>\n",
       "      <td>11493000</td>\n",
       "      <td>11794200</td>\n",
       "      <td>11780400</td>\n",
       "      <td>6759900</td>\n",
       "      <td>3571800</td>\n",
       "      <td>...</td>\n",
       "      <td>2370600</td>\n",
       "      <td>8449200</td>\n",
       "      <td>11049000</td>\n",
       "      <td>8650500</td>\n",
       "      <td>11623200</td>\n",
       "      <td>5994300</td>\n",
       "      <td>4441500</td>\n",
       "      <td>6324900</td>\n",
       "      <td>10424100</td>\n",
       "      <td>7131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19940113</td>\n",
       "      <td>12985200</td>\n",
       "      <td>12510600</td>\n",
       "      <td>13198500</td>\n",
       "      <td>12726900</td>\n",
       "      <td>12289200</td>\n",
       "      <td>12149100</td>\n",
       "      <td>12467100</td>\n",
       "      <td>9930900</td>\n",
       "      <td>9628500</td>\n",
       "      <td>...</td>\n",
       "      <td>8013000</td>\n",
       "      <td>12463200</td>\n",
       "      <td>12099900</td>\n",
       "      <td>13268400</td>\n",
       "      <td>12205500</td>\n",
       "      <td>10277100</td>\n",
       "      <td>9863700</td>\n",
       "      <td>8588400</td>\n",
       "      <td>11655300</td>\n",
       "      <td>10062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19940114</td>\n",
       "      <td>13080000</td>\n",
       "      <td>12552000</td>\n",
       "      <td>13446600</td>\n",
       "      <td>13026600</td>\n",
       "      <td>12393000</td>\n",
       "      <td>12227700</td>\n",
       "      <td>12488700</td>\n",
       "      <td>10799100</td>\n",
       "      <td>10770900</td>\n",
       "      <td>...</td>\n",
       "      <td>6584100</td>\n",
       "      <td>12793800</td>\n",
       "      <td>12221100</td>\n",
       "      <td>12990000</td>\n",
       "      <td>12166800</td>\n",
       "      <td>5569500</td>\n",
       "      <td>11459400</td>\n",
       "      <td>11072100</td>\n",
       "      <td>11614200</td>\n",
       "      <td>9288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19940115</td>\n",
       "      <td>11826300</td>\n",
       "      <td>11997300</td>\n",
       "      <td>11313300</td>\n",
       "      <td>11793300</td>\n",
       "      <td>10750200</td>\n",
       "      <td>10290600</td>\n",
       "      <td>11184600</td>\n",
       "      <td>12337500</td>\n",
       "      <td>11489700</td>\n",
       "      <td>...</td>\n",
       "      <td>8565600</td>\n",
       "      <td>11604000</td>\n",
       "      <td>12094800</td>\n",
       "      <td>11343900</td>\n",
       "      <td>10906500</td>\n",
       "      <td>11029500</td>\n",
       "      <td>12093300</td>\n",
       "      <td>12119100</td>\n",
       "      <td>10898700</td>\n",
       "      <td>10905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19940116</td>\n",
       "      <td>1974000</td>\n",
       "      <td>1339800</td>\n",
       "      <td>3120600</td>\n",
       "      <td>1058700</td>\n",
       "      <td>7187100</td>\n",
       "      <td>9792900</td>\n",
       "      <td>1405500</td>\n",
       "      <td>711900</td>\n",
       "      <td>1133700</td>\n",
       "      <td>...</td>\n",
       "      <td>640200</td>\n",
       "      <td>1401300</td>\n",
       "      <td>1288500</td>\n",
       "      <td>1257600</td>\n",
       "      <td>1125300</td>\n",
       "      <td>739800</td>\n",
       "      <td>756900</td>\n",
       "      <td>469500</td>\n",
       "      <td>3777300</td>\n",
       "      <td>748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19940117</td>\n",
       "      <td>13541700</td>\n",
       "      <td>13021200</td>\n",
       "      <td>13757100</td>\n",
       "      <td>13432800</td>\n",
       "      <td>12486600</td>\n",
       "      <td>11738100</td>\n",
       "      <td>12719700</td>\n",
       "      <td>7774200</td>\n",
       "      <td>9512400</td>\n",
       "      <td>...</td>\n",
       "      <td>10155300</td>\n",
       "      <td>13079400</td>\n",
       "      <td>12507900</td>\n",
       "      <td>13869000</td>\n",
       "      <td>12336300</td>\n",
       "      <td>11681100</td>\n",
       "      <td>9247500</td>\n",
       "      <td>10576200</td>\n",
       "      <td>11586900</td>\n",
       "      <td>9893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19940118</td>\n",
       "      <td>13673700</td>\n",
       "      <td>13042200</td>\n",
       "      <td>13881000</td>\n",
       "      <td>13586100</td>\n",
       "      <td>13158300</td>\n",
       "      <td>12724200</td>\n",
       "      <td>12984000</td>\n",
       "      <td>9400800</td>\n",
       "      <td>11339400</td>\n",
       "      <td>...</td>\n",
       "      <td>8709300</td>\n",
       "      <td>13125900</td>\n",
       "      <td>12694500</td>\n",
       "      <td>13959900</td>\n",
       "      <td>12517800</td>\n",
       "      <td>12465000</td>\n",
       "      <td>12720600</td>\n",
       "      <td>10228200</td>\n",
       "      <td>12076800</td>\n",
       "      <td>10843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19940119</td>\n",
       "      <td>6796800</td>\n",
       "      <td>8217000</td>\n",
       "      <td>13563300</td>\n",
       "      <td>7861800</td>\n",
       "      <td>13200900</td>\n",
       "      <td>13184700</td>\n",
       "      <td>12884700</td>\n",
       "      <td>10399500</td>\n",
       "      <td>12214500</td>\n",
       "      <td>...</td>\n",
       "      <td>8323200</td>\n",
       "      <td>7311600</td>\n",
       "      <td>12622800</td>\n",
       "      <td>5567400</td>\n",
       "      <td>12464700</td>\n",
       "      <td>14057400</td>\n",
       "      <td>11255400</td>\n",
       "      <td>12093900</td>\n",
       "      <td>12372300</td>\n",
       "      <td>10474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19940120</td>\n",
       "      <td>5658900</td>\n",
       "      <td>4757700</td>\n",
       "      <td>1976100</td>\n",
       "      <td>4926000</td>\n",
       "      <td>3088800</td>\n",
       "      <td>10210500</td>\n",
       "      <td>3547200</td>\n",
       "      <td>10607100</td>\n",
       "      <td>5247900</td>\n",
       "      <td>...</td>\n",
       "      <td>4558800</td>\n",
       "      <td>5358300</td>\n",
       "      <td>5883600</td>\n",
       "      <td>2109900</td>\n",
       "      <td>4254900</td>\n",
       "      <td>6372900</td>\n",
       "      <td>8222700</td>\n",
       "      <td>9356400</td>\n",
       "      <td>6758100</td>\n",
       "      <td>7672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19940121</td>\n",
       "      <td>7073400</td>\n",
       "      <td>10822800</td>\n",
       "      <td>4021800</td>\n",
       "      <td>6464100</td>\n",
       "      <td>4468500</td>\n",
       "      <td>11169300</td>\n",
       "      <td>5096100</td>\n",
       "      <td>12288000</td>\n",
       "      <td>11910300</td>\n",
       "      <td>...</td>\n",
       "      <td>11721600</td>\n",
       "      <td>9132300</td>\n",
       "      <td>7908900</td>\n",
       "      <td>3649500</td>\n",
       "      <td>6578100</td>\n",
       "      <td>11701200</td>\n",
       "      <td>12758400</td>\n",
       "      <td>12867300</td>\n",
       "      <td>5543700</td>\n",
       "      <td>11705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19940122</td>\n",
       "      <td>3354000</td>\n",
       "      <td>2764800</td>\n",
       "      <td>2997900</td>\n",
       "      <td>2726100</td>\n",
       "      <td>8875800</td>\n",
       "      <td>12647400</td>\n",
       "      <td>2881500</td>\n",
       "      <td>3448500</td>\n",
       "      <td>8096400</td>\n",
       "      <td>...</td>\n",
       "      <td>7866600</td>\n",
       "      <td>2923200</td>\n",
       "      <td>4995000</td>\n",
       "      <td>1614900</td>\n",
       "      <td>3245400</td>\n",
       "      <td>3254100</td>\n",
       "      <td>3469800</td>\n",
       "      <td>2306100</td>\n",
       "      <td>9792600</td>\n",
       "      <td>5962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19940123</td>\n",
       "      <td>2579700</td>\n",
       "      <td>2339100</td>\n",
       "      <td>6508500</td>\n",
       "      <td>5418600</td>\n",
       "      <td>12936900</td>\n",
       "      <td>12553500</td>\n",
       "      <td>9266400</td>\n",
       "      <td>1948800</td>\n",
       "      <td>6969000</td>\n",
       "      <td>...</td>\n",
       "      <td>2103000</td>\n",
       "      <td>2631900</td>\n",
       "      <td>10932900</td>\n",
       "      <td>2564700</td>\n",
       "      <td>8967900</td>\n",
       "      <td>1685400</td>\n",
       "      <td>2399700</td>\n",
       "      <td>2453400</td>\n",
       "      <td>12991800</td>\n",
       "      <td>3377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19940124</td>\n",
       "      <td>2387700</td>\n",
       "      <td>2096700</td>\n",
       "      <td>6390000</td>\n",
       "      <td>3164700</td>\n",
       "      <td>9348300</td>\n",
       "      <td>7642200</td>\n",
       "      <td>5437500</td>\n",
       "      <td>1742100</td>\n",
       "      <td>3155100</td>\n",
       "      <td>...</td>\n",
       "      <td>1695300</td>\n",
       "      <td>2771400</td>\n",
       "      <td>5281800</td>\n",
       "      <td>3616500</td>\n",
       "      <td>4453500</td>\n",
       "      <td>1114200</td>\n",
       "      <td>2427300</td>\n",
       "      <td>1808400</td>\n",
       "      <td>9188100</td>\n",
       "      <td>2060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19940125</td>\n",
       "      <td>8390700</td>\n",
       "      <td>8621700</td>\n",
       "      <td>11639100</td>\n",
       "      <td>11404200</td>\n",
       "      <td>11722200</td>\n",
       "      <td>13093500</td>\n",
       "      <td>11776500</td>\n",
       "      <td>7530900</td>\n",
       "      <td>4177200</td>\n",
       "      <td>...</td>\n",
       "      <td>6538500</td>\n",
       "      <td>9204900</td>\n",
       "      <td>8374800</td>\n",
       "      <td>12530400</td>\n",
       "      <td>10918200</td>\n",
       "      <td>6499500</td>\n",
       "      <td>7495200</td>\n",
       "      <td>6218700</td>\n",
       "      <td>12260100</td>\n",
       "      <td>7261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19940126</td>\n",
       "      <td>7326600</td>\n",
       "      <td>8104500</td>\n",
       "      <td>10221900</td>\n",
       "      <td>8622300</td>\n",
       "      <td>12958500</td>\n",
       "      <td>7973100</td>\n",
       "      <td>11801400</td>\n",
       "      <td>5955000</td>\n",
       "      <td>7979100</td>\n",
       "      <td>...</td>\n",
       "      <td>3879300</td>\n",
       "      <td>8131500</td>\n",
       "      <td>11583600</td>\n",
       "      <td>8124000</td>\n",
       "      <td>11062800</td>\n",
       "      <td>3655800</td>\n",
       "      <td>6165600</td>\n",
       "      <td>3794100</td>\n",
       "      <td>11837400</td>\n",
       "      <td>6168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19940127</td>\n",
       "      <td>10743900</td>\n",
       "      <td>8913000</td>\n",
       "      <td>13336500</td>\n",
       "      <td>11616000</td>\n",
       "      <td>12311700</td>\n",
       "      <td>9855300</td>\n",
       "      <td>13071000</td>\n",
       "      <td>10467300</td>\n",
       "      <td>11326800</td>\n",
       "      <td>...</td>\n",
       "      <td>6998400</td>\n",
       "      <td>10048800</td>\n",
       "      <td>12608400</td>\n",
       "      <td>11251800</td>\n",
       "      <td>12623100</td>\n",
       "      <td>5641200</td>\n",
       "      <td>5928900</td>\n",
       "      <td>4412100</td>\n",
       "      <td>12267900</td>\n",
       "      <td>11280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19940128</td>\n",
       "      <td>12812100</td>\n",
       "      <td>13680900</td>\n",
       "      <td>12890700</td>\n",
       "      <td>13834800</td>\n",
       "      <td>13994700</td>\n",
       "      <td>13548000</td>\n",
       "      <td>13434300</td>\n",
       "      <td>11479200</td>\n",
       "      <td>8445900</td>\n",
       "      <td>...</td>\n",
       "      <td>9601500</td>\n",
       "      <td>13080300</td>\n",
       "      <td>11967900</td>\n",
       "      <td>12759300</td>\n",
       "      <td>12837300</td>\n",
       "      <td>9696300</td>\n",
       "      <td>12404400</td>\n",
       "      <td>10782600</td>\n",
       "      <td>11917500</td>\n",
       "      <td>8725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19940129</td>\n",
       "      <td>9065100</td>\n",
       "      <td>5018100</td>\n",
       "      <td>10373700</td>\n",
       "      <td>10304100</td>\n",
       "      <td>13456800</td>\n",
       "      <td>13098600</td>\n",
       "      <td>13869900</td>\n",
       "      <td>8671200</td>\n",
       "      <td>12211500</td>\n",
       "      <td>...</td>\n",
       "      <td>12647400</td>\n",
       "      <td>7510500</td>\n",
       "      <td>13708200</td>\n",
       "      <td>5198700</td>\n",
       "      <td>13585800</td>\n",
       "      <td>7622400</td>\n",
       "      <td>5036400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>12272700</td>\n",
       "      <td>11245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19940130</td>\n",
       "      <td>3954900</td>\n",
       "      <td>7685700</td>\n",
       "      <td>3863100</td>\n",
       "      <td>4220700</td>\n",
       "      <td>5629200</td>\n",
       "      <td>2520000</td>\n",
       "      <td>5959500</td>\n",
       "      <td>11066400</td>\n",
       "      <td>6694800</td>\n",
       "      <td>...</td>\n",
       "      <td>9793800</td>\n",
       "      <td>6205500</td>\n",
       "      <td>5371200</td>\n",
       "      <td>6331800</td>\n",
       "      <td>6069600</td>\n",
       "      <td>12349500</td>\n",
       "      <td>9744900</td>\n",
       "      <td>10736400</td>\n",
       "      <td>5056800</td>\n",
       "      <td>7212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>20071202</td>\n",
       "      <td>12177000</td>\n",
       "      <td>11132400</td>\n",
       "      <td>12620100</td>\n",
       "      <td>11796600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12604500</td>\n",
       "      <td>11340600</td>\n",
       "      <td>7562100</td>\n",
       "      <td>8957100</td>\n",
       "      <td>...</td>\n",
       "      <td>5517600</td>\n",
       "      <td>10997700</td>\n",
       "      <td>10304400</td>\n",
       "      <td>12716700</td>\n",
       "      <td>11124600</td>\n",
       "      <td>3845700</td>\n",
       "      <td>8625000</td>\n",
       "      <td>7135500</td>\n",
       "      <td>10745100</td>\n",
       "      <td>7718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>20071203</td>\n",
       "      <td>13453800</td>\n",
       "      <td>12737700</td>\n",
       "      <td>12828300</td>\n",
       "      <td>12809700</td>\n",
       "      <td>12264300</td>\n",
       "      <td>12439200</td>\n",
       "      <td>12496800</td>\n",
       "      <td>12446100</td>\n",
       "      <td>11181000</td>\n",
       "      <td>...</td>\n",
       "      <td>11936700</td>\n",
       "      <td>13204500</td>\n",
       "      <td>12599400</td>\n",
       "      <td>13403400</td>\n",
       "      <td>12964500</td>\n",
       "      <td>12735900</td>\n",
       "      <td>13133100</td>\n",
       "      <td>12933900</td>\n",
       "      <td>11300400</td>\n",
       "      <td>12414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>20071204</td>\n",
       "      <td>12525900</td>\n",
       "      <td>10054800</td>\n",
       "      <td>12328200</td>\n",
       "      <td>12057900</td>\n",
       "      <td>12027900</td>\n",
       "      <td>11833500</td>\n",
       "      <td>11980200</td>\n",
       "      <td>10532100</td>\n",
       "      <td>9660300</td>\n",
       "      <td>...</td>\n",
       "      <td>10314000</td>\n",
       "      <td>12219900</td>\n",
       "      <td>12349200</td>\n",
       "      <td>12531600</td>\n",
       "      <td>12132600</td>\n",
       "      <td>10984800</td>\n",
       "      <td>12141600</td>\n",
       "      <td>11658900</td>\n",
       "      <td>10941300</td>\n",
       "      <td>10495500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>20071205</td>\n",
       "      <td>10543500</td>\n",
       "      <td>8691900</td>\n",
       "      <td>12200100</td>\n",
       "      <td>11790300</td>\n",
       "      <td>9699300</td>\n",
       "      <td>10742100</td>\n",
       "      <td>12017700</td>\n",
       "      <td>10608900</td>\n",
       "      <td>10084500</td>\n",
       "      <td>...</td>\n",
       "      <td>7831500</td>\n",
       "      <td>10752600</td>\n",
       "      <td>11216700</td>\n",
       "      <td>10968900</td>\n",
       "      <td>12183600</td>\n",
       "      <td>9241500</td>\n",
       "      <td>8371200</td>\n",
       "      <td>8967900</td>\n",
       "      <td>9399900</td>\n",
       "      <td>11064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>20071206</td>\n",
       "      <td>6214500</td>\n",
       "      <td>5707500</td>\n",
       "      <td>6339900</td>\n",
       "      <td>6242400</td>\n",
       "      <td>7813800</td>\n",
       "      <td>8302800</td>\n",
       "      <td>6486600</td>\n",
       "      <td>5260200</td>\n",
       "      <td>2649300</td>\n",
       "      <td>...</td>\n",
       "      <td>2001300</td>\n",
       "      <td>7011000</td>\n",
       "      <td>6879900</td>\n",
       "      <td>7605000</td>\n",
       "      <td>7298700</td>\n",
       "      <td>3600900</td>\n",
       "      <td>5550900</td>\n",
       "      <td>5042700</td>\n",
       "      <td>6977700</td>\n",
       "      <td>3435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>20071207</td>\n",
       "      <td>8417400</td>\n",
       "      <td>6139800</td>\n",
       "      <td>6981000</td>\n",
       "      <td>6928200</td>\n",
       "      <td>2229600</td>\n",
       "      <td>1899600</td>\n",
       "      <td>2492100</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1651800</td>\n",
       "      <td>...</td>\n",
       "      <td>1268700</td>\n",
       "      <td>6294300</td>\n",
       "      <td>1935000</td>\n",
       "      <td>10633800</td>\n",
       "      <td>3195900</td>\n",
       "      <td>1305000</td>\n",
       "      <td>3027000</td>\n",
       "      <td>2423100</td>\n",
       "      <td>1835700</td>\n",
       "      <td>1757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>20071208</td>\n",
       "      <td>1683900</td>\n",
       "      <td>1832700</td>\n",
       "      <td>2153700</td>\n",
       "      <td>1853400</td>\n",
       "      <td>2145600</td>\n",
       "      <td>1705200</td>\n",
       "      <td>1886100</td>\n",
       "      <td>1878000</td>\n",
       "      <td>1591200</td>\n",
       "      <td>...</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1853400</td>\n",
       "      <td>1761000</td>\n",
       "      <td>1690200</td>\n",
       "      <td>2251500</td>\n",
       "      <td>1556100</td>\n",
       "      <td>4414800</td>\n",
       "      <td>5186100</td>\n",
       "      <td>1826400</td>\n",
       "      <td>1762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>20071209</td>\n",
       "      <td>1512300</td>\n",
       "      <td>1394400</td>\n",
       "      <td>1685700</td>\n",
       "      <td>1443900</td>\n",
       "      <td>3815100</td>\n",
       "      <td>5288700</td>\n",
       "      <td>2505600</td>\n",
       "      <td>1129800</td>\n",
       "      <td>3070200</td>\n",
       "      <td>...</td>\n",
       "      <td>1245900</td>\n",
       "      <td>1558800</td>\n",
       "      <td>2174100</td>\n",
       "      <td>1551300</td>\n",
       "      <td>2354400</td>\n",
       "      <td>1321500</td>\n",
       "      <td>1269300</td>\n",
       "      <td>1362300</td>\n",
       "      <td>3344400</td>\n",
       "      <td>1480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>20071210</td>\n",
       "      <td>1729500</td>\n",
       "      <td>1530600</td>\n",
       "      <td>1451400</td>\n",
       "      <td>1747500</td>\n",
       "      <td>1628400</td>\n",
       "      <td>3313200</td>\n",
       "      <td>1207800</td>\n",
       "      <td>1018500</td>\n",
       "      <td>2247000</td>\n",
       "      <td>...</td>\n",
       "      <td>1927800</td>\n",
       "      <td>1509300</td>\n",
       "      <td>1086000</td>\n",
       "      <td>1509000</td>\n",
       "      <td>1559400</td>\n",
       "      <td>1415400</td>\n",
       "      <td>1537200</td>\n",
       "      <td>1957500</td>\n",
       "      <td>2050800</td>\n",
       "      <td>2024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>20071211</td>\n",
       "      <td>1512600</td>\n",
       "      <td>819600</td>\n",
       "      <td>1236900</td>\n",
       "      <td>1648800</td>\n",
       "      <td>2398800</td>\n",
       "      <td>2607600</td>\n",
       "      <td>1206600</td>\n",
       "      <td>1074300</td>\n",
       "      <td>1185600</td>\n",
       "      <td>...</td>\n",
       "      <td>1446900</td>\n",
       "      <td>1281600</td>\n",
       "      <td>1359000</td>\n",
       "      <td>1127400</td>\n",
       "      <td>1397700</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1028100</td>\n",
       "      <td>2350500</td>\n",
       "      <td>1860900</td>\n",
       "      <td>1549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5093</th>\n",
       "      <td>20071212</td>\n",
       "      <td>2126100</td>\n",
       "      <td>1477500</td>\n",
       "      <td>5262600</td>\n",
       "      <td>2496900</td>\n",
       "      <td>6317400</td>\n",
       "      <td>8295600</td>\n",
       "      <td>5603100</td>\n",
       "      <td>1671900</td>\n",
       "      <td>1977300</td>\n",
       "      <td>...</td>\n",
       "      <td>1392000</td>\n",
       "      <td>1669200</td>\n",
       "      <td>2820000</td>\n",
       "      <td>2087100</td>\n",
       "      <td>4227900</td>\n",
       "      <td>1423800</td>\n",
       "      <td>1272300</td>\n",
       "      <td>916500</td>\n",
       "      <td>4206300</td>\n",
       "      <td>1726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>20071213</td>\n",
       "      <td>3476400</td>\n",
       "      <td>3016200</td>\n",
       "      <td>8252400</td>\n",
       "      <td>3503700</td>\n",
       "      <td>11037600</td>\n",
       "      <td>11493600</td>\n",
       "      <td>5314800</td>\n",
       "      <td>2861700</td>\n",
       "      <td>3190500</td>\n",
       "      <td>...</td>\n",
       "      <td>2737500</td>\n",
       "      <td>3606900</td>\n",
       "      <td>4408800</td>\n",
       "      <td>3242400</td>\n",
       "      <td>4901700</td>\n",
       "      <td>4254600</td>\n",
       "      <td>3825000</td>\n",
       "      <td>3884700</td>\n",
       "      <td>10216500</td>\n",
       "      <td>3342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>20071214</td>\n",
       "      <td>1262700</td>\n",
       "      <td>1544700</td>\n",
       "      <td>921000</td>\n",
       "      <td>1227000</td>\n",
       "      <td>1224300</td>\n",
       "      <td>1073100</td>\n",
       "      <td>1342200</td>\n",
       "      <td>2343000</td>\n",
       "      <td>1903800</td>\n",
       "      <td>...</td>\n",
       "      <td>6358800</td>\n",
       "      <td>1354200</td>\n",
       "      <td>1438800</td>\n",
       "      <td>1148100</td>\n",
       "      <td>1734000</td>\n",
       "      <td>5733600</td>\n",
       "      <td>2664600</td>\n",
       "      <td>4408500</td>\n",
       "      <td>1123800</td>\n",
       "      <td>1885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>20071215</td>\n",
       "      <td>3074400</td>\n",
       "      <td>1500600</td>\n",
       "      <td>6650400</td>\n",
       "      <td>3486300</td>\n",
       "      <td>7247400</td>\n",
       "      <td>9481800</td>\n",
       "      <td>6552000</td>\n",
       "      <td>1593000</td>\n",
       "      <td>6715200</td>\n",
       "      <td>...</td>\n",
       "      <td>2708700</td>\n",
       "      <td>3045600</td>\n",
       "      <td>7341600</td>\n",
       "      <td>3783000</td>\n",
       "      <td>6811500</td>\n",
       "      <td>1209300</td>\n",
       "      <td>1220700</td>\n",
       "      <td>1414500</td>\n",
       "      <td>5080800</td>\n",
       "      <td>4438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>20071216</td>\n",
       "      <td>12523800</td>\n",
       "      <td>12006300</td>\n",
       "      <td>12210300</td>\n",
       "      <td>12167100</td>\n",
       "      <td>11643000</td>\n",
       "      <td>12363300</td>\n",
       "      <td>11934600</td>\n",
       "      <td>11676900</td>\n",
       "      <td>10769100</td>\n",
       "      <td>...</td>\n",
       "      <td>11114700</td>\n",
       "      <td>12364200</td>\n",
       "      <td>13882200</td>\n",
       "      <td>12572400</td>\n",
       "      <td>12260400</td>\n",
       "      <td>11368200</td>\n",
       "      <td>12465300</td>\n",
       "      <td>11695800</td>\n",
       "      <td>11119800</td>\n",
       "      <td>11652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>20071217</td>\n",
       "      <td>12089700</td>\n",
       "      <td>11742900</td>\n",
       "      <td>12130200</td>\n",
       "      <td>11095800</td>\n",
       "      <td>10793100</td>\n",
       "      <td>11952600</td>\n",
       "      <td>10750200</td>\n",
       "      <td>11702700</td>\n",
       "      <td>10053900</td>\n",
       "      <td>...</td>\n",
       "      <td>10693800</td>\n",
       "      <td>12084300</td>\n",
       "      <td>11665800</td>\n",
       "      <td>11436600</td>\n",
       "      <td>11346600</td>\n",
       "      <td>11470500</td>\n",
       "      <td>12253200</td>\n",
       "      <td>11610000</td>\n",
       "      <td>10333500</td>\n",
       "      <td>10910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>20071218</td>\n",
       "      <td>12300600</td>\n",
       "      <td>11602500</td>\n",
       "      <td>12087900</td>\n",
       "      <td>11866200</td>\n",
       "      <td>11452500</td>\n",
       "      <td>11811900</td>\n",
       "      <td>11696700</td>\n",
       "      <td>11232600</td>\n",
       "      <td>10142100</td>\n",
       "      <td>...</td>\n",
       "      <td>10272300</td>\n",
       "      <td>12104700</td>\n",
       "      <td>11622300</td>\n",
       "      <td>12438300</td>\n",
       "      <td>10248000</td>\n",
       "      <td>9291600</td>\n",
       "      <td>11736900</td>\n",
       "      <td>10968300</td>\n",
       "      <td>10684800</td>\n",
       "      <td>10991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>20071219</td>\n",
       "      <td>12206100</td>\n",
       "      <td>11478000</td>\n",
       "      <td>12020400</td>\n",
       "      <td>11741100</td>\n",
       "      <td>10871400</td>\n",
       "      <td>11946600</td>\n",
       "      <td>11505600</td>\n",
       "      <td>11184600</td>\n",
       "      <td>9769800</td>\n",
       "      <td>...</td>\n",
       "      <td>9334800</td>\n",
       "      <td>11976900</td>\n",
       "      <td>11744400</td>\n",
       "      <td>12171600</td>\n",
       "      <td>11703000</td>\n",
       "      <td>11015700</td>\n",
       "      <td>11782800</td>\n",
       "      <td>11303100</td>\n",
       "      <td>10367700</td>\n",
       "      <td>9920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>20071220</td>\n",
       "      <td>12275700</td>\n",
       "      <td>11530800</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11865600</td>\n",
       "      <td>11614800</td>\n",
       "      <td>11707500</td>\n",
       "      <td>11698800</td>\n",
       "      <td>11053200</td>\n",
       "      <td>10116900</td>\n",
       "      <td>...</td>\n",
       "      <td>10092600</td>\n",
       "      <td>10224300</td>\n",
       "      <td>11861700</td>\n",
       "      <td>12374400</td>\n",
       "      <td>11761500</td>\n",
       "      <td>8301000</td>\n",
       "      <td>8917500</td>\n",
       "      <td>7947300</td>\n",
       "      <td>10679100</td>\n",
       "      <td>11154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>20071221</td>\n",
       "      <td>9737700</td>\n",
       "      <td>9611100</td>\n",
       "      <td>8948400</td>\n",
       "      <td>8915100</td>\n",
       "      <td>10750800</td>\n",
       "      <td>8196000</td>\n",
       "      <td>10429500</td>\n",
       "      <td>9107700</td>\n",
       "      <td>8867700</td>\n",
       "      <td>...</td>\n",
       "      <td>8824800</td>\n",
       "      <td>9253500</td>\n",
       "      <td>11017500</td>\n",
       "      <td>10610400</td>\n",
       "      <td>10148700</td>\n",
       "      <td>8847600</td>\n",
       "      <td>9985200</td>\n",
       "      <td>9971700</td>\n",
       "      <td>9173100</td>\n",
       "      <td>9162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>20071222</td>\n",
       "      <td>1958400</td>\n",
       "      <td>2235300</td>\n",
       "      <td>2364900</td>\n",
       "      <td>2108700</td>\n",
       "      <td>5511000</td>\n",
       "      <td>8464800</td>\n",
       "      <td>2461800</td>\n",
       "      <td>677100</td>\n",
       "      <td>1535400</td>\n",
       "      <td>...</td>\n",
       "      <td>597600</td>\n",
       "      <td>1733100</td>\n",
       "      <td>2578200</td>\n",
       "      <td>1802400</td>\n",
       "      <td>2522700</td>\n",
       "      <td>2739600</td>\n",
       "      <td>6989100</td>\n",
       "      <td>7298400</td>\n",
       "      <td>4391700</td>\n",
       "      <td>902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5104</th>\n",
       "      <td>20071223</td>\n",
       "      <td>12325200</td>\n",
       "      <td>11827200</td>\n",
       "      <td>11969100</td>\n",
       "      <td>11844900</td>\n",
       "      <td>11698500</td>\n",
       "      <td>10371300</td>\n",
       "      <td>11761200</td>\n",
       "      <td>11413200</td>\n",
       "      <td>10904400</td>\n",
       "      <td>...</td>\n",
       "      <td>11088900</td>\n",
       "      <td>12319800</td>\n",
       "      <td>12013800</td>\n",
       "      <td>12653400</td>\n",
       "      <td>12035100</td>\n",
       "      <td>11593500</td>\n",
       "      <td>12151200</td>\n",
       "      <td>11898000</td>\n",
       "      <td>9895800</td>\n",
       "      <td>11434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5105</th>\n",
       "      <td>20071224</td>\n",
       "      <td>12225900</td>\n",
       "      <td>11599500</td>\n",
       "      <td>12061500</td>\n",
       "      <td>11865300</td>\n",
       "      <td>11774700</td>\n",
       "      <td>11851800</td>\n",
       "      <td>11720400</td>\n",
       "      <td>11379600</td>\n",
       "      <td>10454100</td>\n",
       "      <td>...</td>\n",
       "      <td>10995600</td>\n",
       "      <td>12055800</td>\n",
       "      <td>12075900</td>\n",
       "      <td>12308100</td>\n",
       "      <td>11938200</td>\n",
       "      <td>11466300</td>\n",
       "      <td>11734800</td>\n",
       "      <td>11522100</td>\n",
       "      <td>10793700</td>\n",
       "      <td>11464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5106</th>\n",
       "      <td>20071225</td>\n",
       "      <td>12256500</td>\n",
       "      <td>11818200</td>\n",
       "      <td>12093000</td>\n",
       "      <td>11823000</td>\n",
       "      <td>11540400</td>\n",
       "      <td>9601500</td>\n",
       "      <td>11670000</td>\n",
       "      <td>11565300</td>\n",
       "      <td>10189500</td>\n",
       "      <td>...</td>\n",
       "      <td>11088600</td>\n",
       "      <td>12075300</td>\n",
       "      <td>11924400</td>\n",
       "      <td>12457800</td>\n",
       "      <td>11772600</td>\n",
       "      <td>11641200</td>\n",
       "      <td>12170400</td>\n",
       "      <td>11836800</td>\n",
       "      <td>10649700</td>\n",
       "      <td>11517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5107</th>\n",
       "      <td>20071226</td>\n",
       "      <td>1851300</td>\n",
       "      <td>1551600</td>\n",
       "      <td>3227700</td>\n",
       "      <td>1722900</td>\n",
       "      <td>7181100</td>\n",
       "      <td>11574900</td>\n",
       "      <td>3165000</td>\n",
       "      <td>3704100</td>\n",
       "      <td>2848500</td>\n",
       "      <td>...</td>\n",
       "      <td>1938300</td>\n",
       "      <td>2681400</td>\n",
       "      <td>2894400</td>\n",
       "      <td>3147300</td>\n",
       "      <td>3001200</td>\n",
       "      <td>1406400</td>\n",
       "      <td>1595400</td>\n",
       "      <td>1460700</td>\n",
       "      <td>6298200</td>\n",
       "      <td>2382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>20071227</td>\n",
       "      <td>1408500</td>\n",
       "      <td>1371000</td>\n",
       "      <td>1742400</td>\n",
       "      <td>1389300</td>\n",
       "      <td>1349400</td>\n",
       "      <td>1466400</td>\n",
       "      <td>1245900</td>\n",
       "      <td>1674000</td>\n",
       "      <td>922500</td>\n",
       "      <td>...</td>\n",
       "      <td>1511100</td>\n",
       "      <td>1220700</td>\n",
       "      <td>1083300</td>\n",
       "      <td>1887900</td>\n",
       "      <td>1467900</td>\n",
       "      <td>1404000</td>\n",
       "      <td>1510500</td>\n",
       "      <td>1747500</td>\n",
       "      <td>1079100</td>\n",
       "      <td>1100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5109</th>\n",
       "      <td>20071228</td>\n",
       "      <td>10060800</td>\n",
       "      <td>6581700</td>\n",
       "      <td>12027300</td>\n",
       "      <td>11312100</td>\n",
       "      <td>9665100</td>\n",
       "      <td>7993800</td>\n",
       "      <td>11962200</td>\n",
       "      <td>9010500</td>\n",
       "      <td>5747700</td>\n",
       "      <td>...</td>\n",
       "      <td>3735600</td>\n",
       "      <td>7612500</td>\n",
       "      <td>6875700</td>\n",
       "      <td>12105300</td>\n",
       "      <td>11741700</td>\n",
       "      <td>3844800</td>\n",
       "      <td>7136100</td>\n",
       "      <td>5670000</td>\n",
       "      <td>7592400</td>\n",
       "      <td>6705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>20071229</td>\n",
       "      <td>11388000</td>\n",
       "      <td>11353800</td>\n",
       "      <td>11946900</td>\n",
       "      <td>9662400</td>\n",
       "      <td>10938300</td>\n",
       "      <td>11315100</td>\n",
       "      <td>11402400</td>\n",
       "      <td>10683300</td>\n",
       "      <td>8954400</td>\n",
       "      <td>...</td>\n",
       "      <td>10457700</td>\n",
       "      <td>10316700</td>\n",
       "      <td>10559700</td>\n",
       "      <td>11873100</td>\n",
       "      <td>11369400</td>\n",
       "      <td>10711500</td>\n",
       "      <td>11822100</td>\n",
       "      <td>11594100</td>\n",
       "      <td>9687900</td>\n",
       "      <td>10586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>20071230</td>\n",
       "      <td>12441000</td>\n",
       "      <td>11883300</td>\n",
       "      <td>12409200</td>\n",
       "      <td>12155400</td>\n",
       "      <td>11937600</td>\n",
       "      <td>12314100</td>\n",
       "      <td>12006000</td>\n",
       "      <td>11695800</td>\n",
       "      <td>10249500</td>\n",
       "      <td>...</td>\n",
       "      <td>11152800</td>\n",
       "      <td>12258900</td>\n",
       "      <td>12251100</td>\n",
       "      <td>12754200</td>\n",
       "      <td>12230100</td>\n",
       "      <td>11771100</td>\n",
       "      <td>12162900</td>\n",
       "      <td>11933400</td>\n",
       "      <td>10940700</td>\n",
       "      <td>11281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>20071231</td>\n",
       "      <td>12450300</td>\n",
       "      <td>12104100</td>\n",
       "      <td>12015600</td>\n",
       "      <td>12516600</td>\n",
       "      <td>8480100</td>\n",
       "      <td>9302400</td>\n",
       "      <td>11198100</td>\n",
       "      <td>9687300</td>\n",
       "      <td>9957900</td>\n",
       "      <td>...</td>\n",
       "      <td>10182000</td>\n",
       "      <td>11819700</td>\n",
       "      <td>10313700</td>\n",
       "      <td>12808200</td>\n",
       "      <td>11411700</td>\n",
       "      <td>9657300</td>\n",
       "      <td>11844600</td>\n",
       "      <td>11749800</td>\n",
       "      <td>8191800</td>\n",
       "      <td>11211300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5113 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0     19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1     19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2     19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3     19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4     19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "5     19940106   6639000   6817200   8157900   7673100   3500400   2245200   \n",
       "6     19940107  13244700  12418800  12369900  12873000  12181800   9877800   \n",
       "7     19940108  12927900  12375600  12634500  13066500  11608800  11545200   \n",
       "8     19940109  12600300  11601000  12156000  12464700  10866000  11295300   \n",
       "9     19940110   6406500   3935700  12321900   8164800  11328600  10785000   \n",
       "10    19940111  12743400   7137000  12966300  12774600  12005100  11424900   \n",
       "11    19940112  10453500   7371000  12855300  11448000  11493000  11794200   \n",
       "12    19940113  12985200  12510600  13198500  12726900  12289200  12149100   \n",
       "13    19940114  13080000  12552000  13446600  13026600  12393000  12227700   \n",
       "14    19940115  11826300  11997300  11313300  11793300  10750200  10290600   \n",
       "15    19940116   1974000   1339800   3120600   1058700   7187100   9792900   \n",
       "16    19940117  13541700  13021200  13757100  13432800  12486600  11738100   \n",
       "17    19940118  13673700  13042200  13881000  13586100  13158300  12724200   \n",
       "18    19940119   6796800   8217000  13563300   7861800  13200900  13184700   \n",
       "19    19940120   5658900   4757700   1976100   4926000   3088800  10210500   \n",
       "20    19940121   7073400  10822800   4021800   6464100   4468500  11169300   \n",
       "21    19940122   3354000   2764800   2997900   2726100   8875800  12647400   \n",
       "22    19940123   2579700   2339100   6508500   5418600  12936900  12553500   \n",
       "23    19940124   2387700   2096700   6390000   3164700   9348300   7642200   \n",
       "24    19940125   8390700   8621700  11639100  11404200  11722200  13093500   \n",
       "25    19940126   7326600   8104500  10221900   8622300  12958500   7973100   \n",
       "26    19940127  10743900   8913000  13336500  11616000  12311700   9855300   \n",
       "27    19940128  12812100  13680900  12890700  13834800  13994700  13548000   \n",
       "28    19940129   9065100   5018100  10373700  10304100  13456800  13098600   \n",
       "29    19940130   3954900   7685700   3863100   4220700   5629200   2520000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5083  20071202  12177000  11132400  12620100  11796600  11907000  12604500   \n",
       "5084  20071203  13453800  12737700  12828300  12809700  12264300  12439200   \n",
       "5085  20071204  12525900  10054800  12328200  12057900  12027900  11833500   \n",
       "5086  20071205  10543500   8691900  12200100  11790300   9699300  10742100   \n",
       "5087  20071206   6214500   5707500   6339900   6242400   7813800   8302800   \n",
       "5088  20071207   8417400   6139800   6981000   6928200   2229600   1899600   \n",
       "5089  20071208   1683900   1832700   2153700   1853400   2145600   1705200   \n",
       "5090  20071209   1512300   1394400   1685700   1443900   3815100   5288700   \n",
       "5091  20071210   1729500   1530600   1451400   1747500   1628400   3313200   \n",
       "5092  20071211   1512600    819600   1236900   1648800   2398800   2607600   \n",
       "5093  20071212   2126100   1477500   5262600   2496900   6317400   8295600   \n",
       "5094  20071213   3476400   3016200   8252400   3503700  11037600  11493600   \n",
       "5095  20071214   1262700   1544700    921000   1227000   1224300   1073100   \n",
       "5096  20071215   3074400   1500600   6650400   3486300   7247400   9481800   \n",
       "5097  20071216  12523800  12006300  12210300  12167100  11643000  12363300   \n",
       "5098  20071217  12089700  11742900  12130200  11095800  10793100  11952600   \n",
       "5099  20071218  12300600  11602500  12087900  11866200  11452500  11811900   \n",
       "5100  20071219  12206100  11478000  12020400  11741100  10871400  11946600   \n",
       "5101  20071220  12275700  11530800  12134400  11865600  11614800  11707500   \n",
       "5102  20071221   9737700   9611100   8948400   8915100  10750800   8196000   \n",
       "5103  20071222   1958400   2235300   2364900   2108700   5511000   8464800   \n",
       "5104  20071223  12325200  11827200  11969100  11844900  11698500  10371300   \n",
       "5105  20071224  12225900  11599500  12061500  11865300  11774700  11851800   \n",
       "5106  20071225  12256500  11818200  12093000  11823000  11540400   9601500   \n",
       "5107  20071226   1851300   1551600   3227700   1722900   7181100  11574900   \n",
       "5108  20071227   1408500   1371000   1742400   1389300   1349400   1466400   \n",
       "5109  20071228  10060800   6581700  12027300  11312100   9665100   7993800   \n",
       "5110  20071229  11388000  11353800  11946900   9662400  10938300  11315100   \n",
       "5111  20071230  12441000  11883300  12409200  12155400  11937600  12314100   \n",
       "5112  20071231  12450300  12104100  12015600  12516600   8480100   9302400   \n",
       "\n",
       "          BESS      BIXB      BLAC  ...      VINI      WASH      WATO  \\\n",
       "0     11487900  11182800  10848300  ...  10771800  12116400  11308800   \n",
       "1      9235200   3963300   3318300  ...   4314300  10733400   9154800   \n",
       "2     11895900   4512600   5266500  ...   2976900  11775000  10700400   \n",
       "3     12186600   3212700   8270100  ...   3476400  12159600  11907000   \n",
       "4      6411300   9566100   8009400  ...   6393300  11419500   7334400   \n",
       "5      9719400   6137100   4328700  ...   5257200   6687000   5631600   \n",
       "6     12114300  12175200  11836500  ...  11495100  12486300  12098700   \n",
       "7     12029400  12217500  11505300  ...  11284800  12471300  12072300   \n",
       "8     11937900  10443300   9218400  ...   8755800  12391800  11369400   \n",
       "9     12081600   1873800   9658800  ...   3155100   3879900  11709300   \n",
       "10    12149400   2835600   2574000  ...   3411600  12371400  11973000   \n",
       "11    11780400   6759900   3571800  ...   2370600   8449200  11049000   \n",
       "12    12467100   9930900   9628500  ...   8013000  12463200  12099900   \n",
       "13    12488700  10799100  10770900  ...   6584100  12793800  12221100   \n",
       "14    11184600  12337500  11489700  ...   8565600  11604000  12094800   \n",
       "15     1405500    711900   1133700  ...    640200   1401300   1288500   \n",
       "16    12719700   7774200   9512400  ...  10155300  13079400  12507900   \n",
       "17    12984000   9400800  11339400  ...   8709300  13125900  12694500   \n",
       "18    12884700  10399500  12214500  ...   8323200   7311600  12622800   \n",
       "19     3547200  10607100   5247900  ...   4558800   5358300   5883600   \n",
       "20     5096100  12288000  11910300  ...  11721600   9132300   7908900   \n",
       "21     2881500   3448500   8096400  ...   7866600   2923200   4995000   \n",
       "22     9266400   1948800   6969000  ...   2103000   2631900  10932900   \n",
       "23     5437500   1742100   3155100  ...   1695300   2771400   5281800   \n",
       "24    11776500   7530900   4177200  ...   6538500   9204900   8374800   \n",
       "25    11801400   5955000   7979100  ...   3879300   8131500  11583600   \n",
       "26    13071000  10467300  11326800  ...   6998400  10048800  12608400   \n",
       "27    13434300  11479200   8445900  ...   9601500  13080300  11967900   \n",
       "28    13869900   8671200  12211500  ...  12647400   7510500  13708200   \n",
       "29     5959500  11066400   6694800  ...   9793800   6205500   5371200   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5083  11340600   7562100   8957100  ...   5517600  10997700  10304400   \n",
       "5084  12496800  12446100  11181000  ...  11936700  13204500  12599400   \n",
       "5085  11980200  10532100   9660300  ...  10314000  12219900  12349200   \n",
       "5086  12017700  10608900  10084500  ...   7831500  10752600  11216700   \n",
       "5087   6486600   5260200   2649300  ...   2001300   7011000   6879900   \n",
       "5088   2492100   1154700   1651800  ...   1268700   6294300   1935000   \n",
       "5089   1886100   1878000   1591200  ...   1500000   1853400   1761000   \n",
       "5090   2505600   1129800   3070200  ...   1245900   1558800   2174100   \n",
       "5091   1207800   1018500   2247000  ...   1927800   1509300   1086000   \n",
       "5092   1206600   1074300   1185600  ...   1446900   1281600   1359000   \n",
       "5093   5603100   1671900   1977300  ...   1392000   1669200   2820000   \n",
       "5094   5314800   2861700   3190500  ...   2737500   3606900   4408800   \n",
       "5095   1342200   2343000   1903800  ...   6358800   1354200   1438800   \n",
       "5096   6552000   1593000   6715200  ...   2708700   3045600   7341600   \n",
       "5097  11934600  11676900  10769100  ...  11114700  12364200  13882200   \n",
       "5098  10750200  11702700  10053900  ...  10693800  12084300  11665800   \n",
       "5099  11696700  11232600  10142100  ...  10272300  12104700  11622300   \n",
       "5100  11505600  11184600   9769800  ...   9334800  11976900  11744400   \n",
       "5101  11698800  11053200  10116900  ...  10092600  10224300  11861700   \n",
       "5102  10429500   9107700   8867700  ...   8824800   9253500  11017500   \n",
       "5103   2461800    677100   1535400  ...    597600   1733100   2578200   \n",
       "5104  11761200  11413200  10904400  ...  11088900  12319800  12013800   \n",
       "5105  11720400  11379600  10454100  ...  10995600  12055800  12075900   \n",
       "5106  11670000  11565300  10189500  ...  11088600  12075300  11924400   \n",
       "5107   3165000   3704100   2848500  ...   1938300   2681400   2894400   \n",
       "5108   1245900   1674000    922500  ...   1511100   1220700   1083300   \n",
       "5109  11962200   9010500   5747700  ...   3735600   7612500   6875700   \n",
       "5110  11402400  10683300   8954400  ...  10457700  10316700  10559700   \n",
       "5111  12006000  11695800  10249500  ...  11152800  12258900  12251100   \n",
       "5112  11198100   9687300   9957900  ...  10182000  11819700  10313700   \n",
       "\n",
       "          WAUR      WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0     12361800  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1     12041400   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2     12687300  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3     12953100  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4     10178700   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "5      7990500   9402600   8463600   7323900   7113600   2124600   3324300  \n",
       "6     13191300  11855100  11494800  12620400  12380700  10944900  11403600  \n",
       "7     10429500  11939100  11280300  12419700  12225900  11029200  11268900  \n",
       "8     11324400  11498700   9737100  11985000  11454900  10518300   8577300  \n",
       "9      3808500  11526900   2064300   7641000   2282400  10859700   2520600  \n",
       "10    12872400  11970900   1908000   1998300   1577100  11256600   2923500  \n",
       "11     8650500  11623200   5994300   4441500   6324900  10424100   7131000  \n",
       "12    13268400  12205500  10277100   9863700   8588400  11655300  10062300  \n",
       "13    12990000  12166800   5569500  11459400  11072100  11614200   9288600  \n",
       "14    11343900  10906500  11029500  12093300  12119100  10898700  10905000  \n",
       "15     1257600   1125300    739800    756900    469500   3777300    748500  \n",
       "16    13869000  12336300  11681100   9247500  10576200  11586900   9893100  \n",
       "17    13959900  12517800  12465000  12720600  10228200  12076800  10843800  \n",
       "18     5567400  12464700  14057400  11255400  12093900  12372300  10474200  \n",
       "19     2109900   4254900   6372900   8222700   9356400   6758100   7672200  \n",
       "20     3649500   6578100  11701200  12758400  12867300   5543700  11705100  \n",
       "21     1614900   3245400   3254100   3469800   2306100   9792600   5962200  \n",
       "22     2564700   8967900   1685400   2399700   2453400  12991800   3377100  \n",
       "23     3616500   4453500   1114200   2427300   1808400   9188100   2060700  \n",
       "24    12530400  10918200   6499500   7495200   6218700  12260100   7261800  \n",
       "25     8124000  11062800   3655800   6165600   3794100  11837400   6168900  \n",
       "26    11251800  12623100   5641200   5928900   4412100  12267900  11280000  \n",
       "27    12759300  12837300   9696300  12404400  10782600  11917500   8725500  \n",
       "28     5198700  13585800   7622400   5036400   4905000  12272700  11245500  \n",
       "29     6331800   6069600  12349500   9744900  10736400   5056800   7212000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5083  12716700  11124600   3845700   8625000   7135500  10745100   7718700  \n",
       "5084  13403400  12964500  12735900  13133100  12933900  11300400  12414000  \n",
       "5085  12531600  12132600  10984800  12141600  11658900  10941300  10495500  \n",
       "5086  10968900  12183600   9241500   8371200   8967900   9399900  11064900  \n",
       "5087   7605000   7298700   3600900   5550900   5042700   6977700   3435000  \n",
       "5088  10633800   3195900   1305000   3027000   2423100   1835700   1757400  \n",
       "5089   1690200   2251500   1556100   4414800   5186100   1826400   1762500  \n",
       "5090   1551300   2354400   1321500   1269300   1362300   3344400   1480200  \n",
       "5091   1509000   1559400   1415400   1537200   1957500   2050800   2024400  \n",
       "5092   1127400   1397700   1154700   1028100   2350500   1860900   1549800  \n",
       "5093   2087100   4227900   1423800   1272300    916500   4206300   1726800  \n",
       "5094   3242400   4901700   4254600   3825000   3884700  10216500   3342000  \n",
       "5095   1148100   1734000   5733600   2664600   4408500   1123800   1885800  \n",
       "5096   3783000   6811500   1209300   1220700   1414500   5080800   4438800  \n",
       "5097  12572400  12260400  11368200  12465300  11695800  11119800  11652000  \n",
       "5098  11436600  11346600  11470500  12253200  11610000  10333500  10910400  \n",
       "5099  12438300  10248000   9291600  11736900  10968300  10684800  10991700  \n",
       "5100  12171600  11703000  11015700  11782800  11303100  10367700   9920100  \n",
       "5101  12374400  11761500   8301000   8917500   7947300  10679100  11154000  \n",
       "5102  10610400  10148700   8847600   9985200   9971700   9173100   9162900  \n",
       "5103   1802400   2522700   2739600   6989100   7298400   4391700    902400  \n",
       "5104  12653400  12035100  11593500  12151200  11898000   9895800  11434800  \n",
       "5105  12308100  11938200  11466300  11734800  11522100  10793700  11464800  \n",
       "5106  12457800  11772600  11641200  12170400  11836800  10649700  11517300  \n",
       "5107   3147300   3001200   1406400   1595400   1460700   6298200   2382300  \n",
       "5108   1887900   1467900   1404000   1510500   1747500   1079100   1100400  \n",
       "5109  12105300  11741700   3844800   7136100   5670000   7592400   6705900  \n",
       "5110  11873100  11369400  10711500  11822100  11594100   9687900  10586400  \n",
       "5111  12754200  12230100  11771100  12162900  11933400  10940700  11281800  \n",
       "5112  12808200  11411700   9657300  11844600  11749800   8191800  11211300  \n",
       "\n",
       "[5113 rows x 99 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0fa66181bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "validation_set = get_split(df, 'D', split_type = 'validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = get_split(train, 'D', split_type = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': '1994-01-01 00:00:00',\n",
       "  'target': [17412600,\n",
       "   17187300,\n",
       "   16946400,\n",
       "   16883100,\n",
       "   16567500,\n",
       "   5341500,\n",
       "   16074600,\n",
       "   15863400,\n",
       "   11075700,\n",
       "   15723600,\n",
       "   13943700,\n",
       "   6248100,\n",
       "   4716600,\n",
       "   5364900,\n",
       "   8388300,\n",
       "   7826100,\n",
       "   2207100,\n",
       "   5768100,\n",
       "   6405600,\n",
       "   2123100,\n",
       "   3197700,\n",
       "   4359000,\n",
       "   7564500,\n",
       "   6014400,\n",
       "   1881300,\n",
       "   2334900,\n",
       "   11250900,\n",
       "   13417800,\n",
       "   2506200,\n",
       "   8445300,\n",
       "   13469700,\n",
       "   13712400,\n",
       "   13110300,\n",
       "   8187900,\n",
       "   13126200,\n",
       "   13004700,\n",
       "   9901200,\n",
       "   10989600,\n",
       "   8441400,\n",
       "   12669600,\n",
       "   12648300,\n",
       "   12414900,\n",
       "   11738100,\n",
       "   2874900,\n",
       "   11244900,\n",
       "   11052600,\n",
       "   12349200,\n",
       "   11243700,\n",
       "   11311800,\n",
       "   11031600,\n",
       "   1225200,\n",
       "   11949900,\n",
       "   11509200,\n",
       "   978000,\n",
       "   4112400,\n",
       "   10108500,\n",
       "   8343300,\n",
       "   11750400,\n",
       "   11390100,\n",
       "   11635500,\n",
       "   11653200,\n",
       "   11037600,\n",
       "   11112000,\n",
       "   4201200,\n",
       "   11535600,\n",
       "   10921500,\n",
       "   10965600,\n",
       "   4985700,\n",
       "   2643900,\n",
       "   10571100,\n",
       "   11852400,\n",
       "   11770500,\n",
       "   11623200,\n",
       "   5019600,\n",
       "   11444700,\n",
       "   9840300,\n",
       "   10545000,\n",
       "   11651700,\n",
       "   12263700,\n",
       "   7391100,\n",
       "   11423700,\n",
       "   11885700,\n",
       "   12102000,\n",
       "   11874600,\n",
       "   6425100,\n",
       "   6237000,\n",
       "   9430200,\n",
       "   6875100,\n",
       "   1824600,\n",
       "   1956300,\n",
       "   7280100,\n",
       "   13231800,\n",
       "   11670900,\n",
       "   9030600,\n",
       "   13126200,\n",
       "   12843900,\n",
       "   6564600,\n",
       "   13510800,\n",
       "   3535800,\n",
       "   14232900,\n",
       "   13588800,\n",
       "   7439100,\n",
       "   14300100,\n",
       "   3674700,\n",
       "   1204800,\n",
       "   14812200,\n",
       "   11416500,\n",
       "   1043400,\n",
       "   1949100,\n",
       "   12849300,\n",
       "   15569100,\n",
       "   10707600,\n",
       "   3027300,\n",
       "   11967000,\n",
       "   2736300,\n",
       "   15044100,\n",
       "   6067500,\n",
       "   6213900,\n",
       "   12419400,\n",
       "   14732100,\n",
       "   16504200,\n",
       "   13870800,\n",
       "   14771100,\n",
       "   14781600,\n",
       "   17162100,\n",
       "   12979200,\n",
       "   3542700,\n",
       "   2047200,\n",
       "   13183800,\n",
       "   17646900,\n",
       "   18418800,\n",
       "   5924400,\n",
       "   12360300,\n",
       "   18781200,\n",
       "   8481300,\n",
       "   1146600,\n",
       "   3128400,\n",
       "   14432400,\n",
       "   19879200,\n",
       "   20264100,\n",
       "   18618600,\n",
       "   21096900,\n",
       "   20181300,\n",
       "   14267100,\n",
       "   9411300,\n",
       "   3017700,\n",
       "   17495100,\n",
       "   7210500,\n",
       "   16476900,\n",
       "   20935500,\n",
       "   21237000,\n",
       "   16640400,\n",
       "   17032500,\n",
       "   15575400,\n",
       "   10649700,\n",
       "   20110200,\n",
       "   7078800,\n",
       "   14295600,\n",
       "   10215900,\n",
       "   7789200,\n",
       "   20140500,\n",
       "   23717100,\n",
       "   24411900,\n",
       "   23704800,\n",
       "   23746500,\n",
       "   21577200,\n",
       "   23265300,\n",
       "   21837900,\n",
       "   13801200,\n",
       "   10614900,\n",
       "   8971500,\n",
       "   22868400,\n",
       "   16109100,\n",
       "   2610900,\n",
       "   20630400,\n",
       "   17452500,\n",
       "   22936800,\n",
       "   26136300,\n",
       "   25672500,\n",
       "   25449300,\n",
       "   23372700,\n",
       "   13765500,\n",
       "   6854100,\n",
       "   24428100,\n",
       "   22300200,\n",
       "   25803000,\n",
       "   4048800,\n",
       "   14927100,\n",
       "   23465400,\n",
       "   26349600,\n",
       "   26766300,\n",
       "   19307100,\n",
       "   21285600,\n",
       "   7319100,\n",
       "   12789000,\n",
       "   27361500,\n",
       "   27419700,\n",
       "   27917100,\n",
       "   27680400,\n",
       "   21023100,\n",
       "   20709600,\n",
       "   24956100,\n",
       "   24422700,\n",
       "   19665600,\n",
       "   17163000,\n",
       "   20949000,\n",
       "   5761800,\n",
       "   20375700,\n",
       "   28247700,\n",
       "   20102400,\n",
       "   26534400,\n",
       "   20755500,\n",
       "   27054600,\n",
       "   27054600,\n",
       "   21406500,\n",
       "   21733200,\n",
       "   27244800,\n",
       "   24132000,\n",
       "   8185200,\n",
       "   15823500,\n",
       "   7815900,\n",
       "   28124100,\n",
       "   19725300,\n",
       "   27951900,\n",
       "   29661900,\n",
       "   29438100,\n",
       "   22469700,\n",
       "   16734600,\n",
       "   19259700,\n",
       "   21702300,\n",
       "   24736200,\n",
       "   16493100,\n",
       "   10609800,\n",
       "   6051300,\n",
       "   17298300,\n",
       "   24472500,\n",
       "   25282500,\n",
       "   25663800,\n",
       "   27652200,\n",
       "   18786900,\n",
       "   26203200,\n",
       "   21834900,\n",
       "   11501100,\n",
       "   13066800,\n",
       "   20665500,\n",
       "   20773200,\n",
       "   16137000,\n",
       "   29311200,\n",
       "   29473500,\n",
       "   28037400,\n",
       "   20485800,\n",
       "   21624000,\n",
       "   17232000,\n",
       "   11376900,\n",
       "   13869900,\n",
       "   20408100,\n",
       "   20417100,\n",
       "   27554700,\n",
       "   25294800,\n",
       "   23511900,\n",
       "   19968300,\n",
       "   24568500,\n",
       "   27474300,\n",
       "   22161300,\n",
       "   25421100,\n",
       "   27494700,\n",
       "   27892200,\n",
       "   27742200,\n",
       "   27900300,\n",
       "   27991200,\n",
       "   24756300,\n",
       "   21708600,\n",
       "   24743400,\n",
       "   27495900,\n",
       "   27421800,\n",
       "   27198000,\n",
       "   24853200,\n",
       "   19295700,\n",
       "   11206200,\n",
       "   7614300,\n",
       "   26469900,\n",
       "   16269000,\n",
       "   3567300,\n",
       "   5983200,\n",
       "   24680700,\n",
       "   25690200,\n",
       "   26577300,\n",
       "   26222100,\n",
       "   26160600,\n",
       "   17002800,\n",
       "   25483200,\n",
       "   15557400,\n",
       "   18937500,\n",
       "   10055700,\n",
       "   24428100,\n",
       "   17598600,\n",
       "   22085700,\n",
       "   23219700,\n",
       "   24703200,\n",
       "   8449800,\n",
       "   8145300,\n",
       "   24609000,\n",
       "   25087500,\n",
       "   23599200,\n",
       "   12705000,\n",
       "   12489900,\n",
       "   24102600,\n",
       "   20471100,\n",
       "   23104800,\n",
       "   22468800,\n",
       "   20201700,\n",
       "   24102600,\n",
       "   23380800,\n",
       "   19415700,\n",
       "   24704400,\n",
       "   18089700,\n",
       "   20316900,\n",
       "   16792800,\n",
       "   22481100,\n",
       "   22581600,\n",
       "   19866000,\n",
       "   20027400,\n",
       "   23729100,\n",
       "   24081600,\n",
       "   24396900,\n",
       "   24033300,\n",
       "   23324400,\n",
       "   21961800,\n",
       "   20596500,\n",
       "   21479400,\n",
       "   17715300,\n",
       "   16968300,\n",
       "   17342700,\n",
       "   20734500,\n",
       "   17908800,\n",
       "   17911200,\n",
       "   21125400,\n",
       "   20407500,\n",
       "   14832300,\n",
       "   12935700,\n",
       "   15536400,\n",
       "   17864700,\n",
       "   14229300,\n",
       "   19460400,\n",
       "   14463300,\n",
       "   17599500,\n",
       "   7373400,\n",
       "   12957600,\n",
       "   20302200,\n",
       "   15040200,\n",
       "   12049800,\n",
       "   13208400,\n",
       "   6418800,\n",
       "   9271800,\n",
       "   17259600,\n",
       "   4692600,\n",
       "   3283800,\n",
       "   2683500,\n",
       "   17382300,\n",
       "   17661900,\n",
       "   15912900,\n",
       "   17513400,\n",
       "   17460900,\n",
       "   8637600,\n",
       "   17264100,\n",
       "   16735500,\n",
       "   12564900,\n",
       "   14369100,\n",
       "   2716800,\n",
       "   16857300,\n",
       "   15631500,\n",
       "   6292800,\n",
       "   6046200,\n",
       "   2593800,\n",
       "   8279400,\n",
       "   12180600,\n",
       "   14397000,\n",
       "   2788800,\n",
       "   4912800,\n",
       "   1521900,\n",
       "   2619600,\n",
       "   15080700,\n",
       "   15059100,\n",
       "   15027900,\n",
       "   14304900,\n",
       "   13413000,\n",
       "   12486000,\n",
       "   7180500,\n",
       "   4422300,\n",
       "   5221500,\n",
       "   2713500,\n",
       "   3081000,\n",
       "   1929600,\n",
       "   2041800,\n",
       "   1948500,\n",
       "   3912300,\n",
       "   5261100,\n",
       "   5842500,\n",
       "   2691900,\n",
       "   1530900,\n",
       "   4283400,\n",
       "   11910300,\n",
       "   12303900,\n",
       "   12114900,\n",
       "   12329100,\n",
       "   6826500,\n",
       "   1210200,\n",
       "   10651800,\n",
       "   12588300,\n",
       "   12330300,\n",
       "   10459500,\n",
       "   10190700,\n",
       "   2570400,\n",
       "   3653100,\n",
       "   11653800,\n",
       "   8260500,\n",
       "   11336700,\n",
       "   11789400,\n",
       "   11463900,\n",
       "   11497200,\n",
       "   11600100,\n",
       "   11808600,\n",
       "   10635900,\n",
       "   9726000,\n",
       "   11531400,\n",
       "   11046600,\n",
       "   11411400,\n",
       "   11585400,\n",
       "   11196000,\n",
       "   4229700,\n",
       "   9209400,\n",
       "   11630400,\n",
       "   11118900,\n",
       "   11309100,\n",
       "   7236300,\n",
       "   9362700,\n",
       "   6981000,\n",
       "   7562100,\n",
       "   10464600,\n",
       "   4087200,\n",
       "   4105500,\n",
       "   2977500,\n",
       "   1215300,\n",
       "   1647000,\n",
       "   9465600,\n",
       "   9031500,\n",
       "   11736000,\n",
       "   10911900,\n",
       "   4901100,\n",
       "   3090600,\n",
       "   2867700,\n",
       "   12867300,\n",
       "   9649800,\n",
       "   9660300,\n",
       "   12709200,\n",
       "   12212700,\n",
       "   10286700,\n",
       "   10714800,\n",
       "   12967800,\n",
       "   10629300,\n",
       "   13485000,\n",
       "   13588200,\n",
       "   11471400,\n",
       "   13539300,\n",
       "   12701100,\n",
       "   3403500,\n",
       "   1806900,\n",
       "   3456900,\n",
       "   1300500,\n",
       "   2879100,\n",
       "   2625300,\n",
       "   5159100,\n",
       "   15113400,\n",
       "   13054200,\n",
       "   10463100,\n",
       "   3825000,\n",
       "   7112700,\n",
       "   3451200,\n",
       "   15468600,\n",
       "   13884600,\n",
       "   9039300,\n",
       "   2894400,\n",
       "   14977500,\n",
       "   15469200,\n",
       "   15858000,\n",
       "   13447500,\n",
       "   10492800,\n",
       "   5701200,\n",
       "   5082300,\n",
       "   17086200,\n",
       "   16060200,\n",
       "   14715600,\n",
       "   2129400,\n",
       "   17292600,\n",
       "   14264100,\n",
       "   7521000,\n",
       "   4185000,\n",
       "   17981100,\n",
       "   11767500,\n",
       "   17359200,\n",
       "   6294600,\n",
       "   19101000,\n",
       "   11216100,\n",
       "   10654500,\n",
       "   15195900,\n",
       "   18930900,\n",
       "   16344300,\n",
       "   20302500,\n",
       "   20845200,\n",
       "   21272400,\n",
       "   21039600,\n",
       "   12290700,\n",
       "   3846900,\n",
       "   10726200,\n",
       "   21773700,\n",
       "   19973700,\n",
       "   18624600,\n",
       "   19062900,\n",
       "   20363400,\n",
       "   5255700,\n",
       "   13942800,\n",
       "   15795300,\n",
       "   16552200,\n",
       "   3959400,\n",
       "   18909900,\n",
       "   23232000,\n",
       "   16378800,\n",
       "   22051500,\n",
       "   16965300,\n",
       "   23586300,\n",
       "   24955200,\n",
       "   22140900,\n",
       "   21147300,\n",
       "   22431000,\n",
       "   5716800,\n",
       "   24207300,\n",
       "   22047900,\n",
       "   23550000,\n",
       "   13514700,\n",
       "   24289500,\n",
       "   25603500,\n",
       "   25212600,\n",
       "   25040700,\n",
       "   23207700,\n",
       "   22107300,\n",
       "   23494800,\n",
       "   10463100,\n",
       "   13292400,\n",
       "   17672400,\n",
       "   19092300,\n",
       "   24865500,\n",
       "   27204600,\n",
       "   21347700,\n",
       "   22963800,\n",
       "   27200700,\n",
       "   25702800,\n",
       "   25909500,\n",
       "   8908800,\n",
       "   27189000,\n",
       "   11530800,\n",
       "   6896400,\n",
       "   5570100,\n",
       "   6658800,\n",
       "   10806300,\n",
       "   24625800,\n",
       "   9836400,\n",
       "   17992500,\n",
       "   23928300,\n",
       "   24162300,\n",
       "   24038400,\n",
       "   10172400,\n",
       "   18181800,\n",
       "   27988500,\n",
       "   18794700,\n",
       "   21057900,\n",
       "   26783100,\n",
       "   22372200,\n",
       "   23151600,\n",
       "   28227300,\n",
       "   27994200,\n",
       "   27496500,\n",
       "   20253000,\n",
       "   25482300,\n",
       "   14771400,\n",
       "   17659800,\n",
       "   8206500,\n",
       "   13610700,\n",
       "   12516300,\n",
       "   14277000,\n",
       "   11245200,\n",
       "   25816200,\n",
       "   27147900,\n",
       "   20310900,\n",
       "   21003000,\n",
       "   10030500,\n",
       "   24468000,\n",
       "   26152200,\n",
       "   23285100,\n",
       "   21949200,\n",
       "   17139900,\n",
       "   22089300,\n",
       "   23553300,\n",
       "   20533500,\n",
       "   23254800,\n",
       "   18763500,\n",
       "   22404600,\n",
       "   24765300,\n",
       "   27452400,\n",
       "   27594600,\n",
       "   28172100,\n",
       "   28697400,\n",
       "   28043100,\n",
       "   26932200,\n",
       "   26037300,\n",
       "   26067900,\n",
       "   26480700,\n",
       "   27653700,\n",
       "   28026900,\n",
       "   28370100,\n",
       "   27792300,\n",
       "   18503100,\n",
       "   23150100,\n",
       "   28051200,\n",
       "   20945100,\n",
       "   20020800,\n",
       "   25678200,\n",
       "   18772800,\n",
       "   28136400,\n",
       "   27312000,\n",
       "   26578200,\n",
       "   26162700,\n",
       "   26925900,\n",
       "   25961700,\n",
       "   19216800,\n",
       "   24201300,\n",
       "   25859100,\n",
       "   26877000,\n",
       "   19012200,\n",
       "   23841600,\n",
       "   27464100,\n",
       "   27295500,\n",
       "   27378300,\n",
       "   26819400,\n",
       "   21832800,\n",
       "   23465700,\n",
       "   18300900,\n",
       "   22286700,\n",
       "   28458300,\n",
       "   28302900,\n",
       "   26890800,\n",
       "   26932200,\n",
       "   26653800,\n",
       "   26087700,\n",
       "   26110200,\n",
       "   21778200,\n",
       "   23165700,\n",
       "   16723200,\n",
       "   19678500,\n",
       "   14941200,\n",
       "   23424900,\n",
       "   23497200,\n",
       "   21048900,\n",
       "   23110800,\n",
       "   12648300,\n",
       "   5879700,\n",
       "   8742600,\n",
       "   6998100,\n",
       "   23789700,\n",
       "   25192500,\n",
       "   24472500,\n",
       "   24631500,\n",
       "   20253900,\n",
       "   22303500,\n",
       "   19127400,\n",
       "   24222300,\n",
       "   23527800,\n",
       "   24054900,\n",
       "   3354300,\n",
       "   20629200,\n",
       "   23314500,\n",
       "   22550100,\n",
       "   23978700,\n",
       "   23979000,\n",
       "   21546900,\n",
       "   23288400,\n",
       "   21977100,\n",
       "   23523000,\n",
       "   21744000,\n",
       "   21291900,\n",
       "   21665400,\n",
       "   21909300,\n",
       "   20935200,\n",
       "   5980200,\n",
       "   17374800,\n",
       "   21353100,\n",
       "   12294300,\n",
       "   16800300,\n",
       "   22908900,\n",
       "   19839600,\n",
       "   20916000,\n",
       "   21432300,\n",
       "   21034500,\n",
       "   21350700,\n",
       "   20617500,\n",
       "   19909800,\n",
       "   19736100,\n",
       "   20158500,\n",
       "   19358100,\n",
       "   18999600,\n",
       "   18578700,\n",
       "   19113600,\n",
       "   16828800,\n",
       "   14024100,\n",
       "   18148800,\n",
       "   15379200,\n",
       "   15630600,\n",
       "   15508500,\n",
       "   6083400,\n",
       "   14930100,\n",
       "   18298200,\n",
       "   17887800,\n",
       "   7477200,\n",
       "   13002000,\n",
       "   16363200,\n",
       "   15201300,\n",
       "   15946800,\n",
       "   15982500,\n",
       "   17003100,\n",
       "   16998000,\n",
       "   16887000,\n",
       "   16163100,\n",
       "   15624600,\n",
       "   16073100,\n",
       "   15581700,\n",
       "   6965700,\n",
       "   17231700,\n",
       "   16606200,\n",
       "   16045200,\n",
       "   15703200,\n",
       "   15628200,\n",
       "   11771400,\n",
       "   4958400,\n",
       "   8560500,\n",
       "   15712500,\n",
       "   11010048,\n",
       "   11010048,\n",
       "   12254700,\n",
       "   14910300,\n",
       "   15035700,\n",
       "   13860900,\n",
       "   11051100,\n",
       "   10116300,\n",
       "   13116900,\n",
       "   13769700,\n",
       "   12937500,\n",
       "   13844100,\n",
       "   13421400,\n",
       "   13421400,\n",
       "   14074200,\n",
       "   13783500,\n",
       "   13519500,\n",
       "   9787500,\n",
       "   6970500,\n",
       "   13237200,\n",
       "   12667500,\n",
       "   13185300,\n",
       "   6247200]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "write_dicts_to_file('validation.json', validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sage_client = boto3.Session().client('sagemaker')\n",
    "\n",
    "tuning_job_name = 'forecastingteamawesome-tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(training_image, \n",
    "                                                    role, \n",
    "                                                    train_instance_count=1,\n",
    "                                                    train_instance_type='ml.p3.2xlarge',\n",
    "                                          hyperparameters={'time_freq':'D', 'prediction_length':1 },\n",
    "                                                    output_path='s3://forecastingteamawesomedataset/hypertuned', \n",
    "                                                    sagemaker_session=sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime \n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "tuning_job_name = \"awesome-job-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'epochs': IntegerParameter(600,1000),\n",
    "                         'context_length': IntegerParameter(1, 7),\n",
    "                         'learning_rate': ContinuousParameter(0.001,0.1),\n",
    "                         'likelihood': CategoricalParameter(['gaussian',\n",
    "                                                            'student-T'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'test:RMSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator, \n",
    "                            objective_metric_name, \n",
    "                            hyperparameter_ranges,\n",
    "                            objective_type='Minimize', \n",
    "                            max_jobs=21, \n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "tuner.fit({'train': 's3://forecastingteamawesomedataset/morning/train.json', \n",
    "           'test': 's3://forecastingteamawesomedataset/morning/validation.json'}, \n",
    "          job_name=tuning_job_name, include_cls_metadata=False)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
